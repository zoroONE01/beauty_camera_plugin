Directory structure:
└── apparence-io-camerawesome/
    ├── README.md
    ├── analysis_options.yaml
    ├── camerawesome.iml
    ├── CHANGELOG.md
    ├── CONTRIBUTING.md
    ├── docs.json
    ├── LICENSE
    ├── pubspec.lock
    ├── pubspec.yaml
    ├── README.zh.md
    ├── .metadata
    ├── android/
    │   ├── gradle.properties
    │   ├── .gitignore
    │   ├── gradle/
    │   │   └── wrapper/
    │   │       └── gradle-wrapper.properties
    │   ├── res/
    │   │   └── values/
    │   │       └── strings_en.arb
    │   └── src/
    │       └── main/
    │           ├── AndroidManifest.xml
    │           └── kotlin/
    │               └── com/
    │                   └── apparence/
    │                       ├── camerawesome/
    │                       │   ├── CamerawesomePlugin.java
    │                       │   ├── buttons/
    │                       │   │   ├── PhysicalButtonsHandler.kt
    │                       │   │   └── PlayerService.kt
    │                       │   ├── cameraX/
    │                       │   │   ├── AnalysisImageConverter.kt
    │                       │   │   ├── AutoFitPreviewBuilder.kt
    │                       │   │   ├── CameraAwesomeX.kt
    │                       │   │   ├── CameraCapabilities.kt
    │                       │   │   ├── CameraPermissions.kt
    │                       │   │   ├── CameraXState.kt
    │                       │   │   ├── ImageAnalysisBuilder.kt
    │                       │   │   ├── OrientationStreamListener.kt
    │                       │   │   └── Pigeon.kt
    │                       │   ├── exceptions/
    │                       │   │   ├── CameraManagerException.java
    │                       │   │   ├── CameraPreviewException.java
    │                       │   │   └── PermissionNotDeclaredException.kt
    │                       │   ├── image/
    │                       │   │   ├── ImgConverter.java
    │                       │   │   ├── ImgConverterThreaded.java
    │                       │   │   └── YuvToJpgConverter.java
    │                       │   ├── models/
    │                       │   │   ├── CameraCharacteristicsModel.java
    │                       │   │   └── FlashMode.java
    │                       │   ├── sensors/
    │                       │   │   ├── BasicLuminosityNotifier.java
    │                       │   │   ├── CameraSensor.java
    │                       │   │   ├── LuminosityNotifier.java
    │                       │   │   ├── SensorOrientation.kt
    │                       │   │   └── SensorOrientationListener.kt
    │                       │   ├── surface/
    │                       │   │   ├── FlutterSurfaceFactory.java
    │                       │   │   └── SurfaceFactory.java
    │                       │   └── utils/
    │                       │       ├── CameraCharactericitsUtils.kt
    │                       │       ├── CameraProviderUtils.kt
    │                       │       └── ResettableCountDownLatch.kt
    │                       └── camerawesome_example/
    │                           └── MainActivity.kt
    ├── assets/
    │   └── icons/
    ├── docs/
    │   ├── index.mdx
    │   ├── appendix/
    │   │   ├── license.mdx
    │   │   └── roadmap.mdx
    │   ├── getting_started/
    │   │   ├── awesome-ui.mdx
    │   │   ├── custom-ui.mdx
    │   │   ├── installing.mdx
    │   │   └── multicam.mdx
    │   ├── image_analysis/
    │   │   ├── detecting_faces.mdx
    │   │   ├── image_analysis_configuration.mdx
    │   │   ├── image_format_conversions.mdx
    │   │   └── reading_barcodes.mdx
    │   ├── img/
    │   │   └── awesome_ui_parts.webp
    │   ├── migration_guides/
    │   │   └── from_1_to_2.mdx
    │   └── widgets/
    │       ├── awesome_buttons.mdx
    │       ├── awesome_camera_mode_selector.mdx
    │       ├── awesome_filters.mdx
    │       ├── awesome_oriented_widget.mdx
    │       ├── layout.mdx
    │       ├── theming.mdx
    │       └── widgets.mdx
    ├── example/
    │   ├── README.md
    │   ├── analysis_options.yaml
    │   ├── pubspec.lock
    │   ├── pubspec.yaml
    │   ├── .gitignore
    │   ├── .metadata
    │   ├── android/
    │   │   ├── gradle.properties
    │   │   ├── .gitignore
    │   │   ├── app/
    │   │   │   └── src/
    │   │   │       ├── debug/
    │   │   │       │   └── AndroidManifest.xml
    │   │   │       ├── main/
    │   │   │       │   ├── AndroidManifest.xml
    │   │   │       │   ├── kotlin/
    │   │   │       │   │   └── com/
    │   │   │       │   │       └── example/
    │   │   │       │   │           └── camera_app/
    │   │   │       │   │               └── MainActivity.kt
    │   │   │       │   └── res/
    │   │   │       │       ├── drawable/
    │   │   │       │       │   └── launch_background.xml
    │   │   │       │       ├── drawable-v21/
    │   │   │       │       │   └── launch_background.xml
    │   │   │       │       ├── mipmap-hdpi/
    │   │   │       │       ├── mipmap-mdpi/
    │   │   │       │       ├── mipmap-xhdpi/
    │   │   │       │       ├── mipmap-xxhdpi/
    │   │   │       │       ├── mipmap-xxxhdpi/
    │   │   │       │       ├── values/
    │   │   │       │       │   └── styles.xml
    │   │   │       │       └── values-night/
    │   │   │       │           └── styles.xml
    │   │   │       └── profile/
    │   │   │           └── AndroidManifest.xml
    │   │   └── gradle/
    │   │       └── wrapper/
    │   │           └── gradle-wrapper.properties
    │   ├── integration_test/
    │   │   ├── bundled_test.dart
    │   │   ├── common.dart
    │   │   ├── concurrent_camera_test.dart
    │   │   ├── photo_test.dart
    │   │   ├── plugin_integration_test.dart
    │   │   ├── ui_test.dart
    │   │   └── video_test.dart
    │   ├── ios/
    │   │   ├── Podfile
    │   │   ├── Podfile.lock
    │   │   ├── .gitignore
    │   │   ├── Flutter/
    │   │   │   ├── AppFrameworkInfo.plist
    │   │   │   ├── Debug.xcconfig
    │   │   │   └── Release.xcconfig
    │   │   ├── Runner/
    │   │   │   ├── AppDelegate.swift
    │   │   │   ├── Info.plist
    │   │   │   ├── Runner-Bridging-Header.h
    │   │   │   ├── Assets.xcassets/
    │   │   │   │   ├── AppIcon.appiconset/
    │   │   │   │   │   └── Contents.json
    │   │   │   │   └── LaunchImage.imageset/
    │   │   │   │       ├── README.md
    │   │   │   │       └── Contents.json
    │   │   │   └── Base.lproj/
    │   │   │       ├── LaunchScreen.storyboard
    │   │   │       └── Main.storyboard
    │   │   ├── Runner.xcodeproj/
    │   │   │   ├── project.pbxproj
    │   │   │   ├── project.xcworkspace/
    │   │   │   │   ├── contents.xcworkspacedata
    │   │   │   │   └── xcshareddata/
    │   │   │   │       ├── IDEWorkspaceChecks.plist
    │   │   │   │       └── WorkspaceSettings.xcsettings
    │   │   │   └── xcshareddata/
    │   │   │       └── xcschemes/
    │   │   │           ├── Runner.xcscheme
    │   │   │           └── Runner.xcscheme.backup
    │   │   ├── Runner.xcworkspace/
    │   │   │   ├── contents.xcworkspacedata
    │   │   │   └── xcshareddata/
    │   │   │       ├── IDEWorkspaceChecks.plist
    │   │   │       └── WorkspaceSettings.xcsettings
    │   │   ├── RunnerTests/
    │   │   │   └── RunnerTests.swift
    │   │   └── RunnerUITests/
    │   │       └── RunnerTests.m
    │   ├── lib/
    │   │   ├── ai_analysis_barcode.dart
    │   │   ├── ai_analysis_faces.dart
    │   │   ├── ai_analysis_text.dart
    │   │   ├── analysis_image_filter.dart
    │   │   ├── analysis_image_filter_picker.dart
    │   │   ├── camera_analysis_capabilities.dart
    │   │   ├── custom_awesome_ui.dart
    │   │   ├── custom_theme.dart
    │   │   ├── custom_ui_example_1.dart
    │   │   ├── custom_ui_example_2.dart
    │   │   ├── custom_ui_example_3.dart
    │   │   ├── drivable_camera.dart
    │   │   ├── main.dart
    │   │   ├── multi_camera.dart
    │   │   ├── preview_overlay_example.dart
    │   │   ├── run_drivable_camera.dart
    │   │   ├── subroute_camera.dart
    │   │   ├── utils/
    │   │   │   ├── file_utils.dart
    │   │   │   └── mlkit_utils.dart
    │   │   └── widgets/
    │   │       ├── barcode_preview_overlay.dart
    │   │       ├── custom_media_preview.dart
    │   │       └── mini_video_player.dart
    │   └── scripts/
    │       ├── run_firebase_test_lab.sh
    │       ├── run_firebase_test_lab_multicam.sh
    │       ├── run_native_android_multicam_tests.sh
    │       ├── run_native_android_tests.sh
    │       └── run_patrol.sh
    ├── ios/
    │   ├── camerawesome.podspec
    │   ├── .gitignore
    │   ├── Assets/
    │   │   └── .gitkeep
    │   └── camerawesome/
    │       ├── Package.swift
    │       └── Sources/
    │           └── camerawesome/
    │               ├── CamerawesomePlugin.m
    │               ├── PrivacyInfo.xcprivacy
    │               ├── CameraPreview/
    │               │   ├── CameraDeviceInfo/
    │               │   │   └── CameraDeviceInfo.m
    │               │   ├── CameraPreviewTexture/
    │               │   │   └── CameraPreviewTexture.m
    │               │   ├── MultiCameraPreview/
    │               │   │   └── MultiCameraPreview.m
    │               │   └── SingleCameraPreview/
    │               │       └── SingleCameraPreview.m
    │               ├── Controllers/
    │               │   ├── Analysis/
    │               │   │   └── AnalysisController.m
    │               │   ├── Exif/
    │               │   │   ├── ExifContainer.m
    │               │   │   └── NSData+Exif.m
    │               │   ├── ImageStream/
    │               │   │   └── ImageStreamController.m
    │               │   ├── Location/
    │               │   │   └── LocationController.m
    │               │   ├── Motion/
    │               │   │   └── MotionController.m
    │               │   ├── MultiCamera/
    │               │   │   └── MultiCameraController.m
    │               │   ├── Permissions/
    │               │   │   └── PermissionsController.m
    │               │   ├── PhysicalButtons/
    │               │   │   └── PhysicalButtonController.m
    │               │   ├── Picture/
    │               │   │   └── CameraPictureController.m
    │               │   ├── Sensors/
    │               │   │   └── SensorsController.m
    │               │   └── Video/
    │               │       └── VideoController.m
    │               ├── include/
    │               │   ├── AnalysisController.h
    │               │   ├── AspectRatio.h
    │               │   ├── AspectRatioUtils.h
    │               │   ├── CameraDeviceInfo.h
    │               │   ├── CameraFlash.h
    │               │   ├── CameraPictureController.h
    │               │   ├── CameraPreviewTexture.h
    │               │   ├── CameraQualities.h
    │               │   ├── CameraSensor.h
    │               │   ├── CameraSensorType.h
    │               │   ├── CamerawesomePlugin.h
    │               │   ├── CaptureModes.h
    │               │   ├── CaptureModeUtils.h
    │               │   ├── ExifContainer.h
    │               │   ├── FlashModeUtils.h
    │               │   ├── ImageStreamController.h
    │               │   ├── InputAnalysisImageFormat.h
    │               │   ├── InputImageRotation.h
    │               │   ├── LocationController.h
    │               │   ├── MotionController.h
    │               │   ├── MultiCameraController.h
    │               │   ├── MultiCameraPreview.h
    │               │   ├── NSData+Exif.h
    │               │   ├── Permissions.h
    │               │   ├── PermissionsController.h
    │               │   ├── PhysicalButton.h
    │               │   ├── PhysicalButtonController.h
    │               │   ├── Pigeon.h
    │               │   ├── SensorsController.h
    │               │   ├── SensorUtils.h
    │               │   ├── SingleCameraPreview.h
    │               │   └── VideoController.h
    │               ├── Pigeon/
    │               │   └── Pigeon.m
    │               └── Utils/
    │                   ├── CameraQualities.m
    │                   ├── AspectRatio/
    │                   │   └── AspectRatioUtils.m
    │                   ├── CaptureMode/
    │                   │   └── CaptureModeUtils.m
    │                   ├── FlashMode/
    │                   │   └── FlashModeUtils.m
    │                   └── Sensor/
    │                       └── SensorUtils.m
    ├── lib/
    │   ├── camerawesome_plugin.dart
    │   ├── pigeon.dart
    │   └── src/
    │       ├── logger.dart
    │       ├── camera_characteristics/
    │       │   └── camera_characteristics.dart
    │       ├── orchestrator/
    │       │   ├── camera_context.dart
    │       │   ├── adapters/
    │       │   │   └── pigeon_sensor_adapter.dart
    │       │   ├── analysis/
    │       │   │   ├── analysis_controller.dart
    │       │   │   └── analysis_to_image.dart
    │       │   ├── exceptions/
    │       │   │   └── camera_states_exceptions.dart
    │       │   ├── file/
    │       │   │   ├── builder/
    │       │   │   │   ├── capture_request_builder.dart
    │       │   │   │   ├── capture_request_builder_io.dart
    │       │   │   │   ├── capture_request_builder_stub.dart
    │       │   │   │   └── capture_request_builder_web.dart
    │       │   │   └── content/
    │       │   │       ├── file_content.dart
    │       │   │       ├── file_content_io.dart
    │       │   │       ├── file_content_stub.dart
    │       │   │       └── file_content_web.dart
    │       │   ├── models/
    │       │   │   ├── camera_flashes.dart
    │       │   │   ├── camera_orientations.dart
    │       │   │   ├── camera_physical_button.dart
    │       │   │   ├── capture_modes.dart
    │       │   │   ├── capture_request.dart
    │       │   │   ├── media_capture.dart
    │       │   │   ├── models.dart
    │       │   │   ├── permission_utils.dart
    │       │   │   ├── save_config.dart
    │       │   │   ├── sensor_config.dart
    │       │   │   ├── sensor_data.dart
    │       │   │   ├── sensor_type.dart
    │       │   │   ├── sensors.dart
    │       │   │   ├── video_options.dart
    │       │   │   ├── analysis/
    │       │   │   │   ├── analysis.dart
    │       │   │   │   ├── analysis_canvas.dart
    │       │   │   │   ├── analysis_config.dart
    │       │   │   │   ├── analysis_image.dart
    │       │   │   │   ├── analysis_image_ext.dart
    │       │   │   │   ├── image_plane.dart
    │       │   │   │   └── input_analysis.dart
    │       │   │   └── filters/
    │       │   │       ├── awesome_filter.dart
    │       │   │       └── awesome_filters.dart
    │       │   └── states/
    │       │       ├── analysis_camera_state.dart
    │       │       ├── camera_state.dart
    │       │       ├── photo_camera_state.dart
    │       │       ├── preparing_camera_state.dart
    │       │       ├── preview_camera_state.dart
    │       │       ├── states.dart
    │       │       ├── video_camera_recording_state.dart
    │       │       ├── video_camera_state.dart
    │       │       └── handlers/
    │       │           └── filter_handler.dart
    │       ├── photofilters/
    │       │   ├── rgba_model.dart
    │       │   ├── filters/
    │       │   │   ├── color_filters.dart
    │       │   │   ├── convolution_filters.dart
    │       │   │   ├── filters.dart
    │       │   │   ├── image_filters.dart
    │       │   │   ├── preset_filters.dart
    │       │   │   └── subfilters.dart
    │       │   └── utils/
    │       │       ├── color_filter_utils.dart
    │       │       ├── convolution_kernels.dart
    │       │       ├── image_filter_utils.dart
    │       │       └── utils.dart
    │       └── widgets/
    │           ├── awesome_camera_mode_selector.dart
    │           ├── awesome_media_preview.dart
    │           ├── awesome_sensor_type_selector.dart
    │           ├── camera_awesome_builder.dart
    │           ├── widgets.dart
    │           ├── buttons/
    │           │   ├── awesome_aspect_ratio_button.dart
    │           │   ├── awesome_camera_switch_button.dart
    │           │   ├── awesome_capture_button.dart
    │           │   ├── awesome_flash_button.dart
    │           │   ├── awesome_location_button.dart
    │           │   ├── awesome_pause_resume_button.dart
    │           │   └── buttons.dart
    │           ├── filters/
    │           │   ├── awesome_filter_button.dart
    │           │   ├── awesome_filter_name_indicator.dart
    │           │   ├── awesome_filter_selector.dart
    │           │   ├── awesome_filter_widget.dart
    │           │   └── filters.dart
    │           ├── layout/
    │           │   ├── awesome_bottom_actions.dart
    │           │   ├── awesome_camera_layout.dart
    │           │   ├── awesome_top_actions.dart
    │           │   └── layout.dart
    │           ├── preview/
    │           │   ├── awesome_camera_floating_preview.dart
    │           │   ├── awesome_camera_gesture_detector.dart
    │           │   ├── awesome_camera_preview.dart
    │           │   ├── awesome_focus_indicator.dart
    │           │   ├── awesome_preview_fit.dart
    │           │   ├── picture_in_picutre_config.dart
    │           │   └── preview.dart
    │           ├── utils/
    │           │   ├── animated_clip_rect.dart
    │           │   ├── awesome_bouncing_widget.dart
    │           │   ├── awesome_circle_icon.dart
    │           │   ├── awesome_oriented_widget.dart
    │           │   ├── awesome_theme.dart
    │           │   └── utils.dart
    │           └── zoom/
    │               ├── awesome_zoom_selector.dart
    │               └── zoom.dart
    ├── pigeons/
    │   ├── interface.dart
    │   └── pigeon.sh
    ├── test/
    │   ├── camera_context_test.dart
    │   ├── filter_test.dart
    │   └── res/
    ├── .github/
    │   ├── FUNDING.yml
    │   ├── PULL_REQUEST_TEMPLATE.md
    │   └── ISSUE_TEMPLATE/
    │       ├── BUG.md
    │       └── feature_request.md
    └── .run/
        ├── analysis_image_filter.run.xml
        ├── analysis_image_filter_picker.run.xml
        ├── barcode_overlay_example.run.xml
        ├── camera_analysis_capabilities.run.xml
        ├── custom_awesome_ui.run.xml
        ├── custom_theme.run.xml
        ├── detect_barcodes.run.xml
        ├── detect_faces.run.xml
        ├── main.dart.run.xml
        └── multi_camera.run.xml

================================================
FILE: README.md
================================================
<a href="https://apparence.io">
  <img
    src="https://raw.githubusercontent.com/Apparence-io/camera_awesome/master/docs/img/apparence.png"
    width="100%"
  />
</a>
<div style="margin-top:40px">
  <img
    src="https://raw.githubusercontent.com/Apparence-io/camera_awesome/master/docs/img/preview.png"
    width="100%"
  />
  <img
    src="https://raw.githubusercontent.com/Apparence-io/camera_awesome/master/docs/img/features.png"
    width="100%"
    style="margin-top:32px"
  />
</div>

<a href="https://apparencekit.dev" style="margin-top:32px">
  <img
    src="https://raw.githubusercontent.com/Apparence-io/camera_awesome/master/docs/img/flutter_template.png"
    width="100%"
    alt="ApparenceKit Flutter template to bootstrap your next app"
  />
</a>

This plugin is also available as a template in
[ApparenceKit](https://apparencekit.dev).<br>

<br>

# CamerAwesome

<div>
    <a href="https://github.com/Solido/awesome-flutter">
        <img alt="Awesome Flutter" src="https://img.shields.io/badge/Awesome-Flutter-blue.svg?longCache=true&style=for-the-badge" />
    </a>
    <a href="https://github.com/Apparence-io/camera_awesome">
        <img src="https://img.shields.io/github/stars/Apparence-io/camera_awesome.svg?style=for-the-badge&logo=github&colorB=green&label=Stars" alt="Star on Github">
    </a>
    <a href="https://pub.dev/packages/camerawesome">
        <img src="https://img.shields.io/pub/v/camerawesome.svg?style=for-the-badge&label=Pub" alt="Star on Github">
    </a>
</div>

[![en](https://img.shields.io/badge/language-english-cyan.svg)](https://github.com/Apparence-io/CamerAwesome/blob/master/README.md)
[![zh](https://img.shields.io/badge/language-chinese-cyan.svg)](https://github.com/Apparence-io/CamerAwesome/blob/master/README.zh.md)

📸 Embedding a camera experience within your own app shouldn't be that hard.
<br> A flutter plugin to integrate awesome Android / iOS camera experience.<br>

<br>
This package provides you with a fully customizable camera experience that you can use within your app.<br>
Use our awesome built-in interface or customize it as you want.

---

<div style="margin-top:16px;margin-bottom:16px">
  <a href="https://docs.page/Apparence-io/camera_awesome" style="">
    <img
      src="https://raw.githubusercontent.com/Apparence-io/camera_awesome/master/docs/img/doc.png"
      width="100%"
    />
  </a>
</div>

## Migration guide

If you are migrating from version 1.x.x to 2.x.x, please read the
[migration guide](https://docs.page/Apparence-io/camera_awesome/migration_guides/from_1_to_2).

## Native features

Here's all native features that cameraAwesome provides to the flutter side.

| Features                                 | Android | iOS |
| :--------------------------------------- | :-----: | :-: |
| 🔖 Ask permissions                       |   ✅    | ✅  |
| 🎥 Record video                          |   ✅    | ✅  |
| 📹 Multi camera (🚧 BETA)                |   ✅    | ✅  |
| 🔈 Enable/disable audio                  |   ✅    | ✅  |
| 🎞 Take photos                            |   ✅    | ✅  |
| 🌆 Photo live filters                    |   ✅    | ✅  |
| 🌤 Exposure level                         |   ✅    | ✅  |
| 📡 Broadcast live image stream           |   ✅    | ✅  |
| 🧪 Image analysis (barcode scan & more.) |   ✅    | ✅  |
| 👁 Zoom                                   |   ✅    | ✅  |
| 📸 Device flash support                  |   ✅    | ✅  |
| ⌛️ Auto focus                            |   ✅    | ✅  |
| 📲 Live switching camera                 |   ✅    | ✅  |
| 😵‍💫 Camera rotation stream              |   ✅    | ✅  |
| 🤐 Background auto stop                  |   ✅    | ✅  |
| 🔀 Sensor type switching                 |   ⛔️    | ✅  |
| 🪞 Enable/disable front camera mirroring |   ✅    | ✅  |

---

## 📖&nbsp; Installation and usage

### Add the package in your `pubspec.yaml`

```yaml
dependencies:
  camerawesome: ^2.0.0-dev.1
  ...
```

### Platform specific setup

- **iOS**

Add these on `ios/Runner/Info.plist`:

```xml
<key>NSCameraUsageDescription</key>
<string>Your own description</string>

<key>NSMicrophoneUsageDescription</key>
<string>To enable microphone access when recording video</string>

<key>NSLocationWhenInUseUsageDescription</key>
<string>To enable GPS location access for Exif data</string>
```

- **Android**

Change the minimum SDK version to 21 (or higher) in `android/app/build.gradle`:

```
minSdkVersion 21
```

In order to be able to take pictures or record videos, you may need additional
permissions depending on the Android version and where you want to save them.
Read more about it in the
[official documentation](https://developer.android.com/training/data-storage).

> `WRITE_EXTERNAL_STORAGE` is not included in the plugin starting with version
> 1.4.0.

If you want to record videos with audio, add this permission to your
`AndroidManifest.xml`:

```xml
<manifest xmlns:android="http://schemas.android.com/apk/res/android"
        package="com.example.yourpackage">
  <uses-permission android:name="android.permission.RECORD_AUDIO" />

  <!-- Other declarations -->
</manifest>
```

You may also want to save location of your pictures in exif metadata. In this
case, add below permissions:

```xml
<manifest xmlns:android="http://schemas.android.com/apk/res/android"
  package="com.example.yourpackage">
  <uses-permission android:name="android.permission.ACCESS_FINE_LOCATION" />
  <uses-permission android:name="android.permission.ACCESS_COARSE_LOCATION" />

  <!-- Other declarations -->
</manifest>
```

<details>
<summary>⚠️ Overriding Android dependencies</summary>

Some of the dependencies used by CamerAwesome can be overriden if you have a
conflict. Change these variables to define which version you want to use:

```gradle
buildscript {
  ext.kotlin_version = '1.7.10'
  ext {
    // You can override these variables
    compileSdkVersion = 33
    minSdkVersion = 24 // 21 minimum
    playServicesLocationVersion = "20.0.0"
    exifInterfaceVersion = "1.3.4"
  }
  // ...
}
```

Only change these variables if you are sure of what you are doing.

For example, setting the Play Services Location version might help you when you
have conflicts with other plugins. The below line shows an example of these
conflicts:

```
java.lang.IncompatibleClassChangeError: Found interface com.google.android.gms.location.ActivityRecognitionClient, but class was expected
```

</details>

### Import the package in your Flutter app

```dart
import 'package:camerawesome/camerawesome_plugin.dart';
```

---

## 👌 Awesome built-in interface

Just use our builder. <br> That's all you need to create a complete camera
experience within your app.

```dart
CameraAwesomeBuilder.awesome(
  saveConfig: SaveConfig.photoAndVideo(),
  onMediaTap: (mediaCapture) {
    OpenFile.open(mediaCapture.filePath);
  },
),
```

![CamerAwesome default UI](docs/img/base_awesome_ui.jpg)

This builder can be customized with various settings:

- A theme.
- Builders for each part of the screen.
- Initial camera setup.
- Preview positioning.
- Additional preview decoration.
- And much more!

Here is an example:

![Customized UI](docs/img/custom_awesome_ui.jpg)

Check the
[full documentation](https://docs.page/Apparence-io/camera_awesome/getting_started/awesome-ui)
to learn more.

---

## 🎨 Creating a custom interface

If the `awesome()` factory is not enough, you can use `custom()` instead.

It provides a `builder` property that lets you create your own camera
experience. <br>

The camera preview will be visible behind what you will provide to the builder.

```dart
CameraAwesomeBuilder.custom(
  saveConfig: SaveConfig.photo(),
  builder: (state, previewSize, previewRect) {
    // create your interface here
  },
)
```

> See more in
> [documentation](https://docs.page/Apparence-io/camera_awesome/getting_started/custom-ui)

### Working with the custom builder

Here is the definition of our builder method.

```dart
typedef CameraLayoutBuilder = Widget Function(CameraState cameraState, PreviewSize previewSize, Rect previewRect);
```

<br>
The only thing you have access to manage the camera is the cameraState.<br>
Depending on which state is our camera experience you will have access to some different method. <br>
`previewSize` and `previewRect` might be used to position your UI around or on top of the camera preview.
<br>

#### How do CamerAwesome states work ?

Using the state you can do anything you need without having to think about the
camera flow<br><br>

- On app start we are in `PreparingCameraState`<br>
- Then depending on the initialCaptureMode you set you will be
  `PhotoCameraState` or `VideoCameraState`<br>
- Starting a video will push a `VideoRecordingCameraState`<br>
- Stopping the video will push back the `VideoCameraState`<br>
  <br> Also if you want to use some specific function you can use the when
  method so you can write like this.<br>

```dart
state.when(
  onPhotoMode: (photoState) => photoState.start(),
  onVideoMode: (videoState) => videoState.start(),
  onVideoRecordingMode: (videoState) => videoState.pause(),
);
```

> See more in
> [documentation](https://docs.page/Apparence-io/camera_awesome/getting_started/custom-ui)

<br>

---

## 🐝 Listen to picture or video event

Using the onMediaCaptureEvent you can listen to any media capture event and do
whatever you want with it.

```dart
onMediaCaptureEvent: (event) {
    switch ((event.status, event.isPicture, event.isVideo)) {
        case (MediaCaptureStatus.capturing, true, false):
            debugPrint('Capturing picture...');
        case (MediaCaptureStatus.success, true, false):
            event.captureRequest.when(
                single: (single) {
                debugPrint('Picture saved: ${single.file?.path}');
                },
                multiple: (multiple) {
                multiple.fileBySensor.forEach((key, value) {
                    debugPrint('multiple image taken: $key ${value?.path}');
                });
                },
            );
        case (MediaCaptureStatus.failure, true, false):
            debugPrint('Failed to capture picture: ${event.exception}');
        case (MediaCaptureStatus.capturing, false, true):
            debugPrint('Capturing video...');
        case (MediaCaptureStatus.success, false, true):
            event.captureRequest.when(
                single: (single) {
                    debugPrint('Video saved: ${single.file?.path}');
                },
                multiple: (multiple) {
                    multiple.fileBySensor.forEach((key, value) {
                        debugPrint('multiple video taken: $key ${value?.path}');
                    });
                },
            );
        case (MediaCaptureStatus.failure, false, true):
            debugPrint('Failed to capture video: ${event.exception}');
        default:
            debugPrint('Unknown event: $event');
    }
},
```

---

## 🔬 Analysis mode

Use this to achieve:

- QR-Code scanning.
- Facial recognition.
- AI object detection.
- Realtime video chats.
- And much more 🤩

![Face AI](docs/img/face_ai.gif)

You can check examples using MLKit inside the `example` directory. The above
example is from `ai_analysis_faces.dart`. It detects faces and draw their
contours.

It's also possible to use MLKit to read barcodes:

![Barcode scanning](docs/img/barcode_overlay.gif)

Check `ai_analysis_barcode.dart` and `preview_overlay_example.dart` for examples
or the
[documentation](https://docs.page/Apparence-io/camera_awesome/ai_with_mlkit/reading_barcodes).

### How to use it

```dart
CameraAwesomeBuilder.awesome(
  saveConfig: SaveConfig.photo(),
  onImageForAnalysis: analyzeImage,
  imageAnalysisConfig: AnalysisConfig(
        // Android specific options
        androidOptions: const AndroidAnalysisOptions.nv21(
            // Target width (CameraX will chose the closest resolution to this width)
            width: 250,
        ),
        // Wether to start automatically the analysis (true by default)
        autoStart: true,
        // Max frames per second, null for no limit (default)
        maxFramesPerSecond: 20,
    ),
)
```

> MLkit recommends using nv21 format for Android. <br> bgra8888 is the iOS
> format For machine learning you don't need full-resolution images (720 or
> lower should be enough and makes computation easier)

Learn more about the image analysis configuration in the
[documentation](https://docs.page/Apparence-io/camera_awesome/ai_with_mlkit/image_analysis_configuration)
.

Check also detailed explanations on how to use MLKit to
[read barcodes](https://docs.page/Apparence-io/camera_awesome/ai_with_mlkit/reading_barcodes)
and
[detect faces](https://docs.page/Apparence-io/camera_awesome/ai_with_mlkit/detecting_faces).

⚠️ On Android, some devices don't support video recording and image analysis at
the same time.

- If they don't, image analysis will be ignored.
- You can check if a device has this capability by using
  `CameraCharacteristics .isVideoRecordingAndImageAnalysisSupported(Sensors.back)`.

---

## 🐽 Updating Sensor configuration

Through state you can access to a `SensorConfig` class.

<br>

| Function               | Comment                                                    |
| ---------------------- | ---------------------------------------------------------- |
| `setZoom`              | change zoom                                                |
| `setFlashMode`         | change flash between NONE,ON,AUTO,ALWAYS                   |
| `setBrightness`        | change brightness level manually (better to let this auto) |
| `setMirrorFrontCamera` | set mirroring for front camera                             |

All of these configurations are listenable through a stream so your UI can
automatically get updated according to the actual configuration.

<br>

## 🌆 Photo live filters

Apply live filters to your pictures using the built-in interface:

![Built-in live filters](docs/img/filters.gif)

You can also choose to use a specific filter from the start:

```dart
CameraAwesomeBuilder.awesome(
  // other params
  filter: AwesomeFilter.AddictiveRed,
  availableFilters: ...
)
```

Or set the filter programmatically:

```dart
CameraAwesomeBuilder.custom(
  builder: (cameraState, previewSize, previewRect) {
    return cameraState.when(
      onPreparingCamera: (state) =>
      const Center(child: CircularProgressIndicator()),
      onPhotoMode: (state) =>
          TakePhotoUI(state, onFilterTap: () {
            state.setFilter(AwesomeFilter.Sierra);
          }),
      onVideoMode: (state) => RecordVideoUI(state, recording: false),
      onVideoRecordingMode: (state) =>
          RecordVideoUI(state, recording: true),
    );
  },
)
```

See all available filters in the
[documentation](https://docs.page/Apparence-io/camera_awesome/widgets/awesome_filters).

> [!TIP] By default the awesome ui setup has a filter list but you can pass an
> empty list to remove it

## 📷 📷 Concurrent cameras

![Concurrent cameras](docs/img/concurrent_cameras.gif)

> 🚧 Feature in beta 🚧 Any feedback is welcome!

In order to start using CamerAwesome with multiple cameras simulatenously, you
need to define a `SensorConfig` that uses several sensors. You can use the
`SensorConfig.multiple()` constructor for this:

```dart
CameraAwesomeBuilder.awesome(
    sensorConfig: SensorConfig.multiple(
        sensors: [
            Sensor.position(SensorPosition.back),
            Sensor.position(SensorPosition.front),
        ],
        flashMode: FlashMode.auto,
        aspectRatio: CameraAspectRatios.ratio_16_9,
    ),
    // Other params
)
```

This feature is not supported by all devices and even when it is, there are
limitations that you must be aware of.

Check the details in the
[dedicated documentation](https://docs.page/Apparence-io/camera_awesome/getting_started/multicam).

<br>

<a href="https://apparence.io">
  <img
    src="https://raw.githubusercontent.com/Apparence-io/camera_awesome/master/docs/img/apparence.png"
    width="100%"
  />
</a>



================================================
FILE: analysis_options.yaml
================================================
# This file configures the analyzer, which statically analyzes Dart code to
# check for errors, warnings, and lints.
#
# The issues identified by the analyzer are surfaced in the UI of Dart-enabled
# IDEs (https://dart.dev/tools#ides-and-editors). The analyzer can also be
# invoked from the command line by running `flutter analyze`.

# The following line activates a set of recommended lints for Flutter apps,
# packages, and plugins designed to encourage good coding practices.
include: package:flutter_lints/flutter.yaml

linter:
  # The lint rules applied to this project can be customized in the
  # section below to disable rules from the `package:flutter_lints/flutter.yaml`
  # included above or to enable additional rules. A list of all available lints
  # and their documentation is published at
  # https://dart-lang.github.io/linter/lints/index.html.
  #
  # Instead of disabling a lint rule for the entire project in the
  # section below, it can also be suppressed for a single line of code
  # or a specific dart file by using the `// ignore: name_of_lint` and
  # `// ignore_for_file: name_of_lint` syntax on the line or in the file
  # producing the lint.
  rules:
    always_use_package_imports: true
    # avoid_print: false  # Uncomment to disable the `avoid_print` rule
    # prefer_single_quotes: true  # Uncomment to enable the `prefer_single_quotes` rule
# Additional information about this file can be found at
# https://dart.dev/guides/language/analysis-options



================================================
FILE: camerawesome.iml
================================================
<?xml version="1.0" encoding="UTF-8"?>
<module type="JAVA_MODULE" version="4">
  <component name="NewModuleRootManager" inherit-compiler-output="true">
    <exclude-output />
    <content url="file://$MODULE_DIR$">
      <sourceFolder url="file://$MODULE_DIR$/lib" isTestSource="false" />
      <excludeFolder url="file://$MODULE_DIR$/.dart_tool" />
      <excludeFolder url="file://$MODULE_DIR$/.idea" />
      <excludeFolder url="file://$MODULE_DIR$/.pub" />
      <excludeFolder url="file://$MODULE_DIR$/build" />
      <excludeFolder url="file://$MODULE_DIR$/example/.dart_tool" />
      <excludeFolder url="file://$MODULE_DIR$/example/.pub" />
      <excludeFolder url="file://$MODULE_DIR$/example/build" />
    </content>
    <orderEntry type="sourceFolder" forTests="false" />
    <orderEntry type="library" name="Dart SDK" level="project" />
    <orderEntry type="library" name="Flutter Plugins" level="project" />
  </component>
</module>


================================================
FILE: CHANGELOG.md
================================================
# 2.2.O
- migrate to swift package manager
- upgrade cameraX to 1.3.4
- upgrade android to namespace

# 2.1.O
- Update for flutter 3.24.0
- Update dependencies (Thank you @tonistayhungrystayfoolish / @bytemaster / @millerf for the PR) 

# 2.0.1

- 🐛 Fix preview orientation on tablets for iOS and Android
- 🐛 Fix preview alignment
- 🐛 Fix get preview size on iOS

# 2.0.0 - Multi camera is here !

Hello everyone 👋 !

We are proud to announce the two most requested features on the official camera
plugin:

- Multi-camera 📹
- Video settings 🎥
- Preview rework 📸

This release introduces breaking changes in order to support above features. See
the
[migration guide](https://docs.page/Apparence-io/camera_awesome/migration_guides/from_1_to_2)
for details.

Here is the complete changelog:

- ✨ Added multi-camera feature, allowing users to display multiple camera
  previews simultaneously. Note that this feature is currently in beta, and we
  do not recommend using it in production.
- ✨ Users can now pass options (such as bitrate, fps, and quality) when
  recording a video.
- ✨ You can now mirror video recording.
- ✨🍏 Implemented brightness and exposure level settings on iOS / iPadOS.
- ✨🤖 Added zoom indicator UI.
- ✨🤖 Video recording is now mirrored if `mirrorFrontCamera` is set to true.
- ♻️🍏 Completely reworked the code for increased clarity and performance.
- 🐛 Fixed patrol tests.
- 🐛 Fixed the use of capture button parameter in awesome bottom actions (thanks
  to @juliuszmandrosz).
- 📝 Added Chinese README.md (thanks to @chyiiiiiiiiiiii).
- ↗️ Android CameraX version is now 1.3.0
- takePhoto ans stopVideoRecording now have callbacks for success and error.
- by default the awesome builder has a filter list but you can pass an empty
  list to remove it

# 1.4.0

- ✨ Add utilities to convert AnalysisImage into JPEG in order to display them
  using `toJpeg()`.
- ✨ Add `preview()` and `analysisOnly()` constructors to
  `CameraAwesomeBuilder`.
- ✨ Volume button trigger to take picture or record/stop video.
- ✨🍏 Add brightness exposure level on iOS / iPadOS.
- 💥 AnalysisConfig has changed slightly its parameters to have
  platform-specific setup.
- 💥 Storage permission is now optional on Android since the introduction of
  `preview()` and `analysisOnly()` modes.
- 🐛🍏 iOS / iPadOS max zoom limit.
- 🐛🤖 Better handle use cases conflicts (video + image analysis on lower-end
  devices) for Android.

# 1.3.1

- 🐛 Fix video recording overlay image.
- 📝 Update README.md (change feature showcase image & fix broken links).

# 1.3.0

- ✨ Customize the built-in UI by setting an `AwesomeTheme`.
- ✨ Top, middle and bottom parts of `CameraAwesomeBuilder.awesome()` can now be
  replaced by your own.
- ✨ Ability to set camera preview alignment and padding.
- ✨ Ability to set aspect ratio, zoom, flash mode and SensorType when switching
  between front and back camera.
- ✨ Enable/disable front camera mirroring.
- ⬆️ Upgrade `image` dependency.
- 🐛 Fix aspect ratio changes animation.
- 🐛 Smoother flash mode changes (Android).
- 🐛 Fix microphone permission (iOS).
- 🐛 Fix recorded video orientation (iOS).
- 🐛 Fix initial aspect ratio not set (iOS).
- 📝 Updated documentation and more examples.
- 🎨 Format code.

# 1.2.1

- Expose Gradle variables to avoid conflict with other plugins.
- iOS aspect ratio fix.

# 1.2.0

- Add filters for photo mode.
- Rework UI for awesome layout.
- Add start and stop method for image analysis.
- **BREAKING** Location and audio recording permissions are now optional. Add
  them to your AndroidManifest manually if you need them.
- Fix preview aspectRatio on iOS.

# 1.1.0

- Use [**pigeon**](https://pub.dev/packages/pigeon) for iOS instead of classic
  method channel.
- Greatly improve performances on analysis mode when FPS limit disabled.
- Fix barcode scrolling to bottom.
- Fix iOS stream guards.

# 1.0.0+4

- Code formatting and linter

# 1.0.0

- Bugfixes (imageAnalysis, initialAspectRatio...)
- Sensor type switching (iOS)
- Improve AI documentation
- Add `previewSize` and `previewRect` to `CameraAwesomeBuilder` builders

# 1.0.0-rc1

- Full rework of the API
- Better feature parity between iOS and Android
- Use the built-in camera UI or make your own
- Add docs.page documentation

# 0.4.0

- Migrate to CameraX instead of Camera2 on Android.
- Add GPS location in Exif photo on Android.
- Add Video recording for Android.

# 0.3.6

- Add GPS location in Exif photo on iOS.
- Fix some issues

# 0.3.4

- Add pinch to zoom.

# 0.3.3

- update android build tools to 30
- fix first permission request crash

# 0.3.2

- Update to Flutter 3.
- Update Android example project.
- Upgrade dependencies.
- Clean some code.

# 0.3.1

- handle app lifecycle (stop camera on background)

# 0.3.0

- Migrate null safety.
- Fixed aspect ratio of camera preview when using smaller image sizes.
- Fixed image capture on older android devices which use continuous (passive)
  focus.
- Fix image capture on iOS

# 0.2.1+1

- build won't show red screen in debug if camerAwesome is running on slow phones
- [Android] bind activity

# 0.2.1

- [iOS] image stream available to use MLkit or other image live processing
- [iOS] code refactoring

# 0.2.0

- [iOS] video recording support
- [iOS] thread and perf enhancements

# 0.1.2+1

- [Android] onDetachedFromActivity : fix stopping the camera should be only done
  if camera has been started
- listen native Orientation should be canceled correctly on dispose
  CameraAwesomeState
- unlock focus now restart session correctly after taking a photo
- takePicture listener now cannot send result more than one time

# 0.1.2

- [Android] get luminosity level from device
- [Android] apply brightness correction

# 0.1.1+1

- [android] fix release onOpenListener after emit result to Flutter platform

# 0.1.1

- prevent starting camera when already open on Flutter side
- stability between rebuilds improved on Flutter side
- [android] check size is correctly set before starting camera
- CameraPreview try 3 times to start if camera is locked (each try are 1s
  ellapsed)
- Fix android zoom when taking picture

# 0.1.0

- image stream available to use MLkit or other image live processing (Only
  android)

# 0.0.2+3

- fix switch camera on Android with new update (now correctly switch ImageReader
  and cameraCharacteristics when switch sensor).

# 0.0.2+1

- comment com.google.gms.google-services from example build.gradle. This is
  aimed only to start our e2e tests on testlabs. Put your own
  google-services.json if you want to start them there.

# 0.0.2

- updated readme

# 0.0.1

- first version. See readme for complete features list



================================================
FILE: CONTRIBUTING.md
================================================
# Contributing to CamerAwesome

## Reporting issues
You can easily report issues using GitHub.

Please include maximum information like:
- 🎯 Summary of the issue.
- 🚶‍♂️ Steps to reproduce.
- 😃 What you expected would happen.
- 🤔 What actually happens.
- 📝 Notes (possibly including why you think this might be happening, or stuff you tried that didn't work).

## Creating a Pull Request

### Code reviews
All submissions, including submissions by project members, require review.

### Match the actual coding style
Please use the actual project coding style.

- 2 spaces for indentation.
- Use Line Feed **LF**.
- Run ```flutter analyze``` before submit.

## License
By contributing, you agree that your contributions will be licensed under [CamerAwesome's licence](https://github.com/Apparence-io/camera_awesome/blob/master/LICENSE).


================================================
FILE: docs.json
================================================
{
  "name": "CamerAwesome",
  "socialPreview": "/img/preview.png",
  "theme": "#36B9B9",
  "googleAnalytics": "G-J2YNQZ6BE8",
  "sidebar": [
    ["Overview", "/index"],
    [
      "Getting Started",
      [
        ["Installing", "/getting_started/installing"],
        ["Using the built-in UI ", "/getting_started/awesome-ui"],
        ["Creating your own UI", "/getting_started/custom-ui"],
        ["Multiple cameras at once (🚧 BETA)", "/getting_started/multicam"]
      ]
    ],
    [
      "Migration guides",
      [["From 1.x.x to 2.x.x", "/migration_guides/from_1_to_2"]]
    ],
    [
      "Widgets",
      [
        ["Theming", "/widgets/theming"],
        ["Buttons", "/widgets/awesome_buttons"],
        ["Layout", "/widgets/layout"],
        ["AwesomeOrientedWidget", "/widgets/awesome_oriented_widget"],
        ["Filters", "/widgets/awesome_filters"],
        ["AwesomeCameraModeSelector", "/widgets/awesome_camera_mode_selector"]
      ]
    ],
    [
      "Image analysis (AI)",
      [
        [
          "Image analysis configuration",
          "/image_analysis/image_analysis_configuration"
        ],
        [
          "Image analysis formats and conversion",
          "/image_analysis/image_format_conversions"
        ],
        ["Reading barcodes", "/image_analysis/reading_barcodes"],
        ["Detecting faces", "/image_analysis/detecting_faces"]
      ]
    ],
    [
      "Appendix",
      [
        ["License", "/appendix/license"],
        ["Roadmap", "/appendix/roadmap"]
      ]
    ]
  ]
}



================================================
FILE: LICENSE
================================================
Copyright © Apparence.io
Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.

The Software is provided “as is”, without warranty of any kind, express or implied, including but not limited to the warranties of merchantability, fitness for a particular purpose and noninfringement. In no event shall the authors or copyright holders be liable for any claim, damages or other liability, whether in an action of contract, tort or otherwise, arising from, out of or in connection with the software or the use or other dealings in the Software.


================================================
FILE: pubspec.lock
================================================
# Generated by pub
# See https://dart.dev/tools/pub/glossary#lockfile
packages:
  _fe_analyzer_shared:
    dependency: transitive
    description:
      name: _fe_analyzer_shared
      sha256: "16e298750b6d0af7ce8a3ba7c18c69c3785d11b15ec83f6dcd0ad2a0009b3cab"
      url: "https://pub.dev"
    source: hosted
    version: "76.0.0"
  _macros:
    dependency: transitive
    description: dart
    source: sdk
    version: "0.3.3"
  analyzer:
    dependency: transitive
    description:
      name: analyzer
      sha256: "1f14db053a8c23e260789e9b0980fa27f2680dd640932cae5e1137cce0e46e1e"
      url: "https://pub.dev"
    source: hosted
    version: "6.11.0"
  archive:
    dependency: transitive
    description:
      name: archive
      sha256: "6199c74e3db4fbfbd04f66d739e72fe11c8a8957d5f219f1f4482dbde6420b5a"
      url: "https://pub.dev"
    source: hosted
    version: "4.0.2"
  args:
    dependency: transitive
    description:
      name: args
      sha256: bf9f5caeea8d8fe6721a9c358dd8a5c1947b27f1cfaa18b39c301273594919e6
      url: "https://pub.dev"
    source: hosted
    version: "2.6.0"
  async:
    dependency: transitive
    description:
      name: async
      sha256: d2872f9c19731c2e5f10444b14686eb7cc85c76274bd6c16e1816bff9a3bab63
      url: "https://pub.dev"
    source: hosted
    version: "2.12.0"
  boolean_selector:
    dependency: transitive
    description:
      name: boolean_selector
      sha256: "8aab1771e1243a5063b8b0ff68042d67334e3feab9e95b9490f9a6ebf73b42ea"
      url: "https://pub.dev"
    source: hosted
    version: "2.1.2"
  built_collection:
    dependency: transitive
    description:
      name: built_collection
      sha256: "376e3dd27b51ea877c28d525560790aee2e6fbb5f20e2f85d5081027d94e2100"
      url: "https://pub.dev"
    source: hosted
    version: "5.1.1"
  built_value:
    dependency: transitive
    description:
      name: built_value
      sha256: "28a712df2576b63c6c005c465989a348604960c0958d28be5303ba9baa841ac2"
      url: "https://pub.dev"
    source: hosted
    version: "8.9.3"
  carousel_slider:
    dependency: "direct main"
    description:
      name: carousel_slider
      sha256: "7b006ec356205054af5beaef62e2221160ea36b90fb70a35e4deacd49d0349ae"
      url: "https://pub.dev"
    source: hosted
    version: "5.0.0"
  characters:
    dependency: transitive
    description:
      name: characters
      sha256: f71061c654a3380576a52b451dd5532377954cf9dbd272a78fc8479606670803
      url: "https://pub.dev"
    source: hosted
    version: "1.4.0"
  clock:
    dependency: transitive
    description:
      name: clock
      sha256: fddb70d9b5277016c77a80201021d40a2247104d9f4aa7bab7157b7e3f05b84b
      url: "https://pub.dev"
    source: hosted
    version: "1.1.2"
  code_builder:
    dependency: transitive
    description:
      name: code_builder
      sha256: "0ec10bf4a89e4c613960bf1e8b42c64127021740fb21640c29c909826a5eea3e"
      url: "https://pub.dev"
    source: hosted
    version: "4.10.1"
  collection:
    dependency: "direct main"
    description:
      name: collection
      sha256: "2f5709ae4d3d59dd8f7cd309b4e023046b57d8a6c82130785d2b0e5868084e76"
      url: "https://pub.dev"
    source: hosted
    version: "1.19.1"
  colorfilter_generator:
    dependency: "direct main"
    description:
      name: colorfilter_generator
      sha256: ccc2995e440b1d828d55d99150e7cad64624f3cb4a1e235000de3f93cf10d35c
      url: "https://pub.dev"
    source: hosted
    version: "0.0.8"
  convert:
    dependency: transitive
    description:
      name: convert
      sha256: b30acd5944035672bc15c6b7a8b47d773e41e2f17de064350988c5d02adb1c68
      url: "https://pub.dev"
    source: hosted
    version: "3.1.2"
  cross_file:
    dependency: "direct main"
    description:
      name: cross_file
      sha256: "7caf6a750a0c04effbb52a676dce9a4a592e10ad35c34d6d2d0e4811160d5670"
      url: "https://pub.dev"
    source: hosted
    version: "0.3.4+2"
  crypto:
    dependency: transitive
    description:
      name: crypto
      sha256: "1e445881f28f22d6140f181e07737b22f1e099a5e1ff94b0af2f9e4a463f4855"
      url: "https://pub.dev"
    source: hosted
    version: "3.0.6"
  dart_style:
    dependency: transitive
    description:
      name: dart_style
      sha256: "7856d364b589d1f08986e140938578ed36ed948581fbc3bc9aef1805039ac5ab"
      url: "https://pub.dev"
    source: hosted
    version: "2.3.7"
  fake_async:
    dependency: transitive
    description:
      name: fake_async
      sha256: "6a95e56b2449df2273fd8c45a662d6947ce1ebb7aafe80e550a3f68297f3cacc"
      url: "https://pub.dev"
    source: hosted
    version: "1.3.2"
  ffi:
    dependency: transitive
    description:
      name: ffi
      sha256: "16ed7b077ef01ad6170a3d0c57caa4a112a38d7a2ed5602e0aca9ca6f3d98da6"
      url: "https://pub.dev"
    source: hosted
    version: "2.1.3"
  file:
    dependency: transitive
    description:
      name: file
      sha256: a3b4f84adafef897088c160faf7dfffb7696046cb13ae90b508c2cbc95d3b8d4
      url: "https://pub.dev"
    source: hosted
    version: "7.0.1"
  fixnum:
    dependency: transitive
    description:
      name: fixnum
      sha256: b6dc7065e46c974bc7c5f143080a6764ec7a4be6da1285ececdc37be96de53be
      url: "https://pub.dev"
    source: hosted
    version: "1.1.1"
  flutter:
    dependency: "direct main"
    description: flutter
    source: sdk
    version: "0.0.0"
  flutter_lints:
    dependency: "direct dev"
    description:
      name: flutter_lints
      sha256: "3f41d009ba7172d5ff9be5f6e6e6abb4300e263aab8866d2a0842ed2a70f8f0c"
      url: "https://pub.dev"
    source: hosted
    version: "4.0.0"
  flutter_test:
    dependency: "direct dev"
    description: flutter
    source: sdk
    version: "0.0.0"
  glob:
    dependency: transitive
    description:
      name: glob
      sha256: "0e7014b3b7d4dac1ca4d6114f82bf1782ee86745b9b42a92c9289c23d8a0ab63"
      url: "https://pub.dev"
    source: hosted
    version: "2.1.2"
  image:
    dependency: "direct main"
    description:
      name: image
      sha256: "8346ad4b5173924b5ddddab782fc7d8a6300178c8b1dc427775405a01701c4a6"
      url: "https://pub.dev"
    source: hosted
    version: "4.5.2"
  leak_tracker:
    dependency: transitive
    description:
      name: leak_tracker
      sha256: c35baad643ba394b40aac41080300150a4f08fd0fd6a10378f8f7c6bc161acec
      url: "https://pub.dev"
    source: hosted
    version: "10.0.8"
  leak_tracker_flutter_testing:
    dependency: transitive
    description:
      name: leak_tracker_flutter_testing
      sha256: f8b613e7e6a13ec79cfdc0e97638fddb3ab848452eff057653abd3edba760573
      url: "https://pub.dev"
    source: hosted
    version: "3.0.9"
  leak_tracker_testing:
    dependency: transitive
    description:
      name: leak_tracker_testing
      sha256: "6ba465d5d76e67ddf503e1161d1f4a6bc42306f9d66ca1e8f079a47290fb06d3"
      url: "https://pub.dev"
    source: hosted
    version: "3.0.1"
  lints:
    dependency: transitive
    description:
      name: lints
      sha256: "976c774dd944a42e83e2467f4cc670daef7eed6295b10b36ae8c85bcbf828235"
      url: "https://pub.dev"
    source: hosted
    version: "4.0.0"
  macros:
    dependency: transitive
    description:
      name: macros
      sha256: "1d9e801cd66f7ea3663c45fc708450db1fa57f988142c64289142c9b7ee80656"
      url: "https://pub.dev"
    source: hosted
    version: "0.1.3-main.0"
  matcher:
    dependency: transitive
    description:
      name: matcher
      sha256: dc58c723c3c24bf8d3e2d3ad3f2f9d7bd9cf43ec6feaa64181775e60190153f2
      url: "https://pub.dev"
    source: hosted
    version: "0.12.17"
  material_color_utilities:
    dependency: transitive
    description:
      name: material_color_utilities
      sha256: f7142bb1154231d7ea5f96bc7bde4bda2a0945d2806bb11670e30b850d56bdec
      url: "https://pub.dev"
    source: hosted
    version: "0.11.1"
  matrix2d:
    dependency: transitive
    description:
      name: matrix2d
      sha256: "188718dd3bc2a31e372cfd0791b0f77f4f13ea76164147342cc378d9132949e7"
      url: "https://pub.dev"
    source: hosted
    version: "1.0.4"
  meta:
    dependency: transitive
    description:
      name: meta
      sha256: e3641ec5d63ebf0d9b41bd43201a66e3fc79a65db5f61fc181f04cd27aab950c
      url: "https://pub.dev"
    source: hosted
    version: "1.16.0"
  package_config:
    dependency: transitive
    description:
      name: package_config
      sha256: "92d4488434b520a62570293fbd33bb556c7d49230791c1b4bbd973baf6d2dc67"
      url: "https://pub.dev"
    source: hosted
    version: "2.1.1"
  path:
    dependency: transitive
    description:
      name: path
      sha256: "75cca69d1490965be98c73ceaea117e8a04dd21217b37b292c9ddbec0d955bc5"
      url: "https://pub.dev"
    source: hosted
    version: "1.9.1"
  path_provider:
    dependency: "direct main"
    description:
      name: path_provider
      sha256: "50c5dd5b6e1aaf6fb3a78b33f6aa3afca52bf903a8a5298f53101fdaee55bbcd"
      url: "https://pub.dev"
    source: hosted
    version: "2.1.5"
  path_provider_android:
    dependency: transitive
    description:
      name: path_provider_android
      sha256: "4adf4fd5423ec60a29506c76581bc05854c55e3a0b72d35bb28d661c9686edf2"
      url: "https://pub.dev"
    source: hosted
    version: "2.2.15"
  path_provider_foundation:
    dependency: transitive
    description:
      name: path_provider_foundation
      sha256: "4843174df4d288f5e29185bd6e72a6fbdf5a4a4602717eed565497429f179942"
      url: "https://pub.dev"
    source: hosted
    version: "2.4.1"
  path_provider_linux:
    dependency: transitive
    description:
      name: path_provider_linux
      sha256: f7a1fe3a634fe7734c8d3f2766ad746ae2a2884abe22e241a8b301bf5cac3279
      url: "https://pub.dev"
    source: hosted
    version: "2.2.1"
  path_provider_platform_interface:
    dependency: transitive
    description:
      name: path_provider_platform_interface
      sha256: "88f5779f72ba699763fa3a3b06aa4bf6de76c8e5de842cf6f29e2e06476c2334"
      url: "https://pub.dev"
    source: hosted
    version: "2.1.2"
  path_provider_windows:
    dependency: transitive
    description:
      name: path_provider_windows
      sha256: bd6f00dbd873bfb70d0761682da2b3a2c2fccc2b9e84c495821639601d81afe7
      url: "https://pub.dev"
    source: hosted
    version: "2.3.0"
  petitparser:
    dependency: transitive
    description:
      name: petitparser
      sha256: c15605cd28af66339f8eb6fbe0e541bfe2d1b72d5825efc6598f3e0a31b9ad27
      url: "https://pub.dev"
    source: hosted
    version: "6.0.2"
  pigeon:
    dependency: "direct dev"
    description:
      name: pigeon
      sha256: f938cbea2249d68843f96953da7c787a99960578066492ac5962da1da8cabf67
      url: "https://pub.dev"
    source: hosted
    version: "21.2.0"
  platform:
    dependency: transitive
    description:
      name: platform
      sha256: "5d6b1b0036a5f331ebc77c850ebc8506cbc1e9416c27e59b439f917a902a4984"
      url: "https://pub.dev"
    source: hosted
    version: "3.1.6"
  plugin_platform_interface:
    dependency: transitive
    description:
      name: plugin_platform_interface
      sha256: "4820fbfdb9478b1ebae27888254d445073732dae3d6ea81f0b7e06d5dedc3f02"
      url: "https://pub.dev"
    source: hosted
    version: "2.1.8"
  posix:
    dependency: transitive
    description:
      name: posix
      sha256: a0117dc2167805aa9125b82eee515cc891819bac2f538c83646d355b16f58b9a
      url: "https://pub.dev"
    source: hosted
    version: "6.0.1"
  pub_semver:
    dependency: transitive
    description:
      name: pub_semver
      sha256: "7b3cfbf654f3edd0c6298ecd5be782ce997ddf0e00531b9464b55245185bbbbd"
      url: "https://pub.dev"
    source: hosted
    version: "2.1.5"
  rxdart:
    dependency: "direct main"
    description:
      name: rxdart
      sha256: "5c3004a4a8dbb94bd4bf5412a4def4acdaa12e12f269737a5751369e12d1a962"
      url: "https://pub.dev"
    source: hosted
    version: "0.28.0"
  sky_engine:
    dependency: transitive
    description: flutter
    source: sdk
    version: "0.0.0"
  source_span:
    dependency: transitive
    description:
      name: source_span
      sha256: "254ee5351d6cb365c859e20ee823c3bb479bf4a293c22d17a9f1bf144ce86f7c"
      url: "https://pub.dev"
    source: hosted
    version: "1.10.1"
  stack_trace:
    dependency: transitive
    description:
      name: stack_trace
      sha256: "8b27215b45d22309b5cddda1aa2b19bdfec9df0e765f2de506401c071d38d1b1"
      url: "https://pub.dev"
    source: hosted
    version: "1.12.1"
  stream_channel:
    dependency: transitive
    description:
      name: stream_channel
      sha256: "969e04c80b8bcdf826f8f16579c7b14d780458bd97f56d107d3950fdbeef059d"
      url: "https://pub.dev"
    source: hosted
    version: "2.1.4"
  string_scanner:
    dependency: transitive
    description:
      name: string_scanner
      sha256: "921cd31725b72fe181906c6a94d987c78e3b98c2e205b397ea399d4054872b43"
      url: "https://pub.dev"
    source: hosted
    version: "1.4.1"
  term_glyph:
    dependency: transitive
    description:
      name: term_glyph
      sha256: "7f554798625ea768a7518313e58f83891c7f5024f88e46e7182a4558850a4b8e"
      url: "https://pub.dev"
    source: hosted
    version: "1.2.2"
  test_api:
    dependency: transitive
    description:
      name: test_api
      sha256: fb31f383e2ee25fbbfe06b40fe21e1e458d14080e3c67e7ba0acfde4df4e0bbd
      url: "https://pub.dev"
    source: hosted
    version: "0.7.4"
  typed_data:
    dependency: transitive
    description:
      name: typed_data
      sha256: f9049c039ebfeb4cf7a7104a675823cd72dba8297f264b6637062516699fa006
      url: "https://pub.dev"
    source: hosted
    version: "1.4.0"
  vector_math:
    dependency: transitive
    description:
      name: vector_math
      sha256: "80b3257d1492ce4d091729e3a67a60407d227c27241d6927be0130c98e741803"
      url: "https://pub.dev"
    source: hosted
    version: "2.1.4"
  vm_service:
    dependency: transitive
    description:
      name: vm_service
      sha256: "0968250880a6c5fe7edc067ed0a13d4bae1577fe2771dcf3010d52c4a9d3ca14"
      url: "https://pub.dev"
    source: hosted
    version: "14.3.1"
  watcher:
    dependency: transitive
    description:
      name: watcher
      sha256: "69da27e49efa56a15f8afe8f4438c4ec02eff0a117df1b22ea4aad194fe1c104"
      url: "https://pub.dev"
    source: hosted
    version: "1.1.1"
  web:
    dependency: transitive
    description:
      name: web
      sha256: cd3543bd5798f6ad290ea73d210f423502e71900302dde696f8bff84bf89a1cb
      url: "https://pub.dev"
    source: hosted
    version: "1.1.0"
  xdg_directories:
    dependency: transitive
    description:
      name: xdg_directories
      sha256: "7a3f37b05d989967cdddcbb571f1ea834867ae2faa29725fd085180e0883aa15"
      url: "https://pub.dev"
    source: hosted
    version: "1.1.0"
  xml:
    dependency: transitive
    description:
      name: xml
      sha256: b015a8ad1c488f66851d762d3090a21c600e479dc75e68328c52774040cf9226
      url: "https://pub.dev"
    source: hosted
    version: "6.5.0"
  yaml:
    dependency: transitive
    description:
      name: yaml
      sha256: b9da305ac7c39faa3f030eccd175340f968459dae4af175130b3fc47e40d76ce
      url: "https://pub.dev"
    source: hosted
    version: "3.1.3"
sdks:
  dart: ">=3.7.0-0 <4.0.0"
  flutter: ">=3.24.0"



================================================
FILE: pubspec.yaml
================================================
name: camerawesome
description: Easiest Flutter camera Plugin with builtin UI. Supporting capturing
  images, streaming images, video recording, switch sensors, autofocus, flash,
  filters... on Android and iOS.
version: 2.3.0
homepage: https://Apparence.io
repository: https://github.com/Apparence-io/camera_awesome

environment:
  sdk: '>=3.2.6 <4.0.0'
  flutter: ">=1.10.0"

dependencies:
  carousel_slider: ^5.0.0
  collection: ^1.17.2
  colorfilter_generator: ^0.0.8
  cross_file: ^0.3.3+6
  flutter:
    sdk: flutter
  image: ^4.1.3
  path_provider: ^2.1.1
  rxdart: ^0.28.0
dev_dependencies:
  flutter_lints: ^4.0.0
  flutter_test:
    sdk: flutter
  pigeon: ^21.1.0

# For information on the generic Dart part of this file, see the
# following page: https://dart.dev/tools/pub/pubspec
# The following section is specific to Flutter.
flutter:
  assets:
    - assets/icons/1_1.png
    - assets/icons/16_9.png
    - assets/icons/4_3.png
    - assets/icons/expanded.png
    - assets/icons/minimized.png
  plugin:
    platforms:
      android:
        package: com.apparence.camerawesome.cameraX
        pluginClass: CameraAwesomeX
      ios:
        pluginClass: CamerawesomePlugin
  #To add assets to your plugin package, add an assets section, like this:
  #
  # For details regarding assets in packages, see
  # https://flutter.dev/assets-and-images/#from-packages
  #
  # An image asset can refer to one or more resolution-specific "variants", see
  # https://flutter.dev/assets-and-images/#resolution-aware.
  # To add custom fonts to your plugin package, add a fonts section here,
  # in this "flutter" section. Each entry in this list should have a
  # "family" key with the font family name, and a "fonts" key with a
  # list giving the asset and other descriptors for the font. For
  # example:
  # fonts:
  #   - family: Schyler
  #     fonts:
  #       - asset: fonts/Schyler-Regular.ttf
  #       - asset: fonts/Schyler-Italic.ttf
  #         style: italic
  #   - family: Trajan Pro
  #     fonts:
  #       - asset: fonts/TrajanPro.ttf
  #       - asset: fonts/TrajanPro_Bold.ttf
  #         weight: 700
  #
  # For details regarding fonts in packages, see
  # https://flutter.dev/custom-fonts/#from-packages



================================================
FILE: README.zh.md
================================================
<a href="https://apparence.io">
  <img
    src="https://raw.githubusercontent.com/Apparence-io/camera_awesome/master/docs/img/apparence.png"
    width="100%"
  />
</a>
<div style="margin-top:40px">
  <img
    src="https://raw.githubusercontent.com/Apparence-io/camera_awesome/master/docs/img/preview.png"
    width="100%"
  />
  <img
    src="https://raw.githubusercontent.com/Apparence-io/camera_awesome/master/docs/img/features.png"
    width="100%"
    style="margin-top:32px"
  />
</div>

<a href="https://apparencekit.dev" style="margin-top:32px">
  <img
    src="https://raw.githubusercontent.com/Apparence-io/camera_awesome/master/docs/img/flutter_template.png"
    width="100%"
    alt="ApparenceKit Flutter template to bootstrap your next app"
  />
</a>

This plugin is also available as a template in [ApparenceKit](https://apparencekit.dev).<br>

<br>

# CamerAwesome

<div>
    <a href="https://github.com/Solido/awesome-flutter">
        <img alt="Awesome Flutter" src="https://img.shields.io/badge/Awesome-Flutter-blue.svg?longCache=true&style=for-the-badge" />
    </a>
    <a href="https://github.com/Apparence-io/camera_awesome">
        <img src="https://img.shields.io/github/stars/Apparence-io/camera_awesome.svg?style=for-the-badge&logo=github&colorB=green&label=Stars" alt="Star on Github">
    </a>
    <a href="https://pub.dev/packages/camerawesome">
        <img src="https://img.shields.io/pub/v/camerawesome.svg?style=for-the-badge&label=Pub" alt="Star on Github">
    </a>
</div>

[![en](https://img.shields.io/badge/language-english-cyan.svg)](https://github.com/Apparence-io/CamerAwesome/blob/master/README.md)

📸 简单轻松地在您自己的应用程序中嵌入相机。 <br>
这个 Flutter 插件集成了很棒的 Android / iOS 相机体验。 <br>

<br>
为您提供完全可定制的相机体验。<br>
使用我们出色的内置界面或根据需要对其进行自定义調整。

---

<div style="margin-top:16px;margin-bottom:16px">
  <a href="https://docs.page/Apparence-io/camera_awesome" style="">
    <img
      src="https://raw.githubusercontent.com/Apparence-io/camera_awesome/master/docs/img/doc.png"
      width="100%"
    />
  </a>
</div>

## Native features

Here's all native features that cameraAwesome provides to the flutter side.

| System                                   | Android |  iOS  |
| :--------------------------------------- | :-----: | :---: |
| 🔖 询问权限 | ✅ | ✅ |
| 🎥 录制视频 | ✅ | ✅ |
| 🔈 启用/禁用音频 | ✅ | ✅ |
| 🎞 拍照 | ✅ | ✅ |
| 🌆 照片实时滤镜 | ✅ | ✅ |
| 🌤 曝光度 | ✅ | ✅ |
| 📡 直播图像流 | ✅ | ✅ |
| 🧪 图像分析（条形码扫描等）| ✅ | ✅ |
| 👁 放大 | ✅ | ✅ |
| 📸 闪光支持 | ✅ | ✅ |
| ⌛️ 自动对焦 | ✅ | ✅ |
| 📲 直播切换相机 | ✅ | ✅ |
| 😵‍💫 相机旋转流 | ✅ | ✅ |
| 🤐 后台自动停止 | ✅ | ✅ |
| 🔀 传感器类型切换 | ⛔️ | ✅ |
| 🪞 启用/禁用前置摄像头镜像 | ✅ | ✅ |

---

## 📖&nbsp; 安装使用

### 在你的 pubspec.yaml 中添加插件

```yaml
dependencies:
  camerawesome: ^1.3.0
  ...
```

### 平台设置

- **iOS**

在 `ios/Runner/Info.plist` 中添加这些：

```xml

<key>NSCameraUsageDescription</key>
<string>Your own description</string>

<key>NSMicrophoneUsageDescription</key>
<string>To enable microphone access when recording video</string>

<key>NSLocationWhenInUseUsageDescription</key>
<string>To enable GPS location access for Exif data</string>
```

- **Android**

在 `android/app/build.gradle` 中将最低 SDK 版本更改为 21（或更高）：

```
minSdkVersion 21
```

为了能够拍照或录制视频，您可能需要额外的权限，具体取决于 Android 版本和您要保存它们的位置。
在[官方文档](https://developer.android.com/training/data-storage)中阅读更多相关信息.
> `WRITE_EXTERNAL_STORAGE` 不包含在从 1.4.0 版开始的插件中。


如果您想录制附帶音频的视频，请将此权限添加到您的 `AndroidManifest.xml` 中：

```xml

<manifest xmlns:android="http://schemas.android.com/apk/res/android"
        package="com.example.yourpackage">
  <uses-permission android:name="android.permission.RECORD_AUDIO" />

  <!-- Other declarations -->
</manifest>
```

您可能还想将图片位置保存在 exif 元数据中。 在这种情况下，添加以下权限：

```xml
<manifest xmlns:android="http://schemas.android.com/apk/res/android"
  package="com.example.yourpackage">
  <uses-permission android:name="android.permission.ACCESS_FINE_LOCATION" />
  <uses-permission android:name="android.permission.ACCESS_COARSE_LOCATION" />

  <!-- Other declarations -->
</manifest>
```

<details>
<summary>⚠️ 覆写 Android 依赖</summary>

如果您有冲突，可以覆盖 CamerAwesome 使用的一些依赖项。
更改这些变量以定义您要使用的版本：

```gradle
buildscript {
  ext.kotlin_version = '1.7.10'
  ext {
    // You can override these variables
    compileSdkVersion = 33
    minSdkVersion = 24 // 21 minimum
    playServicesLocationVersion = "20.0.0"
    exifInterfaceVersion = "1.3.4"
  }
  // ...
}
```

仅当您确定自己在做什么时才更改这些变量。

例如，当您与其他插件发生冲突时，设置 Play Services Location 版本可能会有所帮助。
下行显示了这些冲突的示例：

```
java.lang.IncompatibleClassChangeError: Found interface com.google.android.gms.location.ActivityRecognitionClient, but class was expected
```

</details>

### 在你的 Flutter 应用中导入插件

```dart
import 'package:camerawesome/camerawesome_plugin.dart';
```

---

## 👌 很棒的内置界面

只需使用我们的构建器。 <br>
这就是在应用中创建完整相机体验所需的全部内容。

```dart
CameraAwesomeBuilder.awesome(
  saveConfig: SaveConfig.image(
    pathBuilder: _path(),
  ),
  onMediaTap: (mediaCapture) {
    OpenFile.open(mediaCapture.filePath);
  },
),
```

![CamerAwesome default UI](docs/img/base_awesome_ui.jpg)

可以使用各种设置自定义此构建器：

- 一个主题
- 屏幕每个部分的构建器
- 初始相机设置
- 预览定位
- 额外的预览装饰
- 和更多！

这是一个例子：

![Customized UI](docs/img/custom_awesome_ui.jpg)

查看 [完整文档](https://docs.page/Apparence-io/camera_awesome/getting_started/awesome-ui) 以了解更多信息。

---

## 🎨 创建自定义界面

如果 `awesome()` 工厂不够用，您可以使用 `custom()` 代替。

它提供了一个 `builder` 属性，可让您创建自己的相机体验。 <br>

相机预览将在您提供给构建器的内容后面显示。

```dart
CameraAwesomeBuilder.custom(
  saveConfig: SaveConfig.image(pathBuilder: _path()),
  builder: (state, previewSize, previewRect) {
    // create your interface here
  },
)
```

> 在 [文档](https://docs.page/Apparence-io/camera_awesome/getting_started/custom-ui) 中查看更多信息

### 使用自定义构建器

这是我们的构建器方法的定义。

```dart
typedef CameraLayoutBuilder = Widget Function(CameraState cameraState, PreviewSize previewSize, Rect previewRect);
```

<br>
您唯一有权管理相机的是 cameraState。<br>
根据我们的相机状态，您可以使用一些不同的方法。 <br>
`previewSize` 和 `previewRect` 可用于将 UI 放置在相机预览周围或之上。
<br>

#### CamerAwesome 状态如何工作？

使用状态，可以做任何您需要的事情，而无需考虑相机流程<br>
- 在应用程序启动时，我们处于 `PreparingCameraState`<br>
- 然后根据您设置的 initialCaptureMode，您将是 `PhotoCameraState` 或 `VideoCameraState`<br>
- 启动视频将推送 `VideoRecordingCameraState`<br>
- 停止视频将推回 `VideoCameraState`<br>

另外，如果你想使用一些特定的功能，你可以这样写。

```dart
state.when(
  onPhotoMode: (photoState) => photoState.start(),
  onVideoMode: (videoState) => videoState.start(),
  onVideoRecordingMode: (videoState) => videoState.pause(),
);
```

> 在 [文档](https://docs.page/Apparence-io/camera_awesome/getting_started/custom-ui) 查看更多信息

<br>

---

## 🔬 分析模式

使用它来实现：

- 二维码扫描。
- 面部识别。
- 人工智能对象检测。
- 实时视频聊天。
- 还有更多🤩

![Face AI](docs/img/face_ai.gif)

您可以在 `example` 目录中使用 MLKit 示例。
上面的例子来自 `ai_analysis_faces.dart`。 它检测人脸并绘制他们的轮廓。

也可以使用 MLKit 读取条形码：

![Barcode scanning](docs/img/barcode_overlay.gif)

检查 `ai_analysis_barcode.dart` 和 `preview_overlay_example.dart` 以获取示例或查看 [文档](https://docs.page/Apparence-io/camera_awesome/ai_with_mlkit/reading_barcodes)。

### 如何使用它

```dart
CameraAwesomeBuilder.awesome(
  saveConfig: SaveConfig.image(
    pathBuilder: _path(),
  ),
  onImageForAnalysis: analyzeImage,
  imageAnalysisConfig: AnalysisConfig(
        // Android specific options
        androidOptions: const AndroidAnalysisOptions.nv21(
            // Target width (CameraX will chose the closest resolution to this width)
            width: 250,
        ),
        // Wether to start automatically the analysis (true by default)
        autoStart: true,
        // Max frames per second, null for no limit (default)
        maxFramesPerSecond: 20,
    ),
```

> MLkit 推荐安卓使用 nv21 格式。 <br>
> bgra8888 是 iOS 格式
> 对于机器学习，您不需要全分辨率图像（720 或更低的图像就足够了，并且使计算更容易）

在 [文档](https://docs.page/Apparence-io/camera_awesome/ai_with_mlkit/image_analysis_configuration) 中了解有关图像分析配置的更多信息.

另请查看有关如何使用 MLKit [读取条形码](https://docs.page/Apparence-io/camera_awesome/ai_with_mlkit/reading_barcodes) 和 [检测人脸](https://docs.page/Apparence-io/camera_awesome/ai_with_mlkit/detecting_faces) 的详细说明.

⚠️ 在Android 上，部分设备不支持同时进行视频录制和图像分析。

- 如果他们不这样做，图像分析将被忽略。
- 您可以使用 `CameraCharacteristics.isVideoRecordingAndImageAnalysisSupported(Sensors.back)` 检查设备是否具有此功能。

---

## 🐽 更新传感器配置

通过状态，您可以访问 `SensorConfig` 类。


| 函式 | 描述 |
| ------------------ | ---------------------------------------------------- |
| setZoom | 改变缩放 |
| setFlashMode | 在 NONE、ON、AUTO、ALWAYS 之间更改闪光灯 |
| setBrightness | 手动更改亮度级别（最好让这个自动） |
| setMirrorFrontCamera | 为前置摄像头设置镜像 |

所有这些配置都可以通过流进行监听，因此您的 UI 可以根据实际配置自动更新.

<br>

## 🌆 照片实时滤镜

使用内置界面将实时滤镜应用于您的图片：

![Built-in live filters](docs/img/filters.gif)

您还可以选择从一开始就使用特定的过滤器：

```dart
CameraAwesomeBuilder.awesome(
  // other params
  filter: AwesomeFilter.AddictiveRed,
)
```

或者以编程方式设置过滤器：

```dart
CameraAwesomeBuilder.custom(
  builder: (cameraState, previewSize, previewRect) {
    return cameraState.when(
      onPreparingCamera: (state) =>
      const Center(child: CircularProgressIndicator()),
      onPhotoMode: (state) =>
          TakePhotoUI(state, onFilterTap: () {
            state.setFilter(AwesomeFilter.Sierra);
          }),
      onVideoMode: (state) => RecordVideoUI(state, recording: false),
      onVideoRecordingMode: (state) =>
          RecordVideoUI(state, recording: true),
    );
  },
)
```

查看 [文档](https://doc.page/Apparence-io/camera_awesome/widgets/awesome_filters) 中的所有可用过滤器.

<br>

<a href="https://apparence.io">
  <img
    src="https://raw.githubusercontent.com/Apparence-io/camera_awesome/master/docs/img/apparence.png"
    width="100%"
  />
</a>

This plugin is also available as a template in [ApparenceKit](https://apparencekit.dev).<br>

<br>

<a href="https://apparencekit.dev">
  <img
    src="https://raw.githubusercontent.com/Apparence-io/camera_awesome/master/docs/img/flutter_template.png"
    width="100%"
    alt="ApparenceKit Flutter template to bootstrap your next app"
  />
</a>



================================================
FILE: .metadata
================================================
# This file tracks properties of this Flutter project.
# Used by Flutter tool to assess capabilities and perform upgrades etc.
#
# This file should be version controlled and should not be manually edited.

version:
  revision: "17025dd88227cd9532c33fa78f5250d548d87e9a"
  channel: "stable"

project_type: plugin

# Tracks metadata for the flutter migrate command
migration:
  platforms:
    - platform: root
      create_revision: 17025dd88227cd9532c33fa78f5250d548d87e9a
      base_revision: 17025dd88227cd9532c33fa78f5250d548d87e9a
    - platform: android
      create_revision: 17025dd88227cd9532c33fa78f5250d548d87e9a
      base_revision: 17025dd88227cd9532c33fa78f5250d548d87e9a
    - platform: ios
      create_revision: 17025dd88227cd9532c33fa78f5250d548d87e9a
      base_revision: 17025dd88227cd9532c33fa78f5250d548d87e9a

  # User provided section

  # List of Local paths (relative to this file) that should be
  # ignored by the migrate tool.
  #
  # Files that are not part of the templates will be ignored by default.
  unmanaged_files:
    - 'lib/main.dart'
    - 'ios/Runner.xcodeproj/project.pbxproj'



================================================
FILE: android/gradle.properties
================================================
org.gradle.jvmargs=-Xmx1536M
android.enableR8=true
android.useAndroidX=true
android.enableJetifier=true




================================================
FILE: android/.gitignore
================================================
*.iml
.gradle
/local.properties
/.idea/workspace.xml
/.idea/libraries
.DS_Store
/build
/captures



================================================
FILE: android/gradle/wrapper/gradle-wrapper.properties
================================================
distributionBase=GRADLE_USER_HOME
distributionPath=wrapper/dists
distributionUrl=https\://services.gradle.org/distributions/gradle-8.9-bin.zip
networkTimeout=10000
validateDistributionUrl=true
zipStoreBase=GRADLE_USER_HOME
zipStorePath=wrapper/dists



================================================
FILE: android/res/values/strings_en.arb
================================================
{}


================================================
FILE: android/src/main/AndroidManifest.xml
================================================
<manifest xmlns:android="http://schemas.android.com/apk/res/android">

    <uses-permission android:name="android.permission.CAMERA" />

    <application>
        <service android:name=".buttons.PlayerService" />
    </application>
</manifest>



================================================
FILE: android/src/main/kotlin/com/apparence/camerawesome/CamerawesomePlugin.java
================================================
 package com.apparence.camerawesome;

 /**
  * CamerawesomePlugin
  * This plugin recquire android Lolipop version (21) as a min version in your Android's gradle build
  * */
 public class CamerawesomePlugin {

   public static final String TAG = CamerawesomePlugin.class.getName();


 }



================================================
FILE: android/src/main/kotlin/com/apparence/camerawesome/buttons/PhysicalButtonsHandler.kt
================================================
package com.apparence.camerawesome.buttons

import android.os.Handler
import android.os.Looper
import android.os.Message
import io.flutter.plugin.common.EventChannel

class PhysicalButtonsHandler : EventChannel.StreamHandler {
    private var sink: EventChannel.EventSink? = null

    fun buttonPressed(buttonId: Int) {
        when (buttonId) {
            VOLUME_DOWN -> {
                sink?.success("VOLUME_DOWN")
            }
            VOLUME_UP -> {
                sink?.success("VOLUME_UP")
            }
        }
    }

    override fun onListen(arguments: Any?, events: EventChannel.EventSink?) {
        sink = events
    }

    override fun onCancel(arguments: Any?) {
        if (sink != null) {
            sink?.endOfStream()
            sink = null
        }
    }

    companion object {
        const val BROADCAST_VOLUME_BUTTONS = "BROADCAST_VOLUME_BUTTONS"

        const val VOLUME_DOWN = 0
        const val VOLUME_UP = 1
    }
}

class PhysicalButtonMessageHandler(private val buttonsHandler: PhysicalButtonsHandler) :
    Handler(Looper.getMainLooper()) {

    override fun handleMessage(message: Message) {
        buttonsHandler.buttonPressed(message.arg1)
    }
}


================================================
FILE: android/src/main/kotlin/com/apparence/camerawesome/buttons/PlayerService.kt
================================================
package com.apparence.camerawesome.buttons

import android.app.Service
import android.content.Intent
import android.os.Build
import android.os.IBinder
import android.os.Message
import android.os.Messenger
import android.support.v4.media.session.MediaSessionCompat
import android.support.v4.media.session.PlaybackStateCompat
import androidx.media.VolumeProviderCompat


class PlayerService : Service() {
    private var mediaSession: MediaSessionCompat? = null
    private var messenger: Messenger? = null

    override fun onStartCommand(intent: Intent?, flags: Int, startId: Int): Int {
        messenger =
            if (Build.VERSION.SDK_INT >= Build.VERSION_CODES.TIRAMISU) intent!!.extras!!.getParcelable(
                PhysicalButtonsHandler.BROADCAST_VOLUME_BUTTONS, Messenger::class.java
            )!!
            else intent!!.extras!!.getParcelable(PhysicalButtonsHandler.BROADCAST_VOLUME_BUTTONS)!!

        return super.onStartCommand(intent, flags, startId)
    }

    override fun onCreate() {
        super.onCreate()


        mediaSession = MediaSessionCompat(this, "PlayerService")
        mediaSession?.setPlaybackState(
            PlaybackStateCompat.Builder().setState(
                PlaybackStateCompat.STATE_PLAYING, 0, 0f
            ) // Simulate a player which plays something.
                .build()
        )

        val myVolumeProvider: VolumeProviderCompat = object : VolumeProviderCompat(
            VOLUME_CONTROL_RELATIVE,
            100,  /*max volume*/
            50  /*initial volume level*/
        ) {
            override fun onAdjustVolume(direction: Int) {
                /*
                -1 -- volume down
                1 -- volume up
                0 -- volume button released
                 */
                if (direction < 0) {
                    messenger?.send(Message.obtain().apply {
                        arg1 = PhysicalButtonsHandler.VOLUME_DOWN
                    })
                } else if (direction > 0) {
                    messenger?.send(Message.obtain().apply {
                        arg1 = PhysicalButtonsHandler.VOLUME_UP
                    })
                }
            }
        }
        mediaSession?.setPlaybackToRemote(myVolumeProvider)
        mediaSession?.isActive = true
    }

    override fun onBind(intent: Intent): IBinder? {
        return null
    }

    override fun onDestroy() {
        super.onDestroy()
        mediaSession?.release()
    }
}


================================================
FILE: android/src/main/kotlin/com/apparence/camerawesome/cameraX/AnalysisImageConverter.kt
================================================
package com.apparence.camerawesome.cameraX

import android.graphics.ImageFormat
import android.graphics.Rect
import android.graphics.YuvImage
import java.io.ByteArrayOutputStream
import java.nio.ByteBuffer
import kotlin.math.min

class AnalysisImageConverter : AnalysisImageUtils {
    override fun nv21toJpeg(
        nv21Image: AnalysisImageWrapper,
        jpegQuality: Long,
        callback: (Result<AnalysisImageWrapper>) -> Unit
    ) {
        val out = ByteArrayOutputStream()
        val yuv = YuvImage(
            nv21Image.bytes, ImageFormat.NV21,
            nv21Image.width.toInt(), nv21Image.height.toInt(),
            // TODO strides might not always be null
            null
        )
        val success = yuv.compressToJpeg(
            Rect(
                nv21Image.cropRect?.left?.toInt() ?: 0, nv21Image.cropRect?.top?.toInt() ?: 0,
                nv21Image.cropRect?.width?.toInt() ?: nv21Image.width.toInt(),
                nv21Image.cropRect?.height?.toInt() ?: nv21Image.height.toInt(),
            ),
            jpegQuality.toInt(), out
        )
        if (!success) {
            callback(
                Result.failure(
                    Exception(
                        "YuvImage failed to encode jpeg."
                    )
                )
            )
        }
        callback(
            Result.success(
                AnalysisImageWrapper(
                    bytes = out.toByteArray(),
                    width = nv21Image.width,
                    height = nv21Image.height,
                    cropRect = nv21Image.cropRect,
                    format = AnalysisImageFormat.JPEG,
                    planes = null,
                    rotation = nv21Image.rotation
                )
            )
        )
    }

    override fun yuv420toJpeg(
        yuvImage: AnalysisImageWrapper,
        jpegQuality: Long,
        callback: (Result<AnalysisImageWrapper>) -> Unit
    ) {
        yuv420toNv21(yuvImage) { result ->
            result.onSuccess {
                nv21toJpeg(it, jpegQuality, callback)
            }
            result.onFailure {
                callback(Result.failure(it))
            }
        }

        // Below code throws the following:
        // java.lang.IllegalArgumentException: only support ImageFormat.NV21 and ImageFormat.YUY2 for now

//        val allPlanes = ByteArrayOutputStream()
//        yuvImage.planes?.forEach { plane ->
//            plane?.let {
//                allPlanes.write(it.bytes)
//            }
//        }
//
//        val out = ByteArrayOutputStream()
//        val yuv = YuvImage(
//            allPlanes.toByteArray(), ImageFormat.YUV_420_888,
//            yuvImage.width.toInt(), yuvImage.height.toInt(),
//            // TODO strides might not always be null
//            yuvImage.planes?.map { it!!.bytesPerRow.toInt() }?.toIntArray()
//        )
//        val success = yuv.compressToJpeg(
//            Rect(
//                yuvImage.cropRect?.left?.toInt() ?: 0, yuvImage.cropRect?.top?.toInt() ?: 0,
//                yuvImage.cropRect?.width?.toInt() ?: yuvImage.width.toInt(),
//                yuvImage.cropRect?.height?.toInt() ?: yuvImage.height.toInt(),
//            ),
//            jpegQuality.toInt(), out
//        )
//        if (!success) {
//            callback(
//                Result.failure<AnalysisImageWrapper>(
//                    Exception(
//                        "YuvImage failed to encode jpeg."
//                    )
//                )
//            )
//        }
//        callback(
//            Result.success(
//                AnalysisImageWrapper(
//                    bytes = out.toByteArray(),
//                    width = yuvImage.width,
//                    height = yuvImage.height,
//                    cropRect = yuvImage.cropRect,
//                    format = AnalysisImageFormat.JPEG,
//                    planes = null
//                )
//            )
//        )
    }

    override fun yuv420toNv21(
        yuvImage: AnalysisImageWrapper,
        callback: (Result<AnalysisImageWrapper>) -> Unit
    ) {
        val yPlane = yuvImage.planes!![0]!!
        val uPlane = yuvImage.planes[1]!!
        val vPlane = yuvImage.planes[2]!!

        val yBuffer = ByteBuffer.wrap(yPlane.bytes)
        val uBuffer = ByteBuffer.wrap(uPlane.bytes)
        val vBuffer = ByteBuffer.wrap(vPlane.bytes)
        yBuffer.rewind()
        uBuffer.rewind()
        vBuffer.rewind()

        val ySize = yBuffer.remaining()

        var position = 0

        val nv21 = ByteArray(ySize + yuvImage.width.toInt() * yuvImage.height.toInt() / 2)
        // Add the full y buffer to the array. If rowStride > 1, some padding may be skipped.

        // Add the full y buffer to the array. If rowStride > 1, some padding may be skipped.
        for (row in 0 until yuvImage.height) {
            yBuffer[nv21, position, yuvImage.width.toInt()]
            position += yuvImage.width.toInt()
            yBuffer.position(
                min(ySize, yBuffer.position() - yuvImage.width.toInt() + yPlane.bytesPerRow.toInt())
            )
        }

        val chromaHeight: Int = yuvImage.height.toInt() / 2
        val chromaWidth: Int = yuvImage.width.toInt() / 2
        val vRowStride = vPlane.bytesPerRow.toInt()
        val uRowStride = uPlane.bytesPerRow.toInt()
        val vPixelStride = vPlane.bytesPerPixel!!.toInt()
        val uPixelStride = uPlane.bytesPerPixel!!.toInt()

        // Interleave the u and v frames, filling up the rest of the buffer. Use two line buffers to
        // perform faster bulk gets from the byte buffers.

        // Interleave the u and v frames, filling up the rest of the buffer. Use two line buffers to
        // perform faster bulk gets from the byte buffers.
        val vLineBuffer = ByteArray(vRowStride)
        val uLineBuffer = ByteArray(uRowStride)
        for (row in 0 until chromaHeight) {
            vBuffer[vLineBuffer, 0, min(vRowStride, vBuffer.remaining())]
            uBuffer[uLineBuffer, 0, Math.min(uRowStride, uBuffer.remaining())]
            var vLineBufferPosition = 0
            var uLineBufferPosition = 0
            for (col in 0 until chromaWidth) {
                nv21[position++] = vLineBuffer[vLineBufferPosition]
                nv21[position++] = uLineBuffer[uLineBufferPosition]
                vLineBufferPosition += vPixelStride
                uLineBufferPosition += uPixelStride
            }
        }

        callback(
            Result.success(
                AnalysisImageWrapper(
                    bytes = nv21,
                    width = yuvImage.width,
                    height = yuvImage.height,
                    cropRect = yuvImage.cropRect,
                    format = AnalysisImageFormat.NV21,
                    planes = null,
                    rotation = yuvImage.rotation
                )
            )
        )
    }

    override fun bgra8888toJpeg(
        bgra8888image: AnalysisImageWrapper,
        jpegQuality: Long,
        callback: (Result<AnalysisImageWrapper>) -> Unit
    ) {
        try {
            val width = bgra8888image.width.toInt()
            val height = bgra8888image.height.toInt()
            
            // Create a bitmap with ARGB_8888 config (Android's internal format)
            val bitmap = android.graphics.Bitmap.createBitmap(width, height, android.graphics.Bitmap.Config.ARGB_8888)
            
            // Convert BGRA to ARGB by swapping B and R channels
            val bgraBuffer = ByteBuffer.wrap(bgra8888image.bytes)
            val pixels = IntArray(width * height)
            for (i in 0 until width * height) {
                val b = bgraBuffer.get().toInt() and 0xFF
                val g = bgraBuffer.get().toInt() and 0xFF
                val r = bgraBuffer.get().toInt() and 0xFF
                val a = bgraBuffer.get().toInt() and 0xFF
                pixels[i] = (a shl 24) or (r shl 16) or (g shl 8) or b
            }
            bitmap.setPixels(pixels, 0, width, 0, 0, width, height)
            
            // Compress to JPEG
            val outputStream = ByteArrayOutputStream()
            bitmap.compress(android.graphics.Bitmap.CompressFormat.JPEG, jpegQuality.toInt(), outputStream)
            
            // Clean up
            bitmap.recycle()
            
            // Create new AnalysisImageWrapper with JPEG data
            val jpegImage = AnalysisImageWrapper(
                bytes = outputStream.toByteArray(),
                width = bgra8888image.width,
                height = bgra8888image.height,
                rotation = bgra8888image.rotation,
                format = AnalysisImageFormat.JPEG,
                cropRect = bgra8888image.cropRect
            )
            
            callback(Result.success(jpegImage))
        } catch (e: Exception) {
            callback(Result.failure(e))
        }
    }
}


================================================
FILE: android/src/main/kotlin/com/apparence/camerawesome/cameraX/AutoFitPreviewBuilder.kt
================================================
package com.apparence.camerawesome.cameraX

import android.annotation.SuppressLint
import android.content.Context
import android.graphics.Matrix
import android.hardware.display.DisplayManager
import android.util.Size
import android.view.Display
import android.view.Surface
import android.view.TextureView
import android.view.View
import androidx.camera.core.Preview
import androidx.camera.core.impl.PreviewConfig
import java.lang.ref.WeakReference
import java.util.*
import kotlin.math.roundToInt

/**
 * Builder for [Preview] that takes in a [WeakReference] of the view finder and
 * [PreviewConfig], then instantiates a [Preview] which automatically
 * resizes and rotates reacting to config changes.
 *
 * credits to yevhenRoman
 * https://gist.github.com/yevhenRoman/90681822adef43350844464be95d23f1
 */
@SuppressLint("RestrictedApi")
class AutoFitPreviewBuilder private constructor(
    config: PreviewConfig,
    viewFinderRef: WeakReference<TextureView>
) {
    /** Public instance of preview use-case which can be used by consumers of this adapter */
    val useCase: Preview

    /** Internal variable used to keep track of the use-case's output rotation */
    private var bufferRotation: Int = 0

    /** Internal variable used to keep track of the view's rotation */
    private var viewFinderRotation: Int? = null

    /** Internal variable used to keep track of the use-case's output dimension */
    private var bufferDimens: Size = Size(0, 0)

    /** Internal variable used to keep track of the view's dimension */
    private var viewFinderDimens: Size = Size(0, 0)

    /** Internal variable used to keep track of the view's display */
    private var viewFinderDisplay: Int = -1

    private lateinit var displayManager: DisplayManager

    /** We need a display listener for 180 degree device orientation changes */
    private val displayListener = object : DisplayManager.DisplayListener {
        override fun onDisplayAdded(displayId: Int) = Unit
        override fun onDisplayRemoved(displayId: Int) = Unit
        override fun onDisplayChanged(displayId: Int) {
            val viewFinder = viewFinderRef.get() ?: return
            if (displayId != viewFinderDisplay) {
                val display = displayManager.getDisplay(displayId)
                val rotation = getDisplaySurfaceRotation(display)
                updateTransform(viewFinder, rotation, bufferDimens, viewFinderDimens)
            }
        }
    }

    init {
        // Make sure that the view finder reference is valid
        val viewFinder = viewFinderRef.get() ?: throw IllegalArgumentException(
            "Invalid reference to view finder used"
        )

        // Initialize the display and rotation from texture view information
        viewFinderDisplay = viewFinder.display.displayId
        viewFinderRotation = getDisplaySurfaceRotation(viewFinder.display) ?: 0

        // Initialize public use-case with the given config
        useCase = Preview.Builder
            .fromConfig(config)
            .build();

        // Every time the view finder is updated, recompute layout
//        useCase.onPreviewOutputUpdateListener = Preview.OnPreviewOutputUpdateListener {
//            val viewFinder =
//                    viewFinderRef.get() ?: return@OnPreviewOutputUpdateListener
//
//            // To update the SurfaceTexture, we have to remove it and re-add it
//            val parent = viewFinder.parent as ViewGroup
//            parent.removeView(viewFinder)
//            parent.addView(viewFinder, 0)
//
//            viewFinder.surfaceTexture = it.surfaceTexture
//            bufferRotation = it.rotationDegrees
//            val rotation = getDisplaySurfaceRotation(viewFinder.display)
//            updateTransform(viewFinder, rotation, it.textureSize, viewFinderDimens)
//        }

        // Every time the provided texture view changes, recompute layout
        viewFinder.addOnLayoutChangeListener { view, left, top, right, bottom, _, _, _, _ ->
            val viewFinder = view as TextureView
            val newViewFinderDimens = Size(right - left, bottom - top)
            val rotation = getDisplaySurfaceRotation(viewFinder.display)
            updateTransform(viewFinder, rotation, bufferDimens, newViewFinderDimens)
        }

        // Every time the orientation of device changes, recompute layout
        displayManager = viewFinder.context
            .getSystemService(Context.DISPLAY_SERVICE) as DisplayManager
        displayManager.registerDisplayListener(displayListener, null)

        // Remove the display listeners when the view is detached to avoid
        // holding a reference to the View outside of a Fragment.
        // NOTE: Even though using a weak reference should take care of this,
        // we still try to avoid unnecessary calls to the listener this way.
        viewFinder.addOnAttachStateChangeListener(object : View.OnAttachStateChangeListener {
            override fun onViewAttachedToWindow(view: View) {}
            override fun onViewDetachedFromWindow(view: View) {
                displayManager.unregisterDisplayListener(displayListener)
            }
        })
    }

    /** Helper function that fits a camera preview into the given [TextureView] */
    private fun updateTransform(
        textureView: TextureView?, rotation: Int?, newBufferDimens: Size,
        newViewFinderDimens: Size
    ) {
        // This should happen anyway, but now the linter knows
        val textureView = textureView ?: return

        if (rotation == viewFinderRotation &&
            Objects.equals(newBufferDimens, bufferDimens) &&
            Objects.equals(newViewFinderDimens, viewFinderDimens)
        ) {
            // Nothing has changed, no need to transform output again
            return
        }

        if (rotation == null) {
            // Invalid rotation - wait for valid inputs before setting matrix
            return
        } else {
            // Update internal field with new inputs
            viewFinderRotation = rotation
        }

        if (newBufferDimens.width == 0 || newBufferDimens.height == 0) {
            // Invalid buffer dimens - wait for valid inputs before setting matrix
            return
        } else {
            // Update internal field with new inputs
            bufferDimens = newBufferDimens
        }

        if (newViewFinderDimens.width == 0 || newViewFinderDimens.height == 0) {
            // Invalid view finder dimens - wait for valid inputs before setting matrix
            return
        } else {
            // Update internal field with new inputs
            viewFinderDimens = newViewFinderDimens
        }

        val matrix = Matrix()

        // Compute the center of the view finder
        val centerX = viewFinderDimens.width / 2f
        val centerY = viewFinderDimens.height / 2f

        // Correct preview output to account for display rotation
        matrix.postRotate(-viewFinderRotation!!.toFloat(), centerX, centerY)

        // Buffers are rotated relative to the device's 'natural' orientation: swap width and height
        val bufferRatio = bufferDimens.height / bufferDimens.width.toFloat()

        val scaledWidth: Int
        val scaledHeight: Int
        // Match longest sides together -- i.e. apply center-crop transformation
        if (viewFinderDimens.width > viewFinderDimens.height) {
            scaledWidth = viewFinderDimens.width
            scaledHeight = (viewFinderDimens.width / bufferRatio).roundToInt()
        } else {
            scaledHeight = viewFinderDimens.height
            scaledWidth = Math.round(viewFinderDimens.height * bufferRatio)
        }

        // Compute the relative scale value
        val xScale = scaledWidth / viewFinderDimens.width.toFloat()
        val yScale = scaledHeight / viewFinderDimens.height.toFloat()

        // Scale input buffers to fill the view finder
        matrix.preScale(xScale, yScale, centerX, centerY)

        // Finally, apply transformations to our TextureView
        textureView.setTransform(matrix)
    }

    companion object {
        /** Helper function that gets the rotation of a [Display] in degrees */
        fun getDisplaySurfaceRotation(display: Display?) = when (display?.rotation) {
            Surface.ROTATION_0 -> 0
            Surface.ROTATION_90 -> 90
            Surface.ROTATION_180 -> 180
            Surface.ROTATION_270 -> 270
            else -> null
        }

        /**
         * Main entrypoint for users of this class: instantiates the adapter and returns an instance
         * of [Preview] which automatically adjusts in size and rotation to compensate for
         * config changes.
         */
        fun build(config: PreviewConfig, viewFinder: TextureView) =
            AutoFitPreviewBuilder(config, WeakReference(viewFinder)).useCase
    }
}


================================================
FILE: android/src/main/kotlin/com/apparence/camerawesome/cameraX/CameraAwesomeX.kt
================================================
package com.apparence.camerawesome.cameraX

import android.Manifest
import android.annotation.SuppressLint
import android.app.Activity
import android.content.Intent
import android.content.pm.PackageManager
import android.graphics.*
import android.hardware.camera2.CameraCharacteristics
import android.location.Location
import android.os.*
import android.util.Log
import android.util.Rational
import android.util.Size
import androidx.camera.camera2.Camera2Config
import androidx.camera.camera2.interop.ExperimentalCamera2Interop
import androidx.camera.core.*
import androidx.camera.lifecycle.ProcessCameraProvider
import androidx.camera.video.FileOutputOptions
import androidx.camera.video.VideoRecordEvent
import androidx.core.app.ActivityCompat
import androidx.core.content.ContextCompat
import androidx.core.util.Consumer
import androidx.exifinterface.media.ExifInterface
import com.apparence.camerawesome.*
import com.apparence.camerawesome.buttons.PhysicalButtonMessageHandler
import com.apparence.camerawesome.buttons.PhysicalButtonsHandler
import com.apparence.camerawesome.buttons.PlayerService
import com.apparence.camerawesome.models.FlashMode
import com.apparence.camerawesome.sensors.SensorOrientationListener
import com.apparence.camerawesome.utils.isMultiCamSupported
import com.google.android.gms.location.FusedLocationProviderClient
import com.google.android.gms.location.LocationServices
import com.google.android.gms.location.Priority
import com.google.android.gms.tasks.CancellationTokenSource
import io.flutter.embedding.engine.plugins.FlutterPlugin
import io.flutter.embedding.engine.plugins.FlutterPlugin.FlutterPluginBinding
import io.flutter.embedding.engine.plugins.activity.ActivityAware
import io.flutter.embedding.engine.plugins.activity.ActivityPluginBinding
import io.flutter.plugin.common.EventChannel
import io.flutter.view.TextureRegistry
import io.reactivex.rxjava3.disposables.Disposable
import io.reactivex.rxjava3.subjects.BehaviorSubject
import kotlinx.coroutines.*
import java.io.File
import java.io.FileOutputStream
import java.io.IOException
import java.util.concurrent.TimeUnit
import kotlin.coroutines.resume
import kotlin.math.roundToInt


enum class CaptureModes {
    PHOTO, VIDEO, PREVIEW, ANALYSIS_ONLY,
}

class CameraAwesomeX : CameraInterface, FlutterPlugin, ActivityAware {
    private lateinit var physicalButtonHandler: PhysicalButtonsHandler
    private var binding: FlutterPluginBinding? = null
    private var textureRegistry: TextureRegistry? = null
    private var activity: Activity? = null
    private lateinit var imageStreamChannel: EventChannel
    private lateinit var orientationStreamChannel: EventChannel
    private var orientationStreamListener: OrientationStreamListener? = null
    private val sensorOrientationListener: SensorOrientationListener = SensorOrientationListener()

    private lateinit var cameraState: CameraXState
    private val cameraPermissions = CameraPermissions()
    private lateinit var fusedLocationClient: FusedLocationProviderClient
    private var exifPreferences = ExifPreferences(false)
    private var cancellationTokenSource = CancellationTokenSource()
    private var lastRecordedVideos: List<BehaviorSubject<Boolean>>? = null
    private var lastRecordedVideoSubscriptions: MutableList<Disposable>? = null
    private var colorMatrix: List<Double>? = null

    private val noneFilter: List<Double> = listOf(
        1.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        1.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        1.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        1.0,
        0.0
    )

    @SuppressLint("UnsafeOptInUsageError")
    fun configureCameraXLogs() {
        try {
            ProcessCameraProvider.configureInstance(
                CameraXConfig.Builder.fromConfig(Camera2Config.defaultConfig())
                    .setMinimumLoggingLevel(Log.ERROR).build()
            )
        } catch (e: IllegalStateException) {
            // Ignore if trying to configure CameraX more than once
        }
    }

    private fun getCameraProvider(): ProcessCameraProvider {
        configureCameraXLogs()
        val future = ProcessCameraProvider.getInstance(
            activity!!
        )
        return future.get()
    }


    @SuppressLint("RestrictedApi")
    override fun setupCamera(
        sensors: List<PigeonSensor>,
        aspectRatio: String,
        zoom: Double,
        mirrorFrontCamera: Boolean,
        enablePhysicalButton: Boolean,
        flashMode: String,
        captureMode: String,
        enableImageStream: Boolean,
        exifPreferences: ExifPreferences,
        videoOptions: VideoOptions?,
        callback: (Result<Boolean>) -> Unit
    ) {
        if (enablePhysicalButton) {
            val serviceIntent = Intent(activity!!, PlayerService::class.java)
            serviceIntent.putExtra(
                PhysicalButtonsHandler.BROADCAST_VOLUME_BUTTONS,
                Messenger(PhysicalButtonMessageHandler(physicalButtonHandler))
            )
            activity!!.startService(serviceIntent)
        } else {
            activity!!.stopService(Intent(activity!!, PlayerService::class.java))
        }

        val cameraProvider = getCameraProvider()

        val mode = CaptureModes.valueOf(captureMode)
        cameraState = CameraXState(cameraProvider = cameraProvider,
            textureEntries = sensors.mapIndexed { index: Int, pigeonSensor: PigeonSensor ->
                (pigeonSensor.deviceId
                    ?: index.toString()) to textureRegistry!!.createSurfaceTexture()
            }.toMap(),
            sensors = sensors,
            mirrorFrontCamera = mirrorFrontCamera,
            currentCaptureMode = mode,
            enableImageStream = enableImageStream,
            videoOptions = videoOptions?.android,
            videoRecordingQuality = videoOptions?.quality,
            onStreamReady = { state -> state.updateLifecycle(activity!!) }).apply {
            this.updateAspectRatio(aspectRatio)
            this.flashMode = FlashMode.valueOf(flashMode)
            this.enableAudioRecording = videoOptions?.enableAudio ?: true
        }
        this.exifPreferences = exifPreferences
        orientationStreamListener =
            OrientationStreamListener(activity!!, listOf(sensorOrientationListener, cameraState))
        imageStreamChannel.setStreamHandler(cameraState)
        if (mode != CaptureModes.ANALYSIS_ONLY) {
            cameraState.updateLifecycle(activity!!)
            // Zoom should be set after updateLifeCycle
            if (zoom > 0) {
                // TODO Find a better way to set initial zoom than using a postDelayed
                Handler(Looper.getMainLooper()).postDelayed({
                    (cameraState.concurrentCamera?.cameras?.firstOrNull()
                        ?: cameraState.previewCamera)?.cameraControl?.setLinearZoom(zoom.toFloat())
                }, 200)
            }
        }

        callback(Result.success(true))
    }

    override fun checkPermissions(permissions: List<String>): List<String> {
        throw Exception("Not implemented on Android")
    }

    override fun setupImageAnalysisStream(
        format: String, width: Long, maxFramesPerSecond: Double?, autoStart: Boolean
    ) {
        cameraState.apply {
            try {
                imageAnalysisBuilder = ImageAnalysisBuilder.configure(
                    aspectRatio ?: AspectRatio.RATIO_4_3,
                    when (format.uppercase()) {
                        "YUV_420" -> OutputImageFormat.YUV_420_888
                        "NV21" -> OutputImageFormat.NV21
                        "JPEG" -> OutputImageFormat.JPEG
                        "BGRA8888" -> OutputImageFormat.RGBA_8888
                        else -> OutputImageFormat.NV21
                    },
                    executor(activity!!), width,
                    maxFramesPerSecond = maxFramesPerSecond,
                )
                enableImageStream = autoStart
                updateLifecycle(activity!!)
            } catch (e: Exception) {
                Log.e(CamerawesomePlugin.TAG, "error while enable image analysis", e)
            }
        }

    }

    override fun setExifPreferences(
        exifPreferences: ExifPreferences, callback: (Result<Boolean>) -> Unit
    ) {
        if (exifPreferences.saveGPSLocation) {
            val permissions = listOf(
                Manifest.permission.ACCESS_COARSE_LOCATION, Manifest.permission.ACCESS_FINE_LOCATION
            )
            CoroutineScope(Dispatchers.Main).launch {
                if (cameraPermissions.hasPermission(activity!!, permissions)) {
                    this@CameraAwesomeX.exifPreferences = exifPreferences
                    callback(Result.success(true))
                } else {
                    cameraPermissions.requestPermissions(
                        activity!!,
                        permissions,
                        CameraPermissions.PERMISSION_GEOLOC,
                    ) { grantedPermissions ->
                        if (grantedPermissions.isNotEmpty()) {
                            this@CameraAwesomeX.exifPreferences = exifPreferences
                        }
                        callback(Result.success(grantedPermissions.isNotEmpty()))
                    }
                }
            }
        } else {
            this.exifPreferences = exifPreferences
            callback(Result.success(true))
        }
    }

    override fun setFilter(matrix: List<Double>) {
        colorMatrix = matrix
    }

    override fun isVideoRecordingAndImageAnalysisSupported(
        sensor: PigeonSensorPosition, callback: (Result<Boolean>) -> Unit
    ) {
        val cameraSelector =
            if (sensor == PigeonSensorPosition.BACK) CameraSelector.DEFAULT_BACK_CAMERA else CameraSelector.DEFAULT_FRONT_CAMERA

        if (Build.VERSION.SDK_INT >= Build.VERSION_CODES.N) {
            val cameraProvider = ProcessCameraProvider.getInstance(
                activity!!
            ).get()
            callback(
                Result.success(
                    CameraCapabilities.getCameraLevel(
                        cameraSelector, cameraProvider
                    ) == CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL_3
                )
            )
        } else {
            callback(Result.success(false))
        }

    }

    override fun startAnalysis() {
        cameraState.apply {
            enableImageStream = true
            updateLifecycle(activity!!)
        }
    }

    override fun stopAnalysis() {
        cameraState.apply {
            enableImageStream = false
            updateLifecycle(activity!!)
        }
    }

    override fun requestPermissions(
        saveGpsLocation: Boolean, callback: (Result<List<String>>) -> Unit
    ) {
        // On a generic call, don't ask for specific permissions (location, record audio)
        cameraPermissions.requestBasePermissions(
            activity!!,
            saveGps = saveGpsLocation,
            recordAudio = false,
        ) { grantedPermissions ->
            callback(Result.success(grantedPermissions.mapNotNull {
                when (it) {
                    Manifest.permission.ACCESS_COARSE_LOCATION, Manifest.permission.ACCESS_FINE_LOCATION -> CamerAwesomePermission.LOCATION.name.lowercase()
                    Manifest.permission.CAMERA -> CamerAwesomePermission.CAMERA.name.lowercase()
                    Manifest.permission.RECORD_AUDIO -> CamerAwesomePermission.RECORD_AUDIO.name.lowercase()
                    Manifest.permission.WRITE_EXTERNAL_STORAGE -> CamerAwesomePermission.STORAGE.name.lowercase()
                    else -> null
                }
            }))
        }
    }

    private fun getOrientedSize(width: Int, height: Int): Size {
        val portrait = cameraState.portrait
        return Size(
            if (portrait) width else height,
            if (portrait) height else width,
        )
    }

    override fun getPreviewTextureId(cameraPosition: Long): Long {
        return cameraState.textureEntries[cameraPosition.toString()]!!.id()
    }

    /***
     * [fusedLocationClient.getCurrentLocation] takes time, we might want to use
     * [fusedLocationClient.lastLocation] instead to go faster
     */
    @SuppressLint("MissingPermission")
    private fun retrieveLocation(callback: (Location?) -> Unit) {
        if (exifPreferences.saveGPSLocation && ActivityCompat.checkSelfPermission(
                activity!!, Manifest.permission.ACCESS_FINE_LOCATION
            ) == PackageManager.PERMISSION_GRANTED
        ) {
            fusedLocationClient.getCurrentLocation(
                Priority.PRIORITY_HIGH_ACCURACY, cancellationTokenSource.token
            ).addOnCompleteListener {
                if (it.isSuccessful) {
                    callback(it.result)
                } else {
                    if (it.exception != null) {
                        Log.e(
                            CamerawesomePlugin.TAG, "Error finding location", it.exception
                        )
                    }
                    callback(null)
                }
            }
        } else {
            callback(null)
        }
    }

    override fun takePhoto(
        sensors: List<PigeonSensor>, paths: List<String?>, callback: (Result<Boolean>) -> Unit
    ) {
        if (sensors.size != paths.size) {
            throw Exception("sensors and paths must have the same length")
        }
        if (paths.size != cameraState.imageCaptures.size) {
            throw Exception("paths and imageCaptures must have the same length")
        }

        val sensorsMap = sensors.mapIndexed { index, pigeonSensor ->
            pigeonSensor to paths[index]
        }.toMap()
        CoroutineScope(Dispatchers.Main).launch {
            val res: MutableMap<PigeonSensor, Boolean?> =
                sensorsMap.mapValues { null }.toMutableMap()
            for ((index, entry) in sensorsMap.entries.withIndex()) {
                // On Android, path should be specified
                val imageFile = File(entry.value!!)
                imageFile.parentFile?.mkdirs()
                // cameraState.imageCaptures must be in the same order as the sensors / paths lists
                res[entry.key] = takePhotoWith(cameraState.imageCaptures[index], imageFile)
            }
            callback(Result.success(res.all { it.value == true }))
        }
    }

    @SuppressLint("RestrictedApi")
    private suspend fun takePhotoWith(
        imageCapture: ImageCapture, imageFile: File
    ): Boolean = suspendCancellableCoroutine { continuation ->
        val metadata = ImageCapture.Metadata()
        if (cameraState.sensors.size == 1 && cameraState.sensors.first().position == PigeonSensorPosition.FRONT) {
            metadata.isReversedHorizontal = cameraState.mirrorFrontCamera
        }
        val outputFileOptions =
            ImageCapture.OutputFileOptions.Builder(imageFile).setMetadata(metadata).build()
//        for (imageCapture in cameraState.imageCaptures) {
        imageCapture.targetRotation = orientationStreamListener!!.surfaceOrientation
        imageCapture.takePicture(outputFileOptions,
            ContextCompat.getMainExecutor(activity!!),
            object : ImageCapture.OnImageSavedCallback {
                override fun onImageSaved(outputFileResults: ImageCapture.OutputFileResults) {
                    if (colorMatrix != null && noneFilter != colorMatrix) {
                        val exif = ExifInterface(outputFileResults.savedUri!!.path!!)

                        val originalBitmap = BitmapFactory.decodeFile(
                            outputFileResults.savedUri?.path
                        )
                        val bitmapCopy = Bitmap.createBitmap(
                            originalBitmap.width, originalBitmap.height, Bitmap.Config.ARGB_8888
                        )

                        val canvas = Canvas(bitmapCopy)
                        canvas.drawBitmap(originalBitmap, 0f, 0f, Paint().apply {
                            colorFilter = ColorMatrixColorFilter(colorMatrix!!.map { it.toFloat() }
                                .toFloatArray())
                        })

                        try {
                            FileOutputStream(outputFileResults.savedUri?.path).use { out ->
                                bitmapCopy.compress(
                                    Bitmap.CompressFormat.JPEG, 100, out
                                )
                            }
                            exif.saveAttributes()
                        } catch (e: IOException) {
                            e.printStackTrace()
                        }
                    }

                    if (exifPreferences.saveGPSLocation) {
                        retrieveLocation {
                            val exif = ExifInterface(outputFileResults.savedUri!!.path!!)
                            outputFileOptions.metadata.location = it
                            exif.setGpsInfo(it)
//                            Log.d("CAMERAX__EXIF", "GPS info saved ${it?.latitude} ${it?.longitude}")
                            // We need to actually save the exif data to the file system
                            exif.saveAttributes()
                            continuation.resume(true)
                        }
                    } else {
                        if (continuation.isActive) continuation.resume(true)
                    }
                }

                override fun onError(exception: ImageCaptureException) {
                    Log.e(CamerawesomePlugin.TAG, "Error capturing picture", exception)
                    continuation.resume(false)
                }
            })
//        }
    }

    @SuppressLint("RestrictedApi", "MissingPermission")
    override fun recordVideo(
        sensors: List<PigeonSensor>, paths: List<String?>, callback: (Result<Unit>) -> Unit
    ) {
        if (sensors.size != paths.size) {
            throw Exception("sensors and paths must have the same length")
        }
        if (paths.size != cameraState.videoCaptures.size) {
            throw Exception("paths and imageCaptures must have the same length")
        }

        val requests = sensors.mapIndexed { index, pigeonSensor ->
            pigeonSensor to paths[index]
        }.toMap()
        CoroutineScope(Dispatchers.Main).launch {
            var ignoreAudio = false
            if (cameraState.enableAudioRecording) {
                if (!cameraPermissions.hasPermission(
                        activity!!, listOf(Manifest.permission.RECORD_AUDIO)
                    )
                ) {
                    cameraPermissions.requestPermissions(
                        activity!!,
                        listOf(Manifest.permission.RECORD_AUDIO),
                        CameraPermissions.PERMISSION_RECORD_AUDIO,
                    ) {
                        ignoreAudio = it.isEmpty()
                    }
                } else {
                    ignoreAudio = false
                }
            }


            lastRecordedVideoSubscriptions?.forEach { it.dispose() }
            lastRecordedVideos = buildList {
                for (i in (0 until requests.size)) {
                    this.add(BehaviorSubject.create())
                }
            }
            cameraState.recordings = mutableListOf()
            lastRecordedVideoSubscriptions = mutableListOf()
            for ((index, videoCapture) in cameraState.videoCaptures.values.withIndex()) {
                val recordingListener = Consumer<VideoRecordEvent> { event ->
                    when (event) {
                        is VideoRecordEvent.Start -> {
                            Log.d(CamerawesomePlugin.TAG, "Capture Started")
                        }

                        is VideoRecordEvent.Finalize -> {
                            if (!event.hasError()) {
                                Log.d(
                                    CamerawesomePlugin.TAG,
                                    "Video capture succeeded: ${event.outputResults.outputUri}"
                                )
                                lastRecordedVideos!![index].onNext(true)
                            } else {
                                // update app state when the capture failed.
                                cameraState.apply {
                                    recordings?.get(index)?.close()
                                    if (recordings?.all {
                                            it.isClosed
                                        } == true) {
                                        recordings = null
                                    }
                                }
                                Log.e(
                                    CamerawesomePlugin.TAG,
                                    "Video capture ends with error: ${event.error}"
                                )
                                lastRecordedVideos!![index].onNext(false)
                            }
                        }
                    }
                }
                videoCapture.targetRotation = orientationStreamListener!!.surfaceOrientation
                cameraState.recordings!!.add(videoCapture.output.prepareRecording(
                    activity!!, FileOutputOptions.Builder(File(paths[index]!!)).build()
                ).apply { if (cameraState.enableAudioRecording && !ignoreAudio) withAudioEnabled() }
                    .start(cameraState.executor(activity!!), recordingListener))
            }
            callback(Result.success(Unit))
        }
    }

    override fun stopRecordingVideo(callback: (Result<Boolean>) -> Unit) {
        var submitted = false
        for (index in 0 until cameraState.recordings!!.size) {
            val countDownTimer = object : CountDownTimer(5000, 5000) {
                override fun onTick(interval: Long) {}
                override fun onFinish() {
                    if (!submitted) {
                        submitted = true
                        callback(Result.success(false))
                    }
                }
            }
            countDownTimer.start()

            cameraState.recordings!![index].stop()
            lastRecordedVideoSubscriptions!!.add(lastRecordedVideos!![index].subscribe({ it ->
                countDownTimer.cancel()
                if (!submitted) {
                    submitted = true
                    callback(Result.success(it))
                }
            }, { error -> error.printStackTrace() }))
        }
    }

    override fun getFrontSensors(): List<PigeonSensorTypeDevice> {
        TODO("Not yet implemented")
    }

    override fun getBackSensors(): List<PigeonSensorTypeDevice> {
        TODO("Not yet implemented")
    }

    override fun pauseVideoRecording() {
        cameraState.recordings?.forEach { it.pause() }
    }

    override fun resumeVideoRecording() {
        cameraState.recordings?.forEach { it.resume() }
    }

    override fun receivedImageFromStream() {
        cameraState.imageAnalysisBuilder?.lastFrameAnalysisFinished()
    }


    override fun start(): Boolean {
        // Already started on setUp
        return true
    }

    override fun stop(): Boolean {
        orientationStreamListener?.stop()
        cameraState.stop()
        return true
    }

    @SuppressLint("RestrictedApi")
    override fun setFlashMode(mode: String) {
        val flashMode = FlashMode.valueOf(mode)
        cameraState.apply {
            this.flashMode = flashMode
            for (imageCapture in cameraState.imageCaptures) {
                imageCapture.flashMode = when (flashMode) {
                    FlashMode.ALWAYS, FlashMode.ON -> ImageCapture.FLASH_MODE_ON
                    FlashMode.AUTO -> ImageCapture.FLASH_MODE_AUTO
                    else -> ImageCapture.FLASH_MODE_OFF
                }
            }
            (cameraState.concurrentCamera?.cameras?.firstOrNull()
                ?: cameraState.previewCamera)?.cameraControl?.enableTorch(flashMode == FlashMode.ALWAYS)
        }
    }

    override fun handleAutoFocus() {
        focus()
    }

    override fun setZoom(zoom: Double) {
        cameraState.setLinearZoom(zoom.toFloat())
    }

    @SuppressLint("RestrictedApi")
    override fun setSensor(sensors: List<PigeonSensor>) {
        cameraState.apply {
            this.sensors = sensors
            // TODO Make below variables parameters
            // Also reset flash mode and aspect ratio
            this.flashMode = FlashMode.NONE
            this.aspectRatio = null
            this.rational = Rational(3, 4)
            updateLifecycle(activity!!)
        }
    }

    @SuppressLint("RestrictedApi")
    override fun setCorrection(brightness: Double) {
        // TODO brightness calculation might not be the same as before CameraX
        val range = (cameraState.concurrentCamera?.cameras?.firstOrNull()
            ?: cameraState.previewCamera!!).cameraInfo.exposureState.exposureCompensationRange
        val actualBrightnessValue = brightness * (range.upper - range.lower) + range.lower
        cameraState.previewCamera?.cameraControl?.setExposureCompensationIndex(
            actualBrightnessValue.roundToInt()
        )
    }

    /**
     * This method must be called after bindToLifecycle has been called
     *
     * @return the max zoom ratio
     */
    override fun getMaxZoom(): Double {
        return cameraState.maxZoomRatio
    }

    /**
     * This method must be called after bindToLifecycle has been called
     *
     * @return the min zoom ratio
     */
    override fun getMinZoom(): Double {
        return cameraState.minZoomRatio
    }

    fun convertLinearToRatio(linear: Double): Double {
        // TODO Not sure if this is correct
        return linear * getMaxZoom() / getMinZoom()
    }

    @Deprecated("Use focusOnPoint instead")
    fun focus() {
        val autoFocusPoint = SurfaceOrientedMeteringPointFactory(1f, 1f).createPoint(.5f, .5f)
        try {
            val autoFocusAction = FocusMeteringAction.Builder(
                autoFocusPoint, FocusMeteringAction.FLAG_AF
            ).apply {
                //start auto-focusing after 2 seconds
                setAutoCancelDuration(2, TimeUnit.SECONDS)
            }.build()
            cameraState.startFocusAndMetering(autoFocusAction)
        } catch (e: CameraInfoUnavailableException) {
            throw e
        }
    }

    @SuppressLint("RestrictedApi")
    override fun focusOnPoint(
        previewSize: PreviewSize,
        x: Double,
        y: Double,
        androidFocusSettings: AndroidFocusSettings?,
    ) {
        val autoCancelDurationInMillis = androidFocusSettings?.autoCancelDurationInMillis ?: 2500L
        val factory: MeteringPointFactory = SurfaceOrientedMeteringPointFactory(
            previewSize.width.toFloat(), previewSize.height.toFloat(),
        )

        val autoFocusPoint = factory.createPoint(x.toFloat(), y.toFloat())
        try {
            (cameraState.concurrentCamera?.cameras?.firstOrNull()
                ?: cameraState.previewCamera!!).cameraControl.startFocusAndMetering(
                FocusMeteringAction.Builder(
                    autoFocusPoint,
                    FocusMeteringAction.FLAG_AF or FocusMeteringAction.FLAG_AE or FocusMeteringAction.FLAG_AWB
                ).apply {
                    if (autoCancelDurationInMillis <= 0) {
                        disableAutoCancel()
                    } else {
                        setAutoCancelDuration(autoCancelDurationInMillis, TimeUnit.MILLISECONDS)
                    }
                }.build()
            )
        } catch (e: CameraInfoUnavailableException) {
            throw e
        }
    }

    @SuppressLint("RestrictedApi")
    override fun setCaptureMode(mode: String) {
        cameraState.apply {
            setCaptureMode(CaptureModes.valueOf(mode))
            updateLifecycle(activity!!)
        }
    }


    @SuppressLint("RestrictedApi")
    @ExperimentalCamera2Interop
    override fun isMultiCamSupported(): Boolean {
        return getCameraProvider().isMultiCamSupported()
    }

    /// Changing the recording audio mode can't be changed once a recording has starded
    override fun setRecordingAudioMode(
        enableAudio: Boolean, callback: (Result<Boolean>) -> Unit
    ) {
        CoroutineScope(Dispatchers.IO).launch {
            cameraPermissions.requestPermissions(
                activity!!,
                listOf(Manifest.permission.RECORD_AUDIO),
                CameraPermissions.PERMISSION_RECORD_AUDIO,
            ) { granted ->
                if (granted.isNotEmpty()) {
                    cameraState.apply {
                        enableAudioRecording = enableAudio
                        // No need to update lifecycle, it will be applied on next recording
                    }
                }
                Dispatchers.Main.run { callback(Result.success(granted.isNotEmpty())) }
            }
        }
    }

    @SuppressLint("RestrictedApi", "UnsafeOptInUsageError")
    override fun availableSizes(): List<PreviewSize> {
        return cameraState.previewSizes().map {
            PreviewSize(
                width = it.width.toDouble(), height = it.height.toDouble()
            )
        }
    }

    override fun refresh() {
//        TODO Nothing to do?
    }

    @SuppressLint("RestrictedApi")
    override fun getEffectivPreviewSize(index: Long): PreviewSize {
        val res = cameraState.previews!![index.toInt()].resolutionInfo?.resolution
        return if (res != null) {
            val rota90 = 90
            val rota270 = 270
            when (cameraState.previews!![index.toInt()].resolutionInfo?.rotationDegrees) {
                rota90, rota270 -> {
                    PreviewSize(res.height.toDouble(), res.width.toDouble())
                }

                else -> {
                    PreviewSize(res.width.toDouble(), res.height.toDouble())
                }
            }
        } else {
            PreviewSize(0.0, 0.0)
        }
    }

    @SuppressLint("RestrictedApi")
    override fun setPhotoSize(size: PreviewSize) {
        cameraState.apply {
            photoSize = getOrientedSize(size.width.toInt(), size.height.toInt())
            updateLifecycle(activity!!)
        }
    }

    @SuppressLint("RestrictedApi")
    override fun setPreviewSize(size: PreviewSize) {
        cameraState.apply {
            previewSize = getOrientedSize(size.width.toInt(), size.height.toInt())
            updateLifecycle(activity!!)
        }
    }

    override fun setAspectRatio(aspectRatio: String) {
        cameraState.apply {
            this.updateAspectRatio(aspectRatio)
            updateLifecycle(activity!!)
        }
    }

    override fun setMirrorFrontCamera(mirror: Boolean) {
        cameraState.apply {
            this.mirrorFrontCamera = mirror
            updateLifecycle(activity!!)
        }
    }


    //    FLUTTER ATTACHMENTS
    override fun onAttachedToEngine(binding: FlutterPluginBinding) {
        this.binding = binding
        textureRegistry = binding.textureRegistry
        CameraInterface.setUp(binding.binaryMessenger, this)
        AnalysisImageUtils.setUp(binding.binaryMessenger, AnalysisImageConverter())
        orientationStreamChannel = EventChannel(binding.binaryMessenger, "camerawesome/orientation")
        orientationStreamChannel.setStreamHandler(sensorOrientationListener)
        imageStreamChannel = EventChannel(binding.binaryMessenger, "camerawesome/images")
        EventChannel(binding.binaryMessenger, "camerawesome/permissions").setStreamHandler(
            cameraPermissions
        )
        physicalButtonHandler = PhysicalButtonsHandler()
        EventChannel(binding.binaryMessenger, "camerawesome/physical_button").setStreamHandler(
            physicalButtonHandler
        )
    }

    override fun onDetachedFromEngine(binding: FlutterPluginBinding) {
        this.binding = null
    }

    override fun onAttachedToActivity(binding: ActivityPluginBinding) {
        activity = binding.activity
        binding.addRequestPermissionsResultListener(cameraPermissions)
        fusedLocationClient = LocationServices.getFusedLocationProviderClient(binding.activity)
    }

    override fun onDetachedFromActivityForConfigChanges() {
        activity = null
    }

    override fun onReattachedToActivityForConfigChanges(binding: ActivityPluginBinding) {
        activity = binding.activity
        binding.addRequestPermissionsResultListener(cameraPermissions)
    }

    override fun onDetachedFromActivity() {
        activity = null
        cancellationTokenSource.cancel()
        cameraPermissions.onCancel(null)
    }

}


================================================
FILE: android/src/main/kotlin/com/apparence/camerawesome/cameraX/CameraCapabilities.kt
================================================
package com.apparence.camerawesome.cameraX

import android.hardware.camera2.CameraCharacteristics
import android.os.Build
import android.util.Log
import androidx.camera.camera2.interop.Camera2CameraInfo
import androidx.camera.camera2.interop.ExperimentalCamera2Interop
import androidx.camera.core.CameraSelector
import androidx.camera.lifecycle.ProcessCameraProvider

class CameraCapabilities {
    companion object {
        @androidx.annotation.OptIn(ExperimentalCamera2Interop::class)
        fun getCameraLevel(
            cameraSelector: CameraSelector,
            cameraProvider: ProcessCameraProvider
        ): Int {
            if (Build.VERSION.SDK_INT >= Build.VERSION_CODES.N) {
                return cameraSelector.filter(cameraProvider.availableCameraInfos).firstOrNull()
                    ?.let { Camera2CameraInfo.from(it) }
                    ?.getCameraCharacteristic(CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL)
                    ?: CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL_LIMITED
            }
            return CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL_LEGACY
        }
    }
}


================================================
FILE: android/src/main/kotlin/com/apparence/camerawesome/cameraX/CameraPermissions.kt
================================================
package com.apparence.camerawesome.cameraX

import android.Manifest
import android.app.Activity
import android.content.Context
import android.content.pm.PackageManager
import android.util.Log
import androidx.core.app.ActivityCompat
import androidx.core.content.ContextCompat
import com.apparence.camerawesome.exceptions.PermissionNotDeclaredException
import io.flutter.plugin.common.EventChannel
import io.flutter.plugin.common.EventChannel.EventSink
import io.flutter.plugin.common.PluginRegistry.RequestPermissionsResultListener
import kotlinx.coroutines.CoroutineScope
import kotlinx.coroutines.Dispatchers
import kotlinx.coroutines.launch
import java.util.*
import kotlin.coroutines.Continuation
import kotlin.coroutines.resume
import kotlin.coroutines.suspendCoroutine

class CameraPermissions : EventChannel.StreamHandler, RequestPermissionsResultListener {
    private var permissionGranted = false
    private var events: EventSink? = null
    private var callbacks: MutableList<PermissionRequest> = mutableListOf()

    // ---------------------------------------------
    // EventChannel.StreamHandler
    // ---------------------------------------------
    override fun onListen(arguments: Any?, events: EventSink?) {
        this.events = events
    }

    override fun onCancel(arguments: Any?) {
        if (events != null) {
            events!!.endOfStream()
            events = null
        }
    }

    // ---------------------------------------------
    // PluginRegistry.RequestPermissionsResultListener
    // ---------------------------------------------
    override fun onRequestPermissionsResult(
        requestCode: Int, permissions: Array<String>, grantResults: IntArray
    ): Boolean {
        val grantedPermissions = mutableListOf<String>()
        val deniedPermissions = mutableListOf<String>()
        permissionGranted = true
        for (i in permissions.indices) {
            if (grantResults[i] == PackageManager.PERMISSION_GRANTED) {
                grantedPermissions.add(permissions[i])
            } else {
                permissionGranted = false
                deniedPermissions.add(permissions[i])
            }
        }
        val toRemove = mutableListOf<PermissionRequest>()
        for (c in callbacks) {
            if (c.permissionsAsked.containsAll(permissions.toList()) && permissions.toList()
                    .containsAll(c.permissionsAsked)
            ) {
                c.callback(grantedPermissions, deniedPermissions)
                toRemove.add(c)
            }
        }
        callbacks.removeAll(toRemove)

        if (events != null) {
            Log.d(
                TAG,
                "_onRequestPermissionsResult: granted " + java.lang.String.join(", ", *permissions)
            )
            events!!.success(permissionGranted)
        } else {
            Log.d(
                TAG, "_onRequestPermissionsResult: received permissions but the EventSink is closed"
            )
        }
        return permissionGranted
    }

    fun requestBasePermissions(
        activity: Activity,
        saveGps: Boolean,
        recordAudio: Boolean,
        callback: (granted: List<String>) -> Unit
    ) {
        val declared = declaredCameraPermissions(activity)
        // Remove declared permissions not required now
        if (!saveGps) {
            declared.remove(Manifest.permission.ACCESS_FINE_LOCATION)
            declared.remove(Manifest.permission.ACCESS_COARSE_LOCATION)
        }
        if (!recordAudio) {
            declared.remove(Manifest.permission.RECORD_AUDIO)
        }
        // Throw exception if permission not declared but required here
        if (saveGps && !declared.contains(Manifest.permission.ACCESS_FINE_LOCATION)) {
            throw PermissionNotDeclaredException(Manifest.permission.ACCESS_FINE_LOCATION)
        }
        if (saveGps && !declared.contains(Manifest.permission.ACCESS_COARSE_LOCATION)) {
            throw PermissionNotDeclaredException(Manifest.permission.ACCESS_COARSE_LOCATION)
        }
        if (recordAudio && !declared.contains(Manifest.permission.RECORD_AUDIO)) {
            throw PermissionNotDeclaredException(Manifest.permission.RECORD_AUDIO)
        }

        // Check if some of the permissions have already been given
        val permissionsToAsk: MutableList<String> = ArrayList()
        val permissionsGranted: MutableList<String> = ArrayList()
        for (permission in declared) {
            if (ContextCompat.checkSelfPermission(
                    activity, permission
                ) != PackageManager.PERMISSION_GRANTED
            ) {
                permissionsToAsk.add(permission)
            } else {
                permissionsGranted.add(permission)
            }
        }
        permissionGranted = permissionsToAsk.size == 0
        if (permissionsToAsk.isEmpty()) {
            callback(permissionsGranted)
        } else {
            // Request the not granted permissions
            CoroutineScope(Dispatchers.IO).launch {
                requestPermissions(activity, permissionsToAsk, PERMISSIONS_MULTIPLE_REQUEST) {
                    callback(permissionsGranted.apply { addAll(it) })
                }
            }
        }
    }


    /**
     * Returns the list of declared camera related permissions
     */
    private fun declaredCameraPermissions(context: Context): MutableList<String> {
        val packageInfo = context.packageManager.getPackageInfo(
            context.packageName, PackageManager.GET_PERMISSIONS
        )
        val permissions = packageInfo.requestedPermissions
        val declaredPermissions = mutableListOf<String>()
        if (permissions.isNullOrEmpty()) return declaredPermissions

        for (perm in permissions) {
            if (allPermissions.contains(perm)) {
                declaredPermissions.add(perm)
            }
        }
        return declaredPermissions
    }

    fun hasPermission(activity: Activity, permissions: List<String>): Boolean {
        var granted = true
        for (p in permissions) {
            if (ContextCompat.checkSelfPermission(
                    activity, p
                ) != PackageManager.PERMISSION_GRANTED
            ) {
                granted = false
                break
            }
        }
        return granted
    }

    suspend fun requestPermissions(
        activity: Activity,
        permissions: List<String>,
        requestCode: Int,
        callback: (denied: List<String>) -> Unit
    ) {
        val result: List<String> = suspendCoroutine { continuation: Continuation<List<String>> ->
            ActivityCompat.requestPermissions(
                activity, permissions.toTypedArray(), requestCode
            )
            callbacks.add(
                PermissionRequest(UUID.randomUUID().toString(),
                    permissions,
                    callback = { granted, _ ->
                        continuation.resume(granted)
                    })
            )
        }
        callback(result)
    }

    companion object {
        private val TAG = CameraPermissions::class.java.name
        const val PERMISSIONS_MULTIPLE_REQUEST = 550
        const val PERMISSION_GEOLOC = 560
        const val PERMISSION_RECORD_AUDIO = 570

        val allPermissions = listOf(
            Manifest.permission.CAMERA,
            Manifest.permission.WRITE_EXTERNAL_STORAGE,
            Manifest.permission.RECORD_AUDIO,
            Manifest.permission.ACCESS_FINE_LOCATION,
            Manifest.permission.ACCESS_COARSE_LOCATION,
        )
    }
}

data class PermissionRequest(
    var id: String,
    val permissionsAsked: List<String>,
    val callback: (permissionsGranted: List<String>, permissionsDenied: List<String>) -> Unit
) {}


================================================
FILE: android/src/main/kotlin/com/apparence/camerawesome/cameraX/CameraXState.kt
================================================
package com.apparence.camerawesome.cameraX

import android.annotation.SuppressLint
import android.app.Activity
import android.hardware.camera2.CameraCharacteristics
import android.util.Log
import android.util.Rational
import android.util.Size
import android.view.Surface
import androidx.camera.camera2.internal.compat.CameraCharacteristicsCompat
import androidx.camera.camera2.internal.compat.quirk.CamcorderProfileResolutionQuirk
import androidx.camera.camera2.interop.Camera2CameraInfo
import androidx.camera.core.*
import androidx.camera.lifecycle.ProcessCameraProvider
import androidx.camera.video.*
import androidx.core.content.ContextCompat
import androidx.lifecycle.LifecycleOwner
import com.apparence.camerawesome.CamerawesomePlugin
import com.apparence.camerawesome.models.FlashMode
import com.apparence.camerawesome.sensors.SensorOrientation
import com.apparence.camerawesome.utils.isMultiCamSupported
import io.flutter.plugin.common.EventChannel
import io.flutter.view.TextureRegistry
import java.util.concurrent.Executor

/// Hold the settings of the camera and use cases in this class and
/// call updateLifecycle() to refresh the state
data class CameraXState(
    private var cameraProvider: ProcessCameraProvider,
    val textureEntries: Map<String, TextureRegistry.SurfaceTextureEntry>,
//    var cameraSelector: CameraSelector,
    var sensors: List<PigeonSensor>,
    var imageCaptures: MutableList<ImageCapture> = mutableListOf(),
    var videoCaptures: MutableMap<PigeonSensor, VideoCapture<Recorder>> = mutableMapOf(),
    var previews: MutableList<Preview>? = null,
    var concurrentCamera: ConcurrentCamera? = null,
    var previewCamera: Camera? = null,
    private var currentCaptureMode: CaptureModes,
    var enableAudioRecording: Boolean = true,
    var recordings: MutableList<Recording>? = null,
    var enableImageStream: Boolean = false,
    var photoSize: Size? = null,
    var previewSize: Size? = null,
    var aspectRatio: Int? = null,
    // Rational is used only in ratio 1:1
    var rational: Rational = Rational(3, 4),
    var flashMode: FlashMode = FlashMode.NONE,
    val onStreamReady: (state: CameraXState) -> Unit,
    var mirrorFrontCamera: Boolean = false,
    val videoRecordingQuality: VideoRecordingQuality?,
    val videoOptions: AndroidVideoOptions?,
) : EventChannel.StreamHandler, SensorOrientation {

    var imageAnalysisBuilder: ImageAnalysisBuilder? = null
    private var imageAnalysis: ImageAnalysis? = null

    private val mainCameraInfos: CameraInfo
        @SuppressLint("RestrictedApi") get() {
            if (previewCamera == null && concurrentCamera == null) {
                throw Exception("Trying to access main camera infos before setting the preview")
            }
            return previewCamera?.cameraInfo ?: concurrentCamera?.cameras?.first()?.cameraInfo!!
        }

    private val mainCameraControl: CameraControl
        @SuppressLint("RestrictedApi") get() {
            if (previewCamera == null && concurrentCamera == null) {
                throw Exception("Trying to access main camera control before setting the preview")
            }
            return previewCamera?.cameraControl
                ?: concurrentCamera?.cameras?.first()?.cameraControl!!
        }

    val maxZoomRatio: Double
        @SuppressLint("RestrictedApi") get() = mainCameraInfos.zoomState.value!!.maxZoomRatio.toDouble()


    val minZoomRatio: Double
        get() = mainCameraInfos.zoomState.value!!.minZoomRatio.toDouble()


    val portrait: Boolean
        get() = mainCameraInfos.sensorRotationDegrees % 180 == 0

    fun executor(activity: Activity): Executor {
        return ContextCompat.getMainExecutor(activity)
    }

    @SuppressLint("RestrictedApi", "UnsafeOptInUsageError")
    fun updateLifecycle(activity: Activity) {
        previews = mutableListOf()
        imageCaptures.clear()
        videoCaptures.clear()
        if (cameraProvider.isMultiCamSupported() && sensors.size > 1) {
            val singleCameraConfigs = mutableListOf<ConcurrentCamera.SingleCameraConfig>()
            var isFirst = true
            for ((index, sensor) in sensors.withIndex()) {
                val useCaseGroupBuilder = UseCaseGroup.Builder()

                val cameraSelector =
                    if (isFirst) CameraSelector.DEFAULT_BACK_CAMERA else CameraSelector.DEFAULT_FRONT_CAMERA
                // TODO Find cameraSelectors based on the sensor and the cameraProvider.availableConcurrentCameraInfos
//                val cameraSelector = CameraSelector.Builder()
//                    .requireLensFacing(if (sensor.position == PigeonSensorPosition.FRONT) CameraSelector.LENS_FACING_FRONT else CameraSelector.LENS_FACING_BACK)
//                    .addCameraFilter(CameraFilter { cameraInfos ->
//                        val list = mutableListOf<CameraInfo>()
//                        cameraInfos.forEach { cameraInfo ->
//                            Camera2CameraInfo.from(cameraInfo).let {
//                                if (it.getPigeonPosition() == sensor.position && (it.getSensorType() == sensor.type || it.getSensorType() == PigeonSensorType.UNKNOWN)) {
//                                    list.add(cameraInfo)
//                                }
//                            }
//                        }
//                        if (list.isEmpty()) {
//                            // If no camera found, only filter based on the sensor position and ignore sensor type
//                            cameraInfos.forEach { cameraInfo ->
//                                Camera2CameraInfo.from(cameraInfo).let {
//                                    if (it.getPigeonPosition() == sensor.position) {
//                                        list.add(cameraInfo)
//                                    }
//                                }
//                            }
//                        }
//                        return@CameraFilter list
//                    })
//                    .build()


                val preview = if (aspectRatio != null) {
                    Preview.Builder().setTargetAspectRatio(aspectRatio!!)
                        .build()
                } else {
                    Preview.Builder().build()
                }

                useCaseGroupBuilder.addUseCase(preview)
                previews!!.add(preview)

                if (currentCaptureMode == CaptureModes.PHOTO) {
                    val imageCapture = ImageCapture.Builder()
//                .setJpegQuality(100)
                        .apply {
                            //photoSize?.let { setTargetResolution(it) }
                            if (rational.denominator != rational.numerator) {
                                setTargetAspectRatio(aspectRatio ?: AspectRatio.RATIO_4_3)
                            }

                            setFlashMode(
                                if (isFirst) when (flashMode) {
                                    FlashMode.ALWAYS, FlashMode.ON -> ImageCapture.FLASH_MODE_ON
                                    FlashMode.AUTO -> ImageCapture.FLASH_MODE_AUTO
                                    else -> ImageCapture.FLASH_MODE_OFF
                                }
                                else ImageCapture.FLASH_MODE_OFF
                            )
                        }.build()
                    useCaseGroupBuilder.addUseCase(imageCapture)
                    imageCaptures.add(imageCapture)
                } else {
                    val videoCapture = buildVideoCapture(videoOptions)
                    useCaseGroupBuilder.addUseCase(videoCapture)
                    videoCaptures[sensor] = videoCapture
                }
                if (isFirst && enableImageStream && imageAnalysisBuilder != null) {
                    imageAnalysis = imageAnalysisBuilder!!.build()
                    useCaseGroupBuilder.addUseCase(imageAnalysis!!)
                } else {
                    imageAnalysis = null
                }

                isFirst = false
                useCaseGroupBuilder.setViewPort(
                    ViewPort.Builder(rational, Surface.ROTATION_0).build()
                )
                singleCameraConfigs.add(
                    ConcurrentCamera.SingleCameraConfig(
                        cameraSelector,
                        useCaseGroupBuilder.build(), activity as LifecycleOwner,
                    )
                )
            }

            cameraProvider.unbindAll()
            previewCamera = null
            concurrentCamera = cameraProvider.bindToLifecycle(
                singleCameraConfigs
            )
            // Only set flash to the main camera (the first one)
            concurrentCamera!!.cameras.first().cameraControl.enableTorch(flashMode == FlashMode.ALWAYS)
        } else {
            val useCaseGroupBuilder = UseCaseGroup.Builder()
            // Handle single camera
            val cameraSelector =
                if (sensors.first().position == PigeonSensorPosition.FRONT) CameraSelector.DEFAULT_FRONT_CAMERA else CameraSelector.DEFAULT_BACK_CAMERA
            // Preview
            if (currentCaptureMode != CaptureModes.ANALYSIS_ONLY) {
                previews!!.add(
                    if (aspectRatio != null) {
                        Preview.Builder().setTargetAspectRatio(aspectRatio!!)
                            .build()
                    } else {
                        Preview.Builder().build()
                    }
                )

                previews!!.first().setSurfaceProvider(
                    surfaceProvider(executor(activity), sensors.first().deviceId ?: "0")
                )
                useCaseGroupBuilder.addUseCase(previews!!.first())
            }

            if (currentCaptureMode == CaptureModes.PHOTO) {
                val imageCapture = ImageCapture.Builder()
//                .setJpegQuality(100)
                    .apply {
                        //photoSize?.let { setTargetResolution(it) }
                        if (rational.denominator != rational.numerator) {
                            setTargetAspectRatio(aspectRatio ?: AspectRatio.RATIO_4_3)
                        }
                        setFlashMode(
                            when (flashMode) {
                                FlashMode.ALWAYS, FlashMode.ON -> ImageCapture.FLASH_MODE_ON
                                FlashMode.AUTO -> ImageCapture.FLASH_MODE_AUTO
                                else -> ImageCapture.FLASH_MODE_OFF
                            }
                        )
                    }.build()
                useCaseGroupBuilder.addUseCase(imageCapture)
                imageCaptures.add(imageCapture)
            } else if (currentCaptureMode == CaptureModes.VIDEO) {
                val videoCapture = buildVideoCapture(videoOptions)
                useCaseGroupBuilder.addUseCase(videoCapture)
                videoCaptures[sensors.first()] = videoCapture
            }


            val addAnalysisUseCase = enableImageStream && imageAnalysisBuilder != null
            val cameraLevel = CameraCapabilities.getCameraLevel(
                cameraSelector, cameraProvider
            )
            cameraProvider.unbindAll()
            if (addAnalysisUseCase) {
                if (currentCaptureMode == CaptureModes.VIDEO && cameraLevel < CameraCharacteristics.INFO_SUPPORTED_HARDWARE_LEVEL_3) {
                    Log.w(
                        CamerawesomePlugin.TAG,
                        "Trying to bind too many use cases for this device (level $cameraLevel), ignoring image analysis"
                    )
                } else {
                    imageAnalysis = imageAnalysisBuilder!!.build()
                    useCaseGroupBuilder.addUseCase(imageAnalysis!!)

                }
            } else {
                imageAnalysis = null
            }
            // TODO Orientation might be wrong, to be verified
            useCaseGroupBuilder.setViewPort(ViewPort.Builder(rational, Surface.ROTATION_0).build())
                .build()

            concurrentCamera = null
            previewCamera = cameraProvider.bindToLifecycle(
                activity as LifecycleOwner,
                cameraSelector,
                useCaseGroupBuilder.build(),
            )
            previewCamera!!.cameraControl.enableTorch(flashMode == FlashMode.ALWAYS)
        }
    }

    private fun buildVideoCapture(videoOptions: AndroidVideoOptions?): VideoCapture<Recorder> {
        val recorderBuilder = Recorder.Builder()
        // Aspect ratio is handled by the setViewPort on the UseCaseGroup
        if (videoRecordingQuality != null) {
            val quality = when (videoRecordingQuality) {
                VideoRecordingQuality.LOWEST -> Quality.LOWEST
                VideoRecordingQuality.SD -> Quality.SD
                VideoRecordingQuality.HD -> Quality.HD
                VideoRecordingQuality.FHD -> Quality.FHD
                VideoRecordingQuality.UHD -> Quality.UHD
                else -> Quality.HIGHEST
            }
            recorderBuilder.setQualitySelector(
                QualitySelector.from(
                    quality,
                    if (videoOptions?.fallbackStrategy == QualityFallbackStrategy.LOWER) FallbackStrategy.lowerQualityOrHigherThan(
                        quality
                    )
                    else FallbackStrategy.higherQualityOrLowerThan(quality)
                )
            )
        }
        if (videoOptions?.bitrate != null) {
            recorderBuilder.setTargetVideoEncodingBitRate(videoOptions.bitrate.toInt())
        }
        val recorder = recorderBuilder.build()
        return VideoCapture.Builder<Recorder>(recorder)
            .setMirrorMode(if (mirrorFrontCamera) MirrorMode.MIRROR_MODE_ON_FRONT_ONLY else MirrorMode.MIRROR_MODE_OFF)
            .build()
    }

    @SuppressLint("RestrictedApi")
    private fun surfaceProvider(executor: Executor, cameraId: String): Preview.SurfaceProvider {
//        Log.d("SurfaceProviderCamX", "Creating surface provider for $cameraId")
        return Preview.SurfaceProvider { request: SurfaceRequest ->
            val resolution = request.resolution
            val texture = textureEntries[cameraId]!!.surfaceTexture()
            texture.setDefaultBufferSize(resolution.width, resolution.height)
            val surface = Surface(texture)
            request.provideSurface(surface, executor) {
//                Log.d("CameraX", "Surface request result: ${it.resultCode}")
                surface.release()
            }
        }
    }

    fun setLinearZoom(zoom: Float) {
        mainCameraControl.setLinearZoom(zoom)
    }

    fun startFocusAndMetering(autoFocusAction: FocusMeteringAction) {
        mainCameraControl.startFocusAndMetering(autoFocusAction)
    }

    fun setCaptureMode(captureMode: CaptureModes) {
        currentCaptureMode = captureMode
        when (currentCaptureMode) {
            CaptureModes.PHOTO -> {
                // Release video related stuff
                videoCaptures.clear()
                recordings?.forEach { it.close() }
                recordings = null

            }

            CaptureModes.VIDEO -> {
                // Release photo related stuff
                imageCaptures.clear()
            }

            else -> {
                // Preview and analysis only modes

                // Release video related stuff
                videoCaptures.clear()
                recordings?.forEach { it.close() }
                recordings = null

                // Release photo related stuff
                imageCaptures.clear()
            }
        }
    }

    @SuppressLint("RestrictedApi", "UnsafeOptInUsageError")
    fun previewSizes(): List<Size> {
        val characteristics = CameraCharacteristicsCompat.toCameraCharacteristicsCompat(
            Camera2CameraInfo.extractCameraCharacteristics(mainCameraInfos),
            Camera2CameraInfo.from(mainCameraInfos).cameraId
        )
        return CamcorderProfileResolutionQuirk(characteristics).supportedResolutions
    }

    fun qualityAvailableSizes(): List<String> {
        val supportedQualities = QualitySelector.getSupportedQualities(mainCameraInfos)
        return supportedQualities.map {
            when (it) {
                Quality.UHD -> {
                    "UHD"
                }

                Quality.HIGHEST -> {
                    "HIGHEST"
                }

                Quality.FHD -> {
                    "FHD"
                }

                Quality.HD -> {
                    "HD"
                }

                Quality.LOWEST -> {
                    "LOWEST"
                }

                Quality.SD -> {
                    "SD"
                }

                else -> {
                    "unknown"
                }
            }
        }
    }

    fun stop() {
        cameraProvider.unbindAll()
    }

    override fun onListen(arguments: Any?, events: EventChannel.EventSink?) {
        val previous = imageAnalysisBuilder?.previewStreamSink
        imageAnalysisBuilder?.previewStreamSink = events
        if (previous == null && events != null) {
            onStreamReady(this)
        }
    }

    override fun onCancel(arguments: Any?) {
        this.imageAnalysisBuilder?.previewStreamSink?.endOfStream()
        this.imageAnalysisBuilder?.previewStreamSink = null
    }

    override fun onOrientationChanged(orientation: Int) {
        imageAnalysis?.targetRotation = when (orientation) {
            in 225 until 315 -> {
                Surface.ROTATION_90
            }

            in 135 until 225 -> {
                Surface.ROTATION_180
            }

            in 45 until 135 -> {
                Surface.ROTATION_270
            }

            else -> {
                Surface.ROTATION_0
            }
        }
    }

    fun updateAspectRatio(newAspectRatio: String) {
        // In CameraX, aspect ratio is an Int. RATIO_4_3 = 0 (default), RATIO_16_9 = 1
        aspectRatio = if (newAspectRatio == "RATIO_16_9") 1 else 0
        rational = when (newAspectRatio) {
            "RATIO_16_9" -> Rational(9, 16)
            "RATIO_1_1" -> Rational(1, 1)
            else -> Rational(3, 4)
        }
    }
}


================================================
FILE: android/src/main/kotlin/com/apparence/camerawesome/cameraX/ImageAnalysisBuilder.kt
================================================
package com.apparence.camerawesome.cameraX

import android.annotation.SuppressLint
import android.graphics.Rect
import android.util.Size
import androidx.camera.core.AspectRatio
import androidx.camera.core.ImageAnalysis
import androidx.camera.core.ImageProxy
import androidx.camera.core.internal.utils.ImageUtil
import com.apparence.camerawesome.utils.ResettableCountDownLatch
import io.flutter.plugin.common.EventChannel
import kotlinx.coroutines.*
import java.util.concurrent.Executor
import kotlin.math.roundToInt
import kotlin.math.roundToLong

enum class OutputImageFormat {
    JPEG, YUV_420_888, NV21, RGBA_8888
}

class ImageAnalysisBuilder private constructor(
    private val format: OutputImageFormat,
    private val width: Int,
    private val height: Int,
    private val executor: Executor,
    var previewStreamSink: EventChannel.EventSink? = null,
    private val maxFramesPerSecond: Double?,
) {
    private var lastImageEmittedTimeStamp: Long? = null
    private var countDownLatch = ResettableCountDownLatch(1)
    fun lastFrameAnalysisFinished() {
        countDownLatch.countDown()
    }

    companion object {
        fun configure(
            aspectRatio: Int,
            format: OutputImageFormat,
            executor: Executor,
            width: Long?,
            maxFramesPerSecond: Double?,
        ): ImageAnalysisBuilder {
            var widthOrDefault = 1024
            if (width != null && width > 0) {
                widthOrDefault = width.toInt()
            }
            val analysisAspectRatio = when (aspectRatio) {
                AspectRatio.RATIO_4_3 -> 4f / 3
                else -> 16f / 9
            }
            val height = widthOrDefault * (1 / analysisAspectRatio)
            val maxFps = if (maxFramesPerSecond == 0.0) null else maxFramesPerSecond
            return ImageAnalysisBuilder(
                format,
                widthOrDefault,
                height.toInt(),
                executor,
                maxFramesPerSecond = maxFps,
            )
        }
    }

    @SuppressLint("RestrictedApi")
    fun build(): ImageAnalysis {
        val outputImageFormat = if (format == OutputImageFormat.RGBA_8888) ImageAnalysis.OUTPUT_IMAGE_FORMAT_RGBA_8888 else ImageAnalysis.OUTPUT_IMAGE_FORMAT_YUV_420_888
        countDownLatch.reset()
        val imageAnalysis = ImageAnalysis.Builder().setTargetResolution(Size(width, height))
            .setBackpressureStrategy(ImageAnalysis.STRATEGY_KEEP_ONLY_LATEST)
            .setOutputImageFormat(outputImageFormat).build()
        imageAnalysis.setAnalyzer(Dispatchers.IO.asExecutor()) { imageProxy ->
            if (previewStreamSink == null) {
                return@setAnalyzer
            }
            when (format) {
                OutputImageFormat.JPEG -> {
                    val jpegImage = ImageUtil.yuvImageToJpegByteArray(
                        imageProxy,
                        Rect(0, 0, imageProxy.width, imageProxy.height),
                        80,
                        imageProxy.imageInfo.rotationDegrees
                    )
                    val imageMap = imageProxyBaseAdapter(imageProxy)
                    imageMap["jpegImage"] = jpegImage
                    imageMap["cropRect"] = cropRect(imageProxy)
                    executor.execute { previewStreamSink?.success(imageMap) }
                }

                OutputImageFormat.YUV_420_888 -> {
                    val planes = imagePlanesAdapter(imageProxy)
                    val imageMap = imageProxyBaseAdapter(imageProxy)
                    imageMap["planes"] = planes
                    imageMap["cropRect"] = cropRect(imageProxy)
                    executor.execute { previewStreamSink?.success(imageMap) }
                }

                OutputImageFormat.NV21 -> {
                    val nv21Image = ImageUtil.yuv_420_888toNv21(imageProxy)
                    val planes = imagePlanesAdapter(imageProxy)
                    val imageMap = imageProxyBaseAdapter(imageProxy)
                    imageMap["nv21Image"] = nv21Image
                    imageMap["planes"] = planes
                    imageMap["cropRect"] = cropRect(imageProxy)
                    executor.execute { previewStreamSink?.success(imageMap) }
                }
                OutputImageFormat.RGBA_8888 -> {
                    val planes = imagePlanesAdapter(imageProxy)
                    val imageMap = imageProxyBaseAdapter(imageProxy)
                    imageMap["planes"] = planes
                    executor.execute { previewStreamSink?.success(imageMap) }
                }
            }
            CoroutineScope(Dispatchers.IO).launch {
                maxFramesPerSecond?.let {
                    if (lastImageEmittedTimeStamp == null) {
                        delay((1000 / it).roundToLong())
                    } else {
                        delay(
                            (1000 / it).roundToInt() - (System.currentTimeMillis() - lastImageEmittedTimeStamp!!)
                        )
                    }
                }
                countDownLatch.await()
                imageProxy.close()
            }
            lastImageEmittedTimeStamp = System.currentTimeMillis()
        }
        return imageAnalysis
    }

    private fun cropRect(imageProxy: ImageProxy): Map<String, Any> {
        return mapOf(
            "left" to imageProxy.cropRect.left,
            "top" to imageProxy.cropRect.top,
            "right" to imageProxy.cropRect.right,
            "bottom" to imageProxy.cropRect.bottom,
        )
    }

    @SuppressLint("RestrictedApi", "UnsafeOptInUsageError")
    private fun imageProxyBaseAdapter(imageProxy: ImageProxy): MutableMap<String, Any> {
        return mutableMapOf(
            "height" to imageProxy.image!!.height,
            "width" to imageProxy.image!!.width,
            "format" to format.name.lowercase(),
            "rotation" to "rotation${imageProxy.imageInfo.rotationDegrees}deg",
        )
    }

    @SuppressLint("RestrictedApi", "UnsafeOptInUsageError")
    private fun imagePlanesAdapter(imageProxy: ImageProxy): List<Map<String, Any>> {
        return imageProxy.image!!.planes.map {
            val byteArray = ByteArray(it.buffer.remaining())
            it.buffer.get(byteArray, 0, byteArray.size)
            mapOf(
                "bytes" to byteArray, "rowStride" to it.rowStride, "pixelStride" to it.pixelStride
            )
        }
    }

}


================================================
FILE: android/src/main/kotlin/com/apparence/camerawesome/cameraX/OrientationStreamListener.kt
================================================
package com.apparence.camerawesome.cameraX

import android.app.Activity
import android.view.OrientationEventListener
import android.view.Surface
import com.apparence.camerawesome.sensors.SensorOrientation

class OrientationStreamListener(
    activity: Activity,
    private var listeners: List<SensorOrientation>
) {
    var currentOrientation: Int = 0
    val surfaceOrientation
        get() = when (currentOrientation) {
            in 225 until 315 -> {
                Surface.ROTATION_90
            }

            in 135 until 225 -> {
                Surface.ROTATION_180
            }

            in 45 until 135 -> {
                Surface.ROTATION_270
            }

            else -> {
                Surface.ROTATION_0
            }
        }

    private val orientationEventListener: OrientationEventListener

    init {
        orientationEventListener =
            object : OrientationEventListener(activity.applicationContext) {
                override fun onOrientationChanged(i: Int) {
                    if (i == ORIENTATION_UNKNOWN) {
                        return
                    }
                    currentOrientation = (i + 45) / 90 * 90
                    if (currentOrientation == 360) currentOrientation = 0
                    for (listener in listeners) {
                        listener.onOrientationChanged(currentOrientation)
                    }
                }
            }
        orientationEventListener.enable()
    }

    fun stop() {
        orientationEventListener.disable()
    }
}


================================================
FILE: android/src/main/kotlin/com/apparence/camerawesome/cameraX/Pigeon.kt
================================================
// Autogenerated from Pigeon (v9.2.5), do not edit directly.
// See also: https://pub.dev/packages/pigeon

package com.apparence.camerawesome.cameraX

import android.util.Log
import io.flutter.plugin.common.BasicMessageChannel
import io.flutter.plugin.common.BinaryMessenger
import io.flutter.plugin.common.MessageCodec
import io.flutter.plugin.common.StandardMessageCodec
import java.io.ByteArrayOutputStream
import java.nio.ByteBuffer

private fun wrapResult(result: Any?): List<Any?> {
  return listOf(result)
}

private fun wrapError(exception: Throwable): List<Any?> {
  if (exception is FlutterError) {
    return listOf(
      exception.code,
      exception.message,
      exception.details
    )
  } else {
    return listOf(
      exception.javaClass.simpleName,
      exception.toString(),
      "Cause: " + exception.cause + ", Stacktrace: " + Log.getStackTraceString(exception)
    )
  }
}

/**
 * Error class for passing custom error details to Flutter via a thrown PlatformException.
 * @property code The error code.
 * @property message The error message.
 * @property details The error details. Must be a datatype supported by the api codec.
 */
class FlutterError (
  val code: String,
  override val message: String? = null,
  val details: Any? = null
) : Throwable()

enum class PigeonSensorPosition(val raw: Int) {
  BACK(0),
  FRONT(1),
  UNKNOWN(2);

  companion object {
    fun ofRaw(raw: Int): PigeonSensorPosition? {
      return values().firstOrNull { it.raw == raw }
    }
  }
}

/**
 * Video recording quality, from [sd] to [uhd], with [highest] and [lowest] to
 * let the device choose the best/worst quality available.
 * [highest] is the default quality.
 *
 * Qualities are defined like this:
 * [sd] < [hd] < [fhd] < [uhd]
 */
enum class VideoRecordingQuality(val raw: Int) {
  LOWEST(0),
  SD(1),
  HD(2),
  FHD(3),
  UHD(4),
  HIGHEST(5);

  companion object {
    fun ofRaw(raw: Int): VideoRecordingQuality? {
      return values().firstOrNull { it.raw == raw }
    }
  }
}

/**
 * If the specified [VideoRecordingQuality] is not available on the device,
 * the [VideoRecordingQuality] will fallback to [higher] or [lower] quality.
 * [higher] is the default fallback strategy.
 */
enum class QualityFallbackStrategy(val raw: Int) {
  HIGHER(0),
  LOWER(1);

  companion object {
    fun ofRaw(raw: Int): QualityFallbackStrategy? {
      return values().firstOrNull { it.raw == raw }
    }
  }
}

enum class CupertinoFileType(val raw: Int) {
  QUICKTIMEMOVIE(0),
  MPEG4(1),
  APPLEM4V(2),
  TYPE3GPP(3),
  TYPE3GPP2(4);

  companion object {
    fun ofRaw(raw: Int): CupertinoFileType? {
      return values().firstOrNull { it.raw == raw }
    }
  }
}

enum class CupertinoCodecType(val raw: Int) {
  H264(0),
  HEVC(1),
  HEVCWITHALPHA(2),
  JPEG(3),
  APPLEPRORES4444(4),
  APPLEPRORES422(5),
  APPLEPRORES422HQ(6),
  APPLEPRORES422LT(7),
  APPLEPRORES422PROXY(8);

  companion object {
    fun ofRaw(raw: Int): CupertinoCodecType? {
      return values().firstOrNull { it.raw == raw }
    }
  }
}

enum class PigeonSensorType(val raw: Int) {
  /**
   * A built-in wide-angle camera.
   *
   * The wide angle sensor is the default sensor for iOS
   */
  WIDEANGLE(0),
  /** A built-in camera with a shorter focal length than that of the wide-angle camera. */
  ULTRAWIDEANGLE(1),
  /** A built-in camera device with a longer focal length than the wide-angle camera. */
  TELEPHOTO(2),
  /**
   * A device that consists of two cameras, one Infrared and one YUV.
   *
   * iOS only
   */
  TRUEDEPTH(3),
  UNKNOWN(4);

  companion object {
    fun ofRaw(raw: Int): PigeonSensorType? {
      return values().firstOrNull { it.raw == raw }
    }
  }
}

enum class CamerAwesomePermission(val raw: Int) {
  STORAGE(0),
  CAMERA(1),
  LOCATION(2),
  RECORD_AUDIO(3);

  companion object {
    fun ofRaw(raw: Int): CamerAwesomePermission? {
      return values().firstOrNull { it.raw == raw }
    }
  }
}

enum class AnalysisImageFormat(val raw: Int) {
  YUV_420(0),
  BGRA8888(1),
  JPEG(2),
  NV21(3),
  UNKNOWN(4);

  companion object {
    fun ofRaw(raw: Int): AnalysisImageFormat? {
      return values().firstOrNull { it.raw == raw }
    }
  }
}

enum class AnalysisRotation(val raw: Int) {
  ROTATION0DEG(0),
  ROTATION90DEG(1),
  ROTATION180DEG(2),
  ROTATION270DEG(3);

  companion object {
    fun ofRaw(raw: Int): AnalysisRotation? {
      return values().firstOrNull { it.raw == raw }
    }
  }
}

/** Generated class from Pigeon that represents data sent in messages. */
data class PreviewSize (
  val width: Double,
  val height: Double

) {
  companion object {
    @Suppress("UNCHECKED_CAST")
    fun fromList(list: List<Any?>): PreviewSize {
      val width = list[0] as Double
      val height = list[1] as Double
      return PreviewSize(width, height)
    }
  }
  fun toList(): List<Any?> {
    return listOf<Any?>(
      width,
      height,
    )
  }
}

/** Generated class from Pigeon that represents data sent in messages. */
data class ExifPreferences (
  val saveGPSLocation: Boolean

) {
  companion object {
    @Suppress("UNCHECKED_CAST")
    fun fromList(list: List<Any?>): ExifPreferences {
      val saveGPSLocation = list[0] as Boolean
      return ExifPreferences(saveGPSLocation)
    }
  }
  fun toList(): List<Any?> {
    return listOf<Any?>(
      saveGPSLocation,
    )
  }
}

/** Generated class from Pigeon that represents data sent in messages. */
data class PigeonSensor (
  val position: PigeonSensorPosition,
  val type: PigeonSensorType,
  val deviceId: String? = null

) {
  companion object {
    @Suppress("UNCHECKED_CAST")
    fun fromList(list: List<Any?>): PigeonSensor {
      val position = PigeonSensorPosition.ofRaw(list[0] as Int)!!
      val type = PigeonSensorType.ofRaw(list[1] as Int)!!
      val deviceId = list[2] as String?
      return PigeonSensor(position, type, deviceId)
    }
  }
  fun toList(): List<Any?> {
    return listOf<Any?>(
      position.raw,
      type.raw,
      deviceId,
    )
  }
}

/**
 * Video recording options. Some of them are specific to each platform.
 *
 * Generated class from Pigeon that represents data sent in messages.
 */
data class VideoOptions (
  /** Enable audio while video recording */
  val enableAudio: Boolean,
  /** The quality of the video recording, defaults to [VideoRecordingQuality.highest]. */
  val quality: VideoRecordingQuality? = null,
  val android: AndroidVideoOptions? = null,
  val ios: CupertinoVideoOptions? = null

) {
  companion object {
    @Suppress("UNCHECKED_CAST")
    fun fromList(list: List<Any?>): VideoOptions {
      val enableAudio = list[0] as Boolean
      val quality: VideoRecordingQuality? = (list[1] as Int?)?.let {
        VideoRecordingQuality.ofRaw(it)
      }
      val android: AndroidVideoOptions? = (list[2] as List<Any?>?)?.let {
        AndroidVideoOptions.fromList(it)
      }
      val ios: CupertinoVideoOptions? = (list[3] as List<Any?>?)?.let {
        CupertinoVideoOptions.fromList(it)
      }
      return VideoOptions(enableAudio, quality, android, ios)
    }
  }
  fun toList(): List<Any?> {
    return listOf<Any?>(
      enableAudio,
      quality?.raw,
      android?.toList(),
      ios?.toList(),
    )
  }
}

/** Generated class from Pigeon that represents data sent in messages. */
data class AndroidVideoOptions (
  /**
   * The bitrate of the video recording. Only set it if a custom bitrate is
   * desired.
   */
  val bitrate: Long? = null,
  val fallbackStrategy: QualityFallbackStrategy? = null

) {
  companion object {
    @Suppress("UNCHECKED_CAST")
    fun fromList(list: List<Any?>): AndroidVideoOptions {
      val bitrate = list[0].let { if (it is Int) it.toLong() else it as Long? }
      val fallbackStrategy: QualityFallbackStrategy? = (list[1] as Int?)?.let {
        QualityFallbackStrategy.ofRaw(it)
      }
      return AndroidVideoOptions(bitrate, fallbackStrategy)
    }
  }
  fun toList(): List<Any?> {
    return listOf<Any?>(
      bitrate,
      fallbackStrategy?.raw,
    )
  }
}

/** Generated class from Pigeon that represents data sent in messages. */
data class CupertinoVideoOptions (
  /** Specify video file type, defaults to [AVFileTypeQuickTimeMovie]. */
  val fileType: CupertinoFileType? = null,
  /** Specify video codec, defaults to [AVVideoCodecTypeH264]. */
  val codec: CupertinoCodecType? = null,
  /** Specify video fps, defaults to [30]. */
  val fps: Long? = null

) {
  companion object {
    @Suppress("UNCHECKED_CAST")
    fun fromList(list: List<Any?>): CupertinoVideoOptions {
      val fileType: CupertinoFileType? = (list[0] as Int?)?.let {
        CupertinoFileType.ofRaw(it)
      }
      val codec: CupertinoCodecType? = (list[1] as Int?)?.let {
        CupertinoCodecType.ofRaw(it)
      }
      val fps = list[2].let { if (it is Int) it.toLong() else it as Long? }
      return CupertinoVideoOptions(fileType, codec, fps)
    }
  }
  fun toList(): List<Any?> {
    return listOf<Any?>(
      fileType?.raw,
      codec?.raw,
      fps,
    )
  }
}

/** Generated class from Pigeon that represents data sent in messages. */
data class PigeonSensorTypeDevice (
  val sensorType: PigeonSensorType,
  /** A localized device name for display in the user interface. */
  val name: String,
  /** The current exposure ISO value. */
  val iso: Double,
  /** A Boolean value that indicates whether the flash is currently available for use. */
  val flashAvailable: Boolean,
  /** An identifier that uniquely identifies the device. */
  val uid: String

) {
  companion object {
    @Suppress("UNCHECKED_CAST")
    fun fromList(list: List<Any?>): PigeonSensorTypeDevice {
      val sensorType = PigeonSensorType.ofRaw(list[0] as Int)!!
      val name = list[1] as String
      val iso = list[2] as Double
      val flashAvailable = list[3] as Boolean
      val uid = list[4] as String
      return PigeonSensorTypeDevice(sensorType, name, iso, flashAvailable, uid)
    }
  }
  fun toList(): List<Any?> {
    return listOf<Any?>(
      sensorType.raw,
      name,
      iso,
      flashAvailable,
      uid,
    )
  }
}

/** Generated class from Pigeon that represents data sent in messages. */
data class AndroidFocusSettings (
  /**
   * The auto focus will be canceled after the given [autoCancelDurationInMillis].
   * If [autoCancelDurationInMillis] is equals to 0 (or less), the auto focus
   * will **not** be canceled. A manual `focusOnPoint` call will be needed to
   * focus on an other point.
   * Minimal duration of [autoCancelDurationInMillis] is 1000 ms. If set
   * between 0 (exclusive) and 1000 (exclusive), it will be raised to 1000.
   */
  val autoCancelDurationInMillis: Long

) {
  companion object {
    @Suppress("UNCHECKED_CAST")
    fun fromList(list: List<Any?>): AndroidFocusSettings {
      val autoCancelDurationInMillis = list[0].let { if (it is Int) it.toLong() else it as Long }
      return AndroidFocusSettings(autoCancelDurationInMillis)
    }
  }
  fun toList(): List<Any?> {
    return listOf<Any?>(
      autoCancelDurationInMillis,
    )
  }
}

/** Generated class from Pigeon that represents data sent in messages. */
data class PlaneWrapper (
  val bytes: ByteArray,
  val bytesPerRow: Long,
  val bytesPerPixel: Long? = null,
  val width: Long? = null,
  val height: Long? = null

) {
  companion object {
    @Suppress("UNCHECKED_CAST")
    fun fromList(list: List<Any?>): PlaneWrapper {
      val bytes = list[0] as ByteArray
      val bytesPerRow = list[1].let { if (it is Int) it.toLong() else it as Long }
      val bytesPerPixel = list[2].let { if (it is Int) it.toLong() else it as Long? }
      val width = list[3].let { if (it is Int) it.toLong() else it as Long? }
      val height = list[4].let { if (it is Int) it.toLong() else it as Long? }
      return PlaneWrapper(bytes, bytesPerRow, bytesPerPixel, width, height)
    }
  }
  fun toList(): List<Any?> {
    return listOf<Any?>(
      bytes,
      bytesPerRow,
      bytesPerPixel,
      width,
      height,
    )
  }
}

/** Generated class from Pigeon that represents data sent in messages. */
data class CropRectWrapper (
  val left: Long,
  val top: Long,
  val width: Long,
  val height: Long

) {
  companion object {
    @Suppress("UNCHECKED_CAST")
    fun fromList(list: List<Any?>): CropRectWrapper {
      val left = list[0].let { if (it is Int) it.toLong() else it as Long }
      val top = list[1].let { if (it is Int) it.toLong() else it as Long }
      val width = list[2].let { if (it is Int) it.toLong() else it as Long }
      val height = list[3].let { if (it is Int) it.toLong() else it as Long }
      return CropRectWrapper(left, top, width, height)
    }
  }
  fun toList(): List<Any?> {
    return listOf<Any?>(
      left,
      top,
      width,
      height,
    )
  }
}

/** Generated class from Pigeon that represents data sent in messages. */
data class AnalysisImageWrapper (
  val format: AnalysisImageFormat,
  val bytes: ByteArray? = null,
  val width: Long,
  val height: Long,
  val planes: List<PlaneWrapper?>? = null,
  val cropRect: CropRectWrapper? = null,
  val rotation: AnalysisRotation? = null

) {
  companion object {
    @Suppress("UNCHECKED_CAST")
    fun fromList(list: List<Any?>): AnalysisImageWrapper {
      val format = AnalysisImageFormat.ofRaw(list[0] as Int)!!
      val bytes = list[1] as ByteArray?
      val width = list[2].let { if (it is Int) it.toLong() else it as Long }
      val height = list[3].let { if (it is Int) it.toLong() else it as Long }
      val planes = list[4] as List<PlaneWrapper?>?
      val cropRect: CropRectWrapper? = (list[5] as List<Any?>?)?.let {
        CropRectWrapper.fromList(it)
      }
      val rotation: AnalysisRotation? = (list[6] as Int?)?.let {
        AnalysisRotation.ofRaw(it)
      }
      return AnalysisImageWrapper(format, bytes, width, height, planes, cropRect, rotation)
    }
  }
  fun toList(): List<Any?> {
    return listOf<Any?>(
      format.raw,
      bytes,
      width,
      height,
      planes,
      cropRect?.toList(),
      rotation?.raw,
    )
  }
}

@Suppress("UNCHECKED_CAST")
private object AnalysisImageUtilsCodec : StandardMessageCodec() {
  override fun readValueOfType(type: Byte, buffer: ByteBuffer): Any? {
    return when (type) {
      128.toByte() -> {
        return (readValue(buffer) as? List<Any?>)?.let {
          AnalysisImageWrapper.fromList(it)
        }
      }
      129.toByte() -> {
        return (readValue(buffer) as? List<Any?>)?.let {
          CropRectWrapper.fromList(it)
        }
      }
      130.toByte() -> {
        return (readValue(buffer) as? List<Any?>)?.let {
          PlaneWrapper.fromList(it)
        }
      }
      else -> super.readValueOfType(type, buffer)
    }
  }
  override fun writeValue(stream: ByteArrayOutputStream, value: Any?)   {
    when (value) {
      is AnalysisImageWrapper -> {
        stream.write(128)
        writeValue(stream, value.toList())
      }
      is CropRectWrapper -> {
        stream.write(129)
        writeValue(stream, value.toList())
      }
      is PlaneWrapper -> {
        stream.write(130)
        writeValue(stream, value.toList())
      }
      else -> super.writeValue(stream, value)
    }
  }
}

/** Generated interface from Pigeon that represents a handler of messages from Flutter. */
interface AnalysisImageUtils {
  fun nv21toJpeg(nv21Image: AnalysisImageWrapper, jpegQuality: Long, callback: (Result<AnalysisImageWrapper>) -> Unit)
  fun yuv420toJpeg(yuvImage: AnalysisImageWrapper, jpegQuality: Long, callback: (Result<AnalysisImageWrapper>) -> Unit)
  fun yuv420toNv21(yuvImage: AnalysisImageWrapper, callback: (Result<AnalysisImageWrapper>) -> Unit)
  fun bgra8888toJpeg(bgra8888image: AnalysisImageWrapper, jpegQuality: Long, callback: (Result<AnalysisImageWrapper>) -> Unit)

  companion object {
    /** The codec used by AnalysisImageUtils. */
    val codec: MessageCodec<Any?> by lazy {
      AnalysisImageUtilsCodec
    }
    /** Sets up an instance of `AnalysisImageUtils` to handle messages through the `binaryMessenger`. */
    @Suppress("UNCHECKED_CAST")
    fun setUp(binaryMessenger: BinaryMessenger, api: AnalysisImageUtils?) {
      run {
        val channel = BasicMessageChannel<Any?>(binaryMessenger, "dev.flutter.pigeon.AnalysisImageUtils.nv21toJpeg", codec)
        if (api != null) {
          channel.setMessageHandler { message, reply ->
            val args = message as List<Any?>
            val nv21ImageArg = args[0] as AnalysisImageWrapper
            val jpegQualityArg = args[1].let { if (it is Int) it.toLong() else it as Long }
            api.nv21toJpeg(nv21ImageArg, jpegQualityArg) { result: Result<AnalysisImageWrapper> ->
              val error = result.exceptionOrNull()
              if (error != null) {
                reply.reply(wrapError(error))
              } else {
                val data = result.getOrNull()
                reply.reply(wrapResult(data))
              }
            }
          }
        } else {
          channel.setMessageHandler(null)
        }
      }
      run {
        val channel = BasicMessageChannel<Any?>(binaryMessenger, "dev.flutter.pigeon.AnalysisImageUtils.yuv420toJpeg", codec)
        if (api != null) {
          channel.setMessageHandler { message, reply ->
            val args = message as List<Any?>
            val yuvImageArg = args[0] as AnalysisImageWrapper
            val jpegQualityArg = args[1].let { if (it is Int) it.toLong() else it as Long }
            api.yuv420toJpeg(yuvImageArg, jpegQualityArg) { result: Result<AnalysisImageWrapper> ->
              val error = result.exceptionOrNull()
              if (error != null) {
                reply.reply(wrapError(error))
              } else {
                val data = result.getOrNull()
                reply.reply(wrapResult(data))
              }
            }
          }
        } else {
          channel.setMessageHandler(null)
        }
      }
      run {
        val channel = BasicMessageChannel<Any?>(binaryMessenger, "dev.flutter.pigeon.AnalysisImageUtils.yuv420toNv21", codec)
        if (api != null) {
          channel.setMessageHandler { message, reply ->
            val args = message as List<Any?>
            val yuvImageArg = args[0] as AnalysisImageWrapper
            api.yuv420toNv21(yuvImageArg) { result: Result<AnalysisImageWrapper> ->
              val error = result.exceptionOrNull()
              if (error != null) {
                reply.reply(wrapError(error))
              } else {
                val data = result.getOrNull()
                reply.reply(wrapResult(data))
              }
            }
          }
        } else {
          channel.setMessageHandler(null)
        }
      }
      run {
        val channel = BasicMessageChannel<Any?>(binaryMessenger, "dev.flutter.pigeon.AnalysisImageUtils.bgra8888toJpeg", codec)
        if (api != null) {
          channel.setMessageHandler { message, reply ->
            val args = message as List<Any?>
            val bgra8888imageArg = args[0] as AnalysisImageWrapper
            val jpegQualityArg = args[1].let { if (it is Int) it.toLong() else it as Long }
            api.bgra8888toJpeg(bgra8888imageArg, jpegQualityArg) { result: Result<AnalysisImageWrapper> ->
              val error = result.exceptionOrNull()
              if (error != null) {
                reply.reply(wrapError(error))
              } else {
                val data = result.getOrNull()
                reply.reply(wrapResult(data))
              }
            }
          }
        } else {
          channel.setMessageHandler(null)
        }
      }
    }
  }
}
@Suppress("UNCHECKED_CAST")
private object CameraInterfaceCodec : StandardMessageCodec() {
  override fun readValueOfType(type: Byte, buffer: ByteBuffer): Any? {
    return when (type) {
      128.toByte() -> {
        return (readValue(buffer) as? List<Any?>)?.let {
          AndroidFocusSettings.fromList(it)
        }
      }
      129.toByte() -> {
        return (readValue(buffer) as? List<Any?>)?.let {
          AndroidVideoOptions.fromList(it)
        }
      }
      130.toByte() -> {
        return (readValue(buffer) as? List<Any?>)?.let {
          CupertinoVideoOptions.fromList(it)
        }
      }
      131.toByte() -> {
        return (readValue(buffer) as? List<Any?>)?.let {
          ExifPreferences.fromList(it)
        }
      }
      132.toByte() -> {
        return (readValue(buffer) as? List<Any?>)?.let {
          PigeonSensor.fromList(it)
        }
      }
      133.toByte() -> {
        return (readValue(buffer) as? List<Any?>)?.let {
          PigeonSensorTypeDevice.fromList(it)
        }
      }
      134.toByte() -> {
        return (readValue(buffer) as? List<Any?>)?.let {
          PreviewSize.fromList(it)
        }
      }
      135.toByte() -> {
        return (readValue(buffer) as? List<Any?>)?.let {
          PreviewSize.fromList(it)
        }
      }
      136.toByte() -> {
        return (readValue(buffer) as? List<Any?>)?.let {
          VideoOptions.fromList(it)
        }
      }
      else -> super.readValueOfType(type, buffer)
    }
  }
  override fun writeValue(stream: ByteArrayOutputStream, value: Any?)   {
    when (value) {
      is AndroidFocusSettings -> {
        stream.write(128)
        writeValue(stream, value.toList())
      }
      is AndroidVideoOptions -> {
        stream.write(129)
        writeValue(stream, value.toList())
      }
      is CupertinoVideoOptions -> {
        stream.write(130)
        writeValue(stream, value.toList())
      }
      is ExifPreferences -> {
        stream.write(131)
        writeValue(stream, value.toList())
      }
      is PigeonSensor -> {
        stream.write(132)
        writeValue(stream, value.toList())
      }
      is PigeonSensorTypeDevice -> {
        stream.write(133)
        writeValue(stream, value.toList())
      }
      is PreviewSize -> {
        stream.write(134)
        writeValue(stream, value.toList())
      }
      is PreviewSize -> {
        stream.write(135)
        writeValue(stream, value.toList())
      }
      is VideoOptions -> {
        stream.write(136)
        writeValue(stream, value.toList())
      }
      else -> super.writeValue(stream, value)
    }
  }
}

/** Generated interface from Pigeon that represents a handler of messages from Flutter. */
interface CameraInterface {
  fun setupCamera(sensors: List<PigeonSensor>, aspectRatio: String, zoom: Double, mirrorFrontCamera: Boolean, enablePhysicalButton: Boolean, flashMode: String, captureMode: String, enableImageStream: Boolean, exifPreferences: ExifPreferences, videoOptions: VideoOptions?, callback: (Result<Boolean>) -> Unit)
  fun checkPermissions(permissions: List<String>): List<String>
  /**
   * Returns given [CamerAwesomePermission] list (as String). Location permission might be
   * refused but the app should still be able to run.
   */
  fun requestPermissions(saveGpsLocation: Boolean, callback: (Result<List<String>>) -> Unit)
  fun getPreviewTextureId(cameraPosition: Long): Long
  fun takePhoto(sensors: List<PigeonSensor>, paths: List<String?>, callback: (Result<Boolean>) -> Unit)
  fun recordVideo(sensors: List<PigeonSensor>, paths: List<String?>, callback: (Result<Unit>) -> Unit)
  fun pauseVideoRecording()
  fun resumeVideoRecording()
  fun receivedImageFromStream()
  fun stopRecordingVideo(callback: (Result<Boolean>) -> Unit)
  fun getFrontSensors(): List<PigeonSensorTypeDevice>
  fun getBackSensors(): List<PigeonSensorTypeDevice>
  fun start(): Boolean
  fun stop(): Boolean
  fun setFlashMode(mode: String)
  fun handleAutoFocus()
  /**
   * Starts auto focus on a point at ([x], [y]).
   *
   * On Android, you can control after how much time you want to switch back
   * to passive focus mode with [androidFocusSettings].
   */
  fun focusOnPoint(previewSize: PreviewSize, x: Double, y: Double, androidFocusSettings: AndroidFocusSettings?)
  fun setZoom(zoom: Double)
  fun setMirrorFrontCamera(mirror: Boolean)
  fun setSensor(sensors: List<PigeonSensor>)
  fun setCorrection(brightness: Double)
  fun getMinZoom(): Double
  fun getMaxZoom(): Double
  fun setCaptureMode(mode: String)
  fun setRecordingAudioMode(enableAudio: Boolean, callback: (Result<Boolean>) -> Unit)
  fun availableSizes(): List<PreviewSize>
  fun refresh()
  fun getEffectivPreviewSize(index: Long): PreviewSize?
  fun setPhotoSize(size: PreviewSize)
  fun setPreviewSize(size: PreviewSize)
  fun setAspectRatio(aspectRatio: String)
  fun setupImageAnalysisStream(format: String, width: Long, maxFramesPerSecond: Double?, autoStart: Boolean)
  fun setExifPreferences(exifPreferences: ExifPreferences, callback: (Result<Boolean>) -> Unit)
  fun startAnalysis()
  fun stopAnalysis()
  fun setFilter(matrix: List<Double>)
  fun isVideoRecordingAndImageAnalysisSupported(sensor: PigeonSensorPosition, callback: (Result<Boolean>) -> Unit)
  fun isMultiCamSupported(): Boolean

  companion object {
    /** The codec used by CameraInterface. */
    val codec: MessageCodec<Any?> by lazy {
      CameraInterfaceCodec
    }
    /** Sets up an instance of `CameraInterface` to handle messages through the `binaryMessenger`. */
    @Suppress("UNCHECKED_CAST")
    fun setUp(binaryMessenger: BinaryMessenger, api: CameraInterface?) {
      run {
        val channel = BasicMessageChannel<Any?>(binaryMessenger, "dev.flutter.pigeon.CameraInterface.setupCamera", codec)
        if (api != null) {
          channel.setMessageHandler { message, reply ->
            val args = message as List<Any?>
            val sensorsArg = args[0] as List<PigeonSensor>
            val aspectRatioArg = args[1] as String
            val zoomArg = args[2] as Double
            val mirrorFrontCameraArg = args[3] as Boolean
            val enablePhysicalButtonArg = args[4] as Boolean
            val flashModeArg = args[5] as String
            val captureModeArg = args[6] as String
            val enableImageStreamArg = args[7] as Boolean
            val exifPreferencesArg = args[8] as ExifPreferences
            val videoOptionsArg = args[9] as VideoOptions?
            api.setupCamera(sensorsArg, aspectRatioArg, zoomArg, mirrorFrontCameraArg, enablePhysicalButtonArg, flashModeArg, captureModeArg, enableImageStreamArg, exifPreferencesArg, videoOptionsArg) { result: Result<Boolean> ->
              val error = result.exceptionOrNull()
              if (error != null) {
                reply.reply(wrapError(error))
              } else {
                val data = result.getOrNull()
                reply.reply(wrapResult(data))
              }
            }
          }
        } else {
          channel.setMessageHandler(null)
        }
      }
      run {
        val channel = BasicMessageChannel<Any?>(binaryMessenger, "dev.flutter.pigeon.CameraInterface.checkPermissions", codec)
        if (api != null) {
          channel.setMessageHandler { message, reply ->
            val args = message as List<Any?>
            val permissionsArg = args[0] as List<String>
            var wrapped: List<Any?>
            try {
              wrapped = listOf<Any?>(api.checkPermissions(permissionsArg))
            } catch (exception: Throwable) {
              wrapped = wrapError(exception)
            }
            reply.reply(wrapped)
          }
        } else {
          channel.setMessageHandler(null)
        }
      }
      run {
        val channel = BasicMessageChannel<Any?>(binaryMessenger, "dev.flutter.pigeon.CameraInterface.requestPermissions", codec)
        if (api != null) {
          channel.setMessageHandler { message, reply ->
            val args = message as List<Any?>
            val saveGpsLocationArg = args[0] as Boolean
            api.requestPermissions(saveGpsLocationArg) { result: Result<List<String>> ->
              val error = result.exceptionOrNull()
              if (error != null) {
                reply.reply(wrapError(error))
              } else {
                val data = result.getOrNull()
                reply.reply(wrapResult(data))
              }
            }
          }
        } else {
          channel.setMessageHandler(null)
        }
      }
      run {
        val channel = BasicMessageChannel<Any?>(binaryMessenger, "dev.flutter.pigeon.CameraInterface.getPreviewTextureId", codec)
        if (api != null) {
          channel.setMessageHandler { message, reply ->
            val args = message as List<Any?>
            val cameraPositionArg = args[0].let { if (it is Int) it.toLong() else it as Long }
            var wrapped: List<Any?>
            try {
              wrapped = listOf<Any?>(api.getPreviewTextureId(cameraPositionArg))
            } catch (exception: Throwable) {
              wrapped = wrapError(exception)
            }
            reply.reply(wrapped)
          }
        } else {
          channel.setMessageHandler(null)
        }
      }
      run {
        val channel = BasicMessageChannel<Any?>(binaryMessenger, "dev.flutter.pigeon.CameraInterface.takePhoto", codec)
        if (api != null) {
          channel.setMessageHandler { message, reply ->
            val args = message as List<Any?>
            val sensorsArg = args[0] as List<PigeonSensor>
            val pathsArg = args[1] as List<String?>
            api.takePhoto(sensorsArg, pathsArg) { result: Result<Boolean> ->
              val error = result.exceptionOrNull()
              if (error != null) {
                reply.reply(wrapError(error))
              } else {
                val data = result.getOrNull()
                reply.reply(wrapResult(data))
              }
            }
          }
        } else {
          channel.setMessageHandler(null)
        }
      }
      run {
        val channel = BasicMessageChannel<Any?>(binaryMessenger, "dev.flutter.pigeon.CameraInterface.recordVideo", codec)
        if (api != null) {
          channel.setMessageHandler { message, reply ->
            val args = message as List<Any?>
            val sensorsArg = args[0] as List<PigeonSensor>
            val pathsArg = args[1] as List<String?>
            api.recordVideo(sensorsArg, pathsArg) { result: Result<Unit> ->
              val error = result.exceptionOrNull()
              if (error != null) {
                reply.reply(wrapError(error))
              } else {
                reply.reply(wrapResult(null))
              }
            }
          }
        } else {
          channel.setMessageHandler(null)
        }
      }
      run {
        val channel = BasicMessageChannel<Any?>(binaryMessenger, "dev.flutter.pigeon.CameraInterface.pauseVideoRecording", codec)
        if (api != null) {
          channel.setMessageHandler { _, reply ->
            var wrapped: List<Any?>
            try {
              api.pauseVideoRecording()
              wrapped = listOf<Any?>(null)
            } catch (exception: Throwable) {
              wrapped = wrapError(exception)
            }
            reply.reply(wrapped)
          }
        } else {
          channel.setMessageHandler(null)
        }
      }
      run {
        val channel = BasicMessageChannel<Any?>(binaryMessenger, "dev.flutter.pigeon.CameraInterface.resumeVideoRecording", codec)
        if (api != null) {
          channel.setMessageHandler { _, reply ->
            var wrapped: List<Any?>
            try {
              api.resumeVideoRecording()
              wrapped = listOf<Any?>(null)
            } catch (exception: Throwable) {
              wrapped = wrapError(exception)
            }
            reply.reply(wrapped)
          }
        } else {
          channel.setMessageHandler(null)
        }
      }
      run {
        val channel = BasicMessageChannel<Any?>(binaryMessenger, "dev.flutter.pigeon.CameraInterface.receivedImageFromStream", codec)
        if (api != null) {
          channel.setMessageHandler { _, reply ->
            var wrapped: List<Any?>
            try {
              api.receivedImageFromStream()
              wrapped = listOf<Any?>(null)
            } catch (exception: Throwable) {
              wrapped = wrapError(exception)
            }
            reply.reply(wrapped)
          }
        } else {
          channel.setMessageHandler(null)
        }
      }
      run {
        val channel = BasicMessageChannel<Any?>(binaryMessenger, "dev.flutter.pigeon.CameraInterface.stopRecordingVideo", codec)
        if (api != null) {
          channel.setMessageHandler { _, reply ->
            api.stopRecordingVideo() { result: Result<Boolean> ->
              val error = result.exceptionOrNull()
              if (error != null) {
                reply.reply(wrapError(error))
              } else {
                val data = result.getOrNull()
                reply.reply(wrapResult(data))
              }
            }
          }
        } else {
          channel.setMessageHandler(null)
        }
      }
      run {
        val channel = BasicMessageChannel<Any?>(binaryMessenger, "dev.flutter.pigeon.CameraInterface.getFrontSensors", codec)
        if (api != null) {
          channel.setMessageHandler { _, reply ->
            var wrapped: List<Any?>
            try {
              wrapped = listOf<Any?>(api.getFrontSensors())
            } catch (exception: Throwable) {
              wrapped = wrapError(exception)
            }
            reply.reply(wrapped)
          }
        } else {
          channel.setMessageHandler(null)
        }
      }
      run {
        val channel = BasicMessageChannel<Any?>(binaryMessenger, "dev.flutter.pigeon.CameraInterface.getBackSensors", codec)
        if (api != null) {
          channel.setMessageHandler { _, reply ->
            var wrapped: List<Any?>
            try {
              wrapped = listOf<Any?>(api.getBackSensors())
            } catch (exception: Throwable) {
              wrapped = wrapError(exception)
            }
            reply.reply(wrapped)
          }
        } else {
          channel.setMessageHandler(null)
        }
      }
      run {
        val channel = BasicMessageChannel<Any?>(binaryMessenger, "dev.flutter.pigeon.CameraInterface.start", codec)
        if (api != null) {
          channel.setMessageHandler { _, reply ->
            var wrapped: List<Any?>
            try {
              wrapped = listOf<Any?>(api.start())
            } catch (exception: Throwable) {
              wrapped = wrapError(exception)
            }
            reply.reply(wrapped)
          }
        } else {
          channel.setMessageHandler(null)
        }
      }
      run {
        val channel = BasicMessageChannel<Any?>(binaryMessenger, "dev.flutter.pigeon.CameraInterface.stop", codec)
        if (api != null) {
          channel.setMessageHandler { _, reply ->
            var wrapped: List<Any?>
            try {
              wrapped = listOf<Any?>(api.stop())
            } catch (exception: Throwable) {
              wrapped = wrapError(exception)
            }
            reply.reply(wrapped)
          }
        } else {
          channel.setMessageHandler(null)
        }
      }
      run {
        val channel = BasicMessageChannel<Any?>(binaryMessenger, "dev.flutter.pigeon.CameraInterface.setFlashMode", codec)
        if (api != null) {
          channel.setMessageHandler { message, reply ->
            val args = message as List<Any?>
            val modeArg = args[0] as String
            var wrapped: List<Any?>
            try {
              api.setFlashMode(modeArg)
              wrapped = listOf<Any?>(null)
            } catch (exception: Throwable) {
              wrapped = wrapError(exception)
            }
            reply.reply(wrapped)
          }
        } else {
          channel.setMessageHandler(null)
        }
      }
      run {
        val channel = BasicMessageChannel<Any?>(binaryMessenger, "dev.flutter.pigeon.CameraInterface.handleAutoFocus", codec)
        if (api != null) {
          channel.setMessageHandler { _, reply ->
            var wrapped: List<Any?>
            try {
              api.handleAutoFocus()
              wrapped = listOf<Any?>(null)
            } catch (exception: Throwable) {
              wrapped = wrapError(exception)
            }
            reply.reply(wrapped)
          }
        } else {
          channel.setMessageHandler(null)
        }
      }
      run {
        val channel = BasicMessageChannel<Any?>(binaryMessenger, "dev.flutter.pigeon.CameraInterface.focusOnPoint", codec)
        if (api != null) {
          channel.setMessageHandler { message, reply ->
            val args = message as List<Any?>
            val previewSizeArg = args[0] as PreviewSize
            val xArg = args[1] as Double
            val yArg = args[2] as Double
            val androidFocusSettingsArg = args[3] as AndroidFocusSettings?
            var wrapped: List<Any?>
            try {
              api.focusOnPoint(previewSizeArg, xArg, yArg, androidFocusSettingsArg)
              wrapped = listOf<Any?>(null)
            } catch (exception: Throwable) {
              wrapped = wrapError(exception)
            }
            reply.reply(wrapped)
          }
        } else {
          channel.setMessageHandler(null)
        }
      }
      run {
        val channel = BasicMessageChannel<Any?>(binaryMessenger, "dev.flutter.pigeon.CameraInterface.setZoom", codec)
        if (api != null) {
          channel.setMessageHandler { message, reply ->
            val args = message as List<Any?>
            val zoomArg = args[0] as Double
            var wrapped: List<Any?>
            try {
              api.setZoom(zoomArg)
              wrapped = listOf<Any?>(null)
            } catch (exception: Throwable) {
              wrapped = wrapError(exception)
            }
            reply.reply(wrapped)
          }
        } else {
          channel.setMessageHandler(null)
        }
      }
      run {
        val channel = BasicMessageChannel<Any?>(binaryMessenger, "dev.flutter.pigeon.CameraInterface.setMirrorFrontCamera", codec)
        if (api != null) {
          channel.setMessageHandler { message, reply ->
            val args = message as List<Any?>
            val mirrorArg = args[0] as Boolean
            var wrapped: List<Any?>
            try {
              api.setMirrorFrontCamera(mirrorArg)
              wrapped = listOf<Any?>(null)
            } catch (exception: Throwable) {
              wrapped = wrapError(exception)
            }
            reply.reply(wrapped)
          }
        } else {
          channel.setMessageHandler(null)
        }
      }
      run {
        val channel = BasicMessageChannel<Any?>(binaryMessenger, "dev.flutter.pigeon.CameraInterface.setSensor", codec)
        if (api != null) {
          channel.setMessageHandler { message, reply ->
            val args = message as List<Any?>
            val sensorsArg = args[0] as List<PigeonSensor>
            var wrapped: List<Any?>
            try {
              api.setSensor(sensorsArg)
              wrapped = listOf<Any?>(null)
            } catch (exception: Throwable) {
              wrapped = wrapError(exception)
            }
            reply.reply(wrapped)
          }
        } else {
          channel.setMessageHandler(null)
        }
      }
      run {
        val channel = BasicMessageChannel<Any?>(binaryMessenger, "dev.flutter.pigeon.CameraInterface.setCorrection", codec)
        if (api != null) {
          channel.setMessageHandler { message, reply ->
            val args = message as List<Any?>
            val brightnessArg = args[0] as Double
            var wrapped: List<Any?>
            try {
              api.setCorrection(brightnessArg)
              wrapped = listOf<Any?>(null)
            } catch (exception: Throwable) {
              wrapped = wrapError(exception)
            }
            reply.reply(wrapped)
          }
        } else {
          channel.setMessageHandler(null)
        }
      }
      run {
        val channel = BasicMessageChannel<Any?>(binaryMessenger, "dev.flutter.pigeon.CameraInterface.getMinZoom", codec)
        if (api != null) {
          channel.setMessageHandler { _, reply ->
            var wrapped: List<Any?>
            try {
              wrapped = listOf<Any?>(api.getMinZoom())
            } catch (exception: Throwable) {
              wrapped = wrapError(exception)
            }
            reply.reply(wrapped)
          }
        } else {
          channel.setMessageHandler(null)
        }
      }
      run {
        val channel = BasicMessageChannel<Any?>(binaryMessenger, "dev.flutter.pigeon.CameraInterface.getMaxZoom", codec)
        if (api != null) {
          channel.setMessageHandler { _, reply ->
            var wrapped: List<Any?>
            try {
              wrapped = listOf<Any?>(api.getMaxZoom())
            } catch (exception: Throwable) {
              wrapped = wrapError(exception)
            }
            reply.reply(wrapped)
          }
        } else {
          channel.setMessageHandler(null)
        }
      }
      run {
        val channel = BasicMessageChannel<Any?>(binaryMessenger, "dev.flutter.pigeon.CameraInterface.setCaptureMode", codec)
        if (api != null) {
          channel.setMessageHandler { message, reply ->
            val args = message as List<Any?>
            val modeArg = args[0] as String
            var wrapped: List<Any?>
            try {
              api.setCaptureMode(modeArg)
              wrapped = listOf<Any?>(null)
            } catch (exception: Throwable) {
              wrapped = wrapError(exception)
            }
            reply.reply(wrapped)
          }
        } else {
          channel.setMessageHandler(null)
        }
      }
      run {
        val channel = BasicMessageChannel<Any?>(binaryMessenger, "dev.flutter.pigeon.CameraInterface.setRecordingAudioMode", codec)
        if (api != null) {
          channel.setMessageHandler { message, reply ->
            val args = message as List<Any?>
            val enableAudioArg = args[0] as Boolean
            api.setRecordingAudioMode(enableAudioArg) { result: Result<Boolean> ->
              val error = result.exceptionOrNull()
              if (error != null) {
                reply.reply(wrapError(error))
              } else {
                val data = result.getOrNull()
                reply.reply(wrapResult(data))
              }
            }
          }
        } else {
          channel.setMessageHandler(null)
        }
      }
      run {
        val channel = BasicMessageChannel<Any?>(binaryMessenger, "dev.flutter.pigeon.CameraInterface.availableSizes", codec)
        if (api != null) {
          channel.setMessageHandler { _, reply ->
            var wrapped: List<Any?>
            try {
              wrapped = listOf<Any?>(api.availableSizes())
            } catch (exception: Throwable) {
              wrapped = wrapError(exception)
            }
            reply.reply(wrapped)
          }
        } else {
          channel.setMessageHandler(null)
        }
      }
      run {
        val channel = BasicMessageChannel<Any?>(binaryMessenger, "dev.flutter.pigeon.CameraInterface.refresh", codec)
        if (api != null) {
          channel.setMessageHandler { _, reply ->
            var wrapped: List<Any?>
            try {
              api.refresh()
              wrapped = listOf<Any?>(null)
            } catch (exception: Throwable) {
              wrapped = wrapError(exception)
            }
            reply.reply(wrapped)
          }
        } else {
          channel.setMessageHandler(null)
        }
      }
      run {
        val channel = BasicMessageChannel<Any?>(binaryMessenger, "dev.flutter.pigeon.CameraInterface.getEffectivPreviewSize", codec)
        if (api != null) {
          channel.setMessageHandler { message, reply ->
            val args = message as List<Any?>
            val indexArg = args[0].let { if (it is Int) it.toLong() else it as Long }
            var wrapped: List<Any?>
            try {
              wrapped = listOf<Any?>(api.getEffectivPreviewSize(indexArg))
            } catch (exception: Throwable) {
              wrapped = wrapError(exception)
            }
            reply.reply(wrapped)
          }
        } else {
          channel.setMessageHandler(null)
        }
      }
      run {
        val channel = BasicMessageChannel<Any?>(binaryMessenger, "dev.flutter.pigeon.CameraInterface.setPhotoSize", codec)
        if (api != null) {
          channel.setMessageHandler { message, reply ->
            val args = message as List<Any?>
            val sizeArg = args[0] as PreviewSize
            var wrapped: List<Any?>
            try {
              api.setPhotoSize(sizeArg)
              wrapped = listOf<Any?>(null)
            } catch (exception: Throwable) {
              wrapped = wrapError(exception)
            }
            reply.reply(wrapped)
          }
        } else {
          channel.setMessageHandler(null)
        }
      }
      run {
        val channel = BasicMessageChannel<Any?>(binaryMessenger, "dev.flutter.pigeon.CameraInterface.setPreviewSize", codec)
        if (api != null) {
          channel.setMessageHandler { message, reply ->
            val args = message as List<Any?>
            val sizeArg = args[0] as PreviewSize
            var wrapped: List<Any?>
            try {
              api.setPreviewSize(sizeArg)
              wrapped = listOf<Any?>(null)
            } catch (exception: Throwable) {
              wrapped = wrapError(exception)
            }
            reply.reply(wrapped)
          }
        } else {
          channel.setMessageHandler(null)
        }
      }
      run {
        val channel = BasicMessageChannel<Any?>(binaryMessenger, "dev.flutter.pigeon.CameraInterface.setAspectRatio", codec)
        if (api != null) {
          channel.setMessageHandler { message, reply ->
            val args = message as List<Any?>
            val aspectRatioArg = args[0] as String
            var wrapped: List<Any?>
            try {
              api.setAspectRatio(aspectRatioArg)
              wrapped = listOf<Any?>(null)
            } catch (exception: Throwable) {
              wrapped = wrapError(exception)
            }
            reply.reply(wrapped)
          }
        } else {
          channel.setMessageHandler(null)
        }
      }
      run {
        val channel = BasicMessageChannel<Any?>(binaryMessenger, "dev.flutter.pigeon.CameraInterface.setupImageAnalysisStream", codec)
        if (api != null) {
          channel.setMessageHandler { message, reply ->
            val args = message as List<Any?>
            val formatArg = args[0] as String
            val widthArg = args[1].let { if (it is Int) it.toLong() else it as Long }
            val maxFramesPerSecondArg = args[2] as Double?
            val autoStartArg = args[3] as Boolean
            var wrapped: List<Any?>
            try {
              api.setupImageAnalysisStream(formatArg, widthArg, maxFramesPerSecondArg, autoStartArg)
              wrapped = listOf<Any?>(null)
            } catch (exception: Throwable) {
              wrapped = wrapError(exception)
            }
            reply.reply(wrapped)
          }
        } else {
          channel.setMessageHandler(null)
        }
      }
      run {
        val channel = BasicMessageChannel<Any?>(binaryMessenger, "dev.flutter.pigeon.CameraInterface.setExifPreferences", codec)
        if (api != null) {
          channel.setMessageHandler { message, reply ->
            val args = message as List<Any?>
            val exifPreferencesArg = args[0] as ExifPreferences
            api.setExifPreferences(exifPreferencesArg) { result: Result<Boolean> ->
              val error = result.exceptionOrNull()
              if (error != null) {
                reply.reply(wrapError(error))
              } else {
                val data = result.getOrNull()
                reply.reply(wrapResult(data))
              }
            }
          }
        } else {
          channel.setMessageHandler(null)
        }
      }
      run {
        val channel = BasicMessageChannel<Any?>(binaryMessenger, "dev.flutter.pigeon.CameraInterface.startAnalysis", codec)
        if (api != null) {
          channel.setMessageHandler { _, reply ->
            var wrapped: List<Any?>
            try {
              api.startAnalysis()
              wrapped = listOf<Any?>(null)
            } catch (exception: Throwable) {
              wrapped = wrapError(exception)
            }
            reply.reply(wrapped)
          }
        } else {
          channel.setMessageHandler(null)
        }
      }
      run {
        val channel = BasicMessageChannel<Any?>(binaryMessenger, "dev.flutter.pigeon.CameraInterface.stopAnalysis", codec)
        if (api != null) {
          channel.setMessageHandler { _, reply ->
            var wrapped: List<Any?>
            try {
              api.stopAnalysis()
              wrapped = listOf<Any?>(null)
            } catch (exception: Throwable) {
              wrapped = wrapError(exception)
            }
            reply.reply(wrapped)
          }
        } else {
          channel.setMessageHandler(null)
        }
      }
      run {
        val channel = BasicMessageChannel<Any?>(binaryMessenger, "dev.flutter.pigeon.CameraInterface.setFilter", codec)
        if (api != null) {
          channel.setMessageHandler { message, reply ->
            val args = message as List<Any?>
            val matrixArg = args[0] as List<Double>
            var wrapped: List<Any?>
            try {
              api.setFilter(matrixArg)
              wrapped = listOf<Any?>(null)
            } catch (exception: Throwable) {
              wrapped = wrapError(exception)
            }
            reply.reply(wrapped)
          }
        } else {
          channel.setMessageHandler(null)
        }
      }
      run {
        val channel = BasicMessageChannel<Any?>(binaryMessenger, "dev.flutter.pigeon.CameraInterface.isVideoRecordingAndImageAnalysisSupported", codec)
        if (api != null) {
          channel.setMessageHandler { message, reply ->
            val args = message as List<Any?>
            val sensorArg = PigeonSensorPosition.ofRaw(args[0] as Int)!!
            api.isVideoRecordingAndImageAnalysisSupported(sensorArg) { result: Result<Boolean> ->
              val error = result.exceptionOrNull()
              if (error != null) {
                reply.reply(wrapError(error))
              } else {
                val data = result.getOrNull()
                reply.reply(wrapResult(data))
              }
            }
          }
        } else {
          channel.setMessageHandler(null)
        }
      }
      run {
        val channel = BasicMessageChannel<Any?>(binaryMessenger, "dev.flutter.pigeon.CameraInterface.isMultiCamSupported", codec)
        if (api != null) {
          channel.setMessageHandler { _, reply ->
            var wrapped: List<Any?>
            try {
              wrapped = listOf<Any?>(api.isMultiCamSupported())
            } catch (exception: Throwable) {
              wrapped = wrapError(exception)
            }
            reply.reply(wrapped)
          }
        } else {
          channel.setMessageHandler(null)
        }
      }
    }
  }
}



================================================
FILE: android/src/main/kotlin/com/apparence/camerawesome/exceptions/CameraManagerException.java
================================================
package com.apparence.camerawesome.exceptions;

public class CameraManagerException extends Exception {

    public enum Codes {
        MISSING_PERMISSION,
        INTERRUPTED,
        CANNOT_OPEN_CAMERA,
        LOCKED
    }

    public CameraManagerException() {
    }

    public CameraManagerException(Codes code) {
        super(code.name());
    }

    public CameraManagerException(Codes code, Throwable cause) {
        super(code.name(), cause);
    }

    public CameraManagerException(Throwable cause) {
        super(cause);
    }


}



================================================
FILE: android/src/main/kotlin/com/apparence/camerawesome/exceptions/CameraPreviewException.java
================================================
package com.apparence.camerawesome.exceptions;

public class CameraPreviewException extends Exception {

    public enum Codes {
        EMPTY_SIZE
    }

    public CameraPreviewException() {
    }

    public CameraPreviewException(Codes code) {
        super(code.name());
    }

    public CameraPreviewException(Codes code, Throwable cause) {
        super(code.name(), cause);
    }

    public CameraPreviewException(Throwable cause) {
        super(cause);
    }


}



================================================
FILE: android/src/main/kotlin/com/apparence/camerawesome/exceptions/PermissionNotDeclaredException.kt
================================================
package com.apparence.camerawesome.exceptions

class PermissionNotDeclaredException(permission: String) :
    Exception("Permission not declared: $permission\nAdd it to your AndroidManifest.xml:\n<uses-permission android:name=\"$permission\" />") {
}


================================================
FILE: android/src/main/kotlin/com/apparence/camerawesome/image/ImgConverter.java
================================================
package com.apparence.camerawesome.image;

import android.media.Image;
import android.media.ImageReader;

public interface ImgConverter {

    byte[] process(ImageReader imageReader);
}



================================================
FILE: android/src/main/kotlin/com/apparence/camerawesome/image/ImgConverterThreaded.java
================================================
package com.apparence.camerawesome.image;

import android.media.ImageReader;
import android.os.Handler;
import android.os.HandlerThread;
import android.os.Looper;


public class ImgConverterThreaded {

    private static HandlerThread handlerThread = new HandlerThread("ImgConverterThreaded");

    private ImgConverter converter;

    public ImgConverterThreaded(ImgConverter converter) {
        if(handlerThread != null) {
            handlerThread.quit();
            handlerThread = new HandlerThread("ImgConverterThreaded");
        }
        this.converter = converter;
        handlerThread.start();
    }

    public void process(final ImageReader imageReader, final Consumer consumer) {
        Looper looper = handlerThread.getLooper();
        if(looper == null) {
            return;
        }
        Handler handler = new Handler(looper);
        handler.post(new Runnable() {
            @Override
            public void run() {
                consumer.process(converter.process(imageReader));
            }
        });
    }

    public void dispose() {
        handlerThread.quitSafely();
    }

    public interface Consumer {
        void process(byte[] result);
    }
}



================================================
FILE: android/src/main/kotlin/com/apparence/camerawesome/image/YuvToJpgConverter.java
================================================
package com.apparence.camerawesome.image;

import android.graphics.ImageFormat;
import android.graphics.Rect;
import android.graphics.YuvImage;
import android.media.Image;
import android.media.ImageReader;
import android.os.Build;

import androidx.annotation.RequiresApi;

import java.io.ByteArrayOutputStream;
import java.nio.ByteBuffer;

@RequiresApi(api = Build.VERSION_CODES.LOLLIPOP)
public
class YuvToJpgConverter implements ImgConverter{

    @Override
    public byte[] process(ImageReader reader) {
        final Image image = reader.acquireLatestImage();
        byte[] data = null;
        if (image != null) {
            Image.Plane[] planes = image.getPlanes();
            if (image.getFormat() == ImageFormat.JPEG) {
                ByteBuffer buffer = planes[0].getBuffer();
                data = new byte[buffer.capacity()];
                buffer.get(data);
                return data;
            } else if (image.getFormat() == ImageFormat.YUV_420_888) {
                data = NV21toJPEG(
                    YUV_420_888toI420SemiPlanar(
                            planes[0].getBuffer(),
                            planes[1].getBuffer(),
                            planes[2].getBuffer(),
                            image.getWidth(), image.getHeight(),
                            false),
                    image.getWidth(), image.getHeight(), 80);

            }
            image.close();
        }
        return data;
    }

    public byte[] NV21toJPEG(byte[] nv21, int width, int height, int quality) {
        ByteArrayOutputStream out = new ByteArrayOutputStream();
        YuvImage yuv = new YuvImage(nv21, ImageFormat.NV21, width, height, null);
        yuv.compressToJpeg(new Rect(0, 0, width, height), quality, out);
        return out.toByteArray();
    }

    // nv12: true = NV12, false = NV21
    public byte[] YUV_420_888toNV(ByteBuffer yBuffer, ByteBuffer uBuffer, ByteBuffer vBuffer, boolean nv12) {
        byte[] nv;

        int ySize = yBuffer.remaining();
        int uSize = uBuffer.remaining();
        int vSize = vBuffer.remaining();

        nv = new byte[ySize + uSize + vSize];

        yBuffer.get(nv, 0, ySize);
        if (nv12) {//U and V are swapped
            vBuffer.get(nv, ySize, vSize);
            uBuffer.get(nv, ySize + vSize, uSize);
        } else {
            uBuffer.get(nv, ySize , uSize);
            vBuffer.get(nv, ySize + uSize, vSize);
        }
        return nv;
    }

    public byte[] YUV_420_888toI420SemiPlanar(ByteBuffer yBuffer, ByteBuffer uBuffer, ByteBuffer vBuffer,
                                                     int width, int height, boolean deInterleaveUV) {
        byte[] data = YUV_420_888toNV(yBuffer, uBuffer, vBuffer, deInterleaveUV);
        int size = width * height;
        if (deInterleaveUV) {
            byte[] buffer = new byte[3 * width * height / 2];

            // De-interleave U and V
            for (int i = 0; i < size / 4; i += 1) {
                buffer[i] = data[size + 2 * i + 1];
                buffer[size / 4 + i] = data[size + 2 * i];
            }
            System.arraycopy(buffer, 0, data, size, size / 2);
        } else {
            for (int i = size; i < data.length; i += 2) {
                byte b1 = data[i];
                data[i] = data[i + 1];
                data[i + 1] = b1;
            }
        }
        return data;
    }
}



================================================
FILE: android/src/main/kotlin/com/apparence/camerawesome/models/CameraCharacteristicsModel.java
================================================
package com.apparence.camerawesome.models;

import android.graphics.Rect;
import android.hardware.camera2.CameraCharacteristics;
import android.util.Range;
import android.util.Rational;

public class CameraCharacteristicsModel {

    private float maxZoom;

    private Rect availablePreviewZone;

    private boolean hasAutoFocus;

    private Boolean flashAvailable;

    private Range<Integer> aeCompensationRange;

    private Rational aeCompensationRatio;

    public CameraCharacteristicsModel(float maxZoom, Rect availablePreviewZone, boolean hasAutoFocus, boolean hasFlash,
                                      Range<Integer> aeCompensationRange, Rational aeCompensationRatio) {
        this.maxZoom = maxZoom;
        this.availablePreviewZone = availablePreviewZone;
        this.hasAutoFocus = hasAutoFocus;
        this.flashAvailable = hasFlash;
        this.aeCompensationRange = aeCompensationRange;
        this.aeCompensationRatio = aeCompensationRatio;
    }

    public float getMaxZoom() {
        return maxZoom;
    }

    public Boolean hasFlashAvailable() { return flashAvailable; }

    public boolean hasAutoFocus() { return hasAutoFocus; }

    public Rect getAvailablePreviewZone() {
        return availablePreviewZone;
    }

    public Range<Integer> getAeCompensationRange() { return aeCompensationRange; }

    public Rational getAeCompensationRatio() { return aeCompensationRatio; }

    public static class Builder {

        private float maxZoom;

        private Rect availablePreviewZone;

        private boolean hasAutoFocus;

        private Boolean flashAvailable;

        private Rational aeCompensationRatio;

        private Range<Integer> aeCompensationRange;

        public Builder() {}

        public Builder withMaxZoom(float maxZoom) {
            this.maxZoom = maxZoom;
            return this;
        }

        public Builder withAvailablePreviewZone(Rect availablePreviewZone) {
            this.availablePreviewZone = availablePreviewZone;
            return this;
        }

        public Builder withAutoFocus(int[] modes) {
            if (modes == null || modes.length == 0
                    || (modes.length == 1 && modes[0] == CameraCharacteristics.CONTROL_AF_MODE_OFF)) {
                this.hasAutoFocus = false;
            } else {
                this.hasAutoFocus = true;
            }
            return this;
        }

        public Builder withFlash(Boolean flashAvailable) {
            this.flashAvailable = flashAvailable;
            return this;
        }

        public Builder withAeCompensationRange(Range<Integer> aeCompensationRange) {
            this.aeCompensationRange = aeCompensationRange;
            return this;
        }

        public Builder withAeCompensationStep(Rational rational) {
            aeCompensationRatio = rational;
            return this;
        }

        public CameraCharacteristicsModel build() {
            return new CameraCharacteristicsModel(
              this.maxZoom, this.availablePreviewZone, this.hasAutoFocus, this.flashAvailable, this.aeCompensationRange, this.aeCompensationRatio
            );
        }
    }
}



================================================
FILE: android/src/main/kotlin/com/apparence/camerawesome/models/FlashMode.java
================================================
package com.apparence.camerawesome.models;

public enum FlashMode {
    NONE,
    ON,
    AUTO,
    ALWAYS
}



================================================
FILE: android/src/main/kotlin/com/apparence/camerawesome/sensors/BasicLuminosityNotifier.java
================================================
package com.apparence.camerawesome.sensors;

import android.content.Context;
import android.hardware.Sensor;
import android.hardware.SensorEvent;
import android.hardware.SensorEventListener;
import android.hardware.SensorManager;

import io.flutter.plugin.common.EventChannel;

import static android.content.Context.SENSOR_SERVICE;

public class BasicLuminosityNotifier implements LuminosityNotifier, EventChannel.StreamHandler {

    SensorManager mSensorManager;
    Sensor mLightSensor;

    EventChannel.EventSink notifyChannel;

    @Override
    public void init(Context context) {
        if(mSensorManager != null && mLightSensor != null)
            return;
        mSensorManager = (SensorManager) context.getSystemService(SENSOR_SERVICE);
        mLightSensor = mSensorManager.getDefaultSensor(Sensor.TYPE_LIGHT);

        mSensorManager.registerListener(lightListener, mLightSensor, SensorManager.SENSOR_DELAY_UI);
    }

    final SensorEventListener lightListener = new SensorEventListener() {
        @Override
        public void onSensorChanged(SensorEvent event) {
            if(notifyChannel != null && event != null && event.values != null &&  event.values.length > 0) {
                notifyChannel.success(event.values[0]);
            }
        }

        @Override
        public void onAccuracyChanged(Sensor sensor, int accuracy) { }
    };

    @Override
    public void onListen(Object arguments, EventChannel.EventSink events) {
        this.notifyChannel = events;
    }

    @Override
    public void onCancel(Object arguments) {
        this.notifyChannel.endOfStream();
        this.notifyChannel = null;
    }
}



================================================
FILE: android/src/main/kotlin/com/apparence/camerawesome/sensors/CameraSensor.java
================================================
package com.apparence.camerawesome.sensors;

public enum CameraSensor {
    FRONT,
    BACK
}



================================================
FILE: android/src/main/kotlin/com/apparence/camerawesome/sensors/LuminosityNotifier.java
================================================
package com.apparence.camerawesome.sensors;

import android.content.Context;

public interface LuminosityNotifier {

    void init(Context context);
}



================================================
FILE: android/src/main/kotlin/com/apparence/camerawesome/sensors/SensorOrientation.kt
================================================
package com.apparence.camerawesome.sensors

interface SensorOrientation {
    fun onOrientationChanged(orientation: Int)
}


================================================
FILE: android/src/main/kotlin/com/apparence/camerawesome/sensors/SensorOrientationListener.kt
================================================
package com.apparence.camerawesome.sensors

import io.flutter.plugin.common.EventChannel
import io.flutter.plugin.common.EventChannel.EventSink

class SensorOrientationListener : EventChannel.StreamHandler, SensorOrientation {
    var events: EventSink? = null
    override fun onListen(arguments: Any, events: EventSink) {
        this.events = events
    }

    override fun onCancel(arguments: Any?) {
        events?.endOfStream()
        events = null
    }

    override fun onOrientationChanged(orientation: Int) {
        if (events == null) {
            return
        }
        when (orientation) {
            0 -> events!!.success("PORTRAIT_UP")
            90 -> events!!.success("LANDSCAPE_LEFT")
            180 -> events!!.success("PORTRAIT_DOWN")
            270 -> events!!.success("LANDSCAPE_RIGHT")
        }
    }
}


================================================
FILE: android/src/main/kotlin/com/apparence/camerawesome/surface/FlutterSurfaceFactory.java
================================================
package com.apparence.camerawesome.surface;

import android.graphics.SurfaceTexture;
import android.os.Build;
import android.util.Size;
import android.view.Surface;

import androidx.annotation.RequiresApi;

import io.flutter.view.TextureRegistry;

public class FlutterSurfaceFactory implements SurfaceFactory {

    private TextureRegistry registry;

    private TextureRegistry.SurfaceTextureEntry flutterTexture;


    public FlutterSurfaceFactory(TextureRegistry registry) {
        this.registry = registry;
    }

    @Override
    public Surface build(Size size) {
        flutterTexture = registry.createSurfaceTexture();
        SurfaceTexture surfaceTexture = flutterTexture.surfaceTexture();
        surfaceTexture.setDefaultBufferSize(size.getWidth(), size.getHeight());
        return new Surface(surfaceTexture);
    }

    @Override
    public long getSurfaceId() {
        if(flutterTexture == null) {
            throw new RuntimeException("flutterTexture is null");
        }
        return flutterTexture.id();
    }
}



================================================
FILE: android/src/main/kotlin/com/apparence/camerawesome/surface/SurfaceFactory.java
================================================
package com.apparence.camerawesome.surface;

import android.util.Size;
import android.view.Surface;

public interface SurfaceFactory {

    /**
     * Creates a surfaceTexture used to create a Surface
     * Surface are used to show camera preview
     * @param previewSize
     * @return
     */
    Surface build(Size previewSize);

    long getSurfaceId();
}



================================================
FILE: android/src/main/kotlin/com/apparence/camerawesome/utils/CameraCharactericitsUtils.kt
================================================
package com.apparence.camerawesome.utils

import android.hardware.camera2.CameraCharacteristics
import android.util.Size
import android.util.SizeF
import androidx.camera.camera2.interop.Camera2CameraInfo
import androidx.camera.camera2.interop.ExperimentalCamera2Interop
import androidx.camera.core.CameraSelector.LENS_FACING_BACK
import com.apparence.camerawesome.cameraX.PigeonSensorPosition
import com.apparence.camerawesome.cameraX.PigeonSensorType
import kotlin.math.max
import kotlin.math.min

// 35mm is 135 film format, a standard in which focal lengths are usually measured
val Size35mm = Size(36, 24)

/**
 * Convert a given array of focal lengths to the corresponding TypeScript union type name.
 *
 * Possible values for single cameras:
 * * `"wide-angle-camera"`
 * * `"ultra-wide-angle-camera"`
 * * `"telephoto-camera"`
 *
 * Sources for the focal length categories:
 * * [Telephoto Lens (wikipedia)](https://en.wikipedia.org/wiki/Telephoto_lens)
 * * [Normal Lens (wikipedia)](https://en.wikipedia.org/wiki/Normal_lens)
 * * [Wide-Angle Lens (wikipedia)](https://en.wikipedia.org/wiki/Wide-angle_lens)
 * * [Ultra-Wide-Angle Lens (wikipedia)](https://en.wikipedia.org/wiki/Ultra_wide_angle_lens)
 */
@ExperimentalCamera2Interop
fun Camera2CameraInfo.getSensorType(): PigeonSensorType {
    val focalLengths =
        this.getCameraCharacteristic(CameraCharacteristics.LENS_INFO_AVAILABLE_FOCAL_LENGTHS)!!
    val sensorSize =
        this.getCameraCharacteristic(CameraCharacteristics.SENSOR_INFO_PHYSICAL_SIZE)!!

    // To get valid focal length standards we have to upscale to the 35mm measurement (film standard)
    val cropFactor = Size35mm.bigger / sensorSize.bigger


    val containsTelephoto =
        focalLengths.any { l -> (l * cropFactor) > 35 } // TODO: Telephoto lenses are > 85mm, but we don't have anything between that range..
    // val containsNormalLens = focalLengths.any { l -> (l * cropFactor) > 35 && (l * cropFactor) <= 55 }
    val containsWideAngle =
        focalLengths.any { l -> (l * cropFactor) >= 24 && (l * cropFactor) <= 35 }
    val containsUltraWideAngle = focalLengths.any { l -> (l * cropFactor) < 24 }

    if (containsTelephoto)
        return PigeonSensorType.TELEPHOTO
    if (containsWideAngle)
        return PigeonSensorType.WIDEANGLE
    if (containsUltraWideAngle)
        return PigeonSensorType.ULTRAWIDEANGLE
    return PigeonSensorType.UNKNOWN
}

@ExperimentalCamera2Interop
fun Camera2CameraInfo.getPigeonPosition(): PigeonSensorPosition {
    val facing = this.getCameraCharacteristic(CameraCharacteristics.LENS_FACING)!!
    return if (facing == LENS_FACING_BACK)
        PigeonSensorPosition.BACK
    else
        PigeonSensorPosition.FRONT
}


val Size.bigger: Int
    get() = max(this.width, this.height)
val Size.smaller: Int
    get() = min(this.width, this.height)

val SizeF.bigger: Float
    get() = max(this.width, this.height)
val SizeF.smaller: Float
    get() = min(this.width, this.height)


================================================
FILE: android/src/main/kotlin/com/apparence/camerawesome/utils/CameraProviderUtils.kt
================================================
package com.apparence.camerawesome.utils

import android.annotation.SuppressLint
import androidx.camera.lifecycle.ProcessCameraProvider

@SuppressLint("RestrictedApi")
fun ProcessCameraProvider.isMultiCamSupported(): Boolean {
    val concurrentInfos = availableConcurrentCameraInfos
    var hasOnePair = false
    for (cameraInfos in concurrentInfos) {
        if (cameraInfos.size > 1) {
            hasOnePair = true
        }
    }
    return hasOnePair
}


================================================
FILE: android/src/main/kotlin/com/apparence/camerawesome/utils/ResettableCountDownLatch.kt
================================================
package com.apparence.camerawesome.utils

import java.util.concurrent.TimeUnit
import java.util.concurrent.locks.AbstractQueuedSynchronizer

/**
 * A synchronization aid that allows one or more threads to wait until
 * a set of operations being performed in other threads completes.
 *
 *
 * A `CountDownLatch` is initialized with a given *count*.
 * The [await][.await] methods block until the current count reaches
 * zero due to invocations of the [.countDown] method, after which
 * all waiting threads are released and any subsequent invocations of
 * [await][.await] return immediately.  This is a one-shot phenomenon
 * -- the count cannot be reset.  If you need a version that resets the
 * count, consider using a [CyclicBarrier].
 *
 *
 * A `CountDownLatch` is a versatile synchronization tool
 * and can be used for a number of purposes.  A
 * `CountDownLatch` initialized with a count of one serves as a
 * simple on/off latch, or gate: all threads invoking [await][.await]
 * wait at the gate until it is opened by a thread invoking [ ][.countDown].  A `CountDownLatch` initialized to *N*
 * can be used to make one thread wait until *N* threads have
 * completed some action, or some action has been completed N times.
 *
 *
 * A useful property of a `CountDownLatch` is that it
 * doesn't require that threads calling `countDown` wait for
 * the count to reach zero before proceeding, it simply prevents any
 * thread from proceeding past an [await][.await] until all
 * threads could pass.
 *
 *
 * **Sample usage:** Here is a pair of classes in which a group
 * of worker threads use two countdown latches:
 *
 *  * The first is a start signal that prevents any worker from proceeding
 * until the driver is ready for them to proceed;
 *  * The second is a completion signal that allows the driver to wait
 * until all workers have completed.
 *
 *
 * <pre>
 * class Driver { // ...
 * void main() throws InterruptedException {
 * CountDownLatch startSignal = new CountDownLatch(1);
 * CountDownLatch doneSignal = new CountDownLatch(N);
 *
 * for (int i = 0; i < N; ++i) // create and start threads
 * new Thread(new Worker(startSignal, doneSignal)).start();
 *
 * doSomethingElse();            // don't let run yet
 * startSignal.countDown();      // let all threads proceed
 * doSomethingElse();
 * doneSignal.await();           // wait for all to finish
 * }
 * }
 *
 * class Worker implements Runnable {
 * private final CountDownLatch startSignal;
 * private final CountDownLatch doneSignal;
 * Worker(CountDownLatch startSignal, CountDownLatch doneSignal) {
 * this.startSignal = startSignal;
 * this.doneSignal = doneSignal;
 * }
 * public void run() {
 * try {
 * startSignal.await();
 * doWork();
 * doneSignal.countDown();
 * } catch (InterruptedException ex) {} // return;
 * }
 *
 * void doWork() { ... }
 * }
 *
</pre> *
 *
 *
 * Another typical usage would be to divide a problem into N parts,
 * describe each part with a Runnable that executes that portion and
 * counts down on the latch, and queue all the Runnables to an
 * Executor.  When all sub-parts are complete, the coordinating thread
 * will be able to pass through await. (When threads must repeatedly
 * count down in this way, instead use a [CyclicBarrier].)
 *
 * <pre>
 * class Driver2 { // ...
 * void main() throws InterruptedException {
 * CountDownLatch doneSignal = new CountDownLatch(N);
 * Executor e = ...
 *
 * for (int i = 0; i < N; ++i) // create and start threads
 * e.execute(new WorkerRunnable(doneSignal, i));
 *
 * doneSignal.await();           // wait for all to finish
 * }
 * }
 *
 * class WorkerRunnable implements Runnable {
 * private final CountDownLatch doneSignal;
 * private final int i;
 * WorkerRunnable(CountDownLatch doneSignal, int i) {
 * this.doneSignal = doneSignal;
 * this.i = i;
 * }
 * public void run() {
 * try {
 * doWork(i);
 * doneSignal.countDown();
 * } catch (InterruptedException ex) {} // return;
 * }
 *
 * void doWork() { ... }
 * }
 *
</pre> *
 *
 *
 * Memory consistency effects: Actions in a thread prior to calling
 * `countDown()`
 * [*happen-before*](package-summary.html#MemoryVisibility)
 * actions following a successful return from a corresponding
 * `await()` in another thread.
 *
 * @since 1.5
 * @author Doug Lea
 */
class ResettableCountDownLatch(count: Int) {
    /**
     * Synchronization control For CountDownLatch.
     * Uses AQS state to represent count.
     */
    private class Sync internal constructor(val startCount: Int) : AbstractQueuedSynchronizer() {
        init {
            state = startCount
        }

        val count: Int
            get() = state

        public override fun tryAcquireShared(acquires: Int): Int {
            return if (state == 0) 1 else -1
        }

        public override fun tryReleaseShared(releases: Int): Boolean {
            // Decrement count; signal when transition to zero
            while (true) {
                val c = state
                if (c == 0) return false
                val nextc = c - 1
                if (compareAndSetState(c, nextc)) return nextc == 0
            }
        }

        fun reset() {
            state = startCount
        }

        companion object {
            private const val serialVersionUID = 4982264981922014374L
        }
    }

    private val sync: Sync

    /**
     * Constructs a `CountDownLatch` initialized with the given count.
     *
     * @param count the number of times [.countDown] must be invoked
     * before threads can pass through [.await]
     * @throws IllegalArgumentException if `count` is negative
     */
    init {
        require(count >= 0) { "count < 0" }
        sync = Sync(count)
    }

    /**
     * Causes the current thread to wait until the latch has counted down to
     * zero, unless the thread is [interrupted][Thread.interrupt].
     *
     *
     * If the current count is zero then this method returns immediately.
     *
     *
     * If the current count is greater than zero then the current
     * thread becomes disabled for thread scheduling purposes and lies
     * dormant until one of two things happen:
     *
     *  * The count reaches zero due to invocations of the
     * [.countDown] method; or
     *  * Some other thread [interrupts][Thread.interrupt]
     * the current thread.
     *
     *
     *
     * If the current thread:
     *
     *  * has its interrupted status set on entry to this method; or
     *  * is [interrupted][Thread.interrupt] while waiting,
     *
     * then [InterruptedException] is thrown and the current thread's
     * interrupted status is cleared.
     *
     * @throws InterruptedException if the current thread is interrupted
     * while waiting
     */
    @Throws(InterruptedException::class)
    fun await() {
        sync.acquireSharedInterruptibly(1)
    }

    fun reset() {
        sync.reset()
    }

    /**
     * Causes the current thread to wait until the latch has counted down to
     * zero, unless the thread is [interrupted][Thread.interrupt],
     * or the specified waiting time elapses.
     *
     *
     * If the current count is zero then this method returns immediately
     * with the value `true`.
     *
     *
     * If the current count is greater than zero then the current
     * thread becomes disabled for thread scheduling purposes and lies
     * dormant until one of three things happen:
     *
     *  * The count reaches zero due to invocations of the
     * [.countDown] method; or
     *  * Some other thread [interrupts][Thread.interrupt]
     * the current thread; or
     *  * The specified waiting time elapses.
     *
     *
     *
     * If the count reaches zero then the method returns with the
     * value `true`.
     *
     *
     * If the current thread:
     *
     *  * has its interrupted status set on entry to this method; or
     *  * is [interrupted][Thread.interrupt] while waiting,
     *
     * then [InterruptedException] is thrown and the current thread's
     * interrupted status is cleared.
     *
     *
     * If the specified waiting time elapses then the value `false`
     * is returned.  If the time is less than or equal to zero, the method
     * will not wait at all.
     *
     * @param timeout the maximum time to wait
     * @param unit the time unit of the `timeout` argument
     * @return `true` if the count reached zero and `false`
     * if the waiting time elapsed before the count reached zero
     * @throws InterruptedException if the current thread is interrupted
     * while waiting
     */
    @Throws(InterruptedException::class)
    fun await(timeout: Long, unit: TimeUnit): Boolean {
        return sync.tryAcquireSharedNanos(1, unit.toNanos(timeout))
    }

    /**
     * Decrements the count of the latch, releasing all waiting threads if
     * the count reaches zero.
     *
     *
     * If the current count is greater than zero then it is decremented.
     * If the new count is zero then all waiting threads are re-enabled for
     * thread scheduling purposes.
     *
     *
     * If the current count equals zero then nothing happens.
     */
    fun countDown() {
        sync.releaseShared(1)
    }

    /**
     * Returns the current count.
     *
     *
     * This method is typically used for debugging and testing purposes.
     *
     * @return the current count
     */
    val count: Int
        get() = sync.count

    /**
     * Returns a string identifying this latch, as well as its state.
     * The state, in brackets, includes the String `"Count ="`
     * followed by the current count.
     *
     * @return a string identifying this latch, as well as its state
     */
    override fun toString(): String {
        return super.toString() + "[Count = " + sync.count + "]"
    }
}


================================================
FILE: android/src/main/kotlin/com/apparence/camerawesome_example/MainActivity.kt
================================================
package com.apparence.camerawesome_example

import io.flutter.embedding.android.FlutterActivity

class MainActivity : FlutterActivity()



================================================
FILE: docs/index.mdx
================================================
# CamerAwesome documentation

<Image src="https://github.com/Apparence-io/camera_awesome/blob/master/docs/img/apparence.png?raw=true" alt="Apparence.io flutter studio" />
<Image src="https://github.com/Apparence-io/camera_awesome/blob/master/docs/img/preview.png?raw=true" alt="awesome built-in camera plugin for flutter" />
<Image src="https://github.com/Apparence-io/camera_awesome/blob/master/docs/img/features.png?raw=true" alt="camera flutter plugin features" />

This packages provides you a fully customizable camera experience that you can use within your app. <br/>
Use our awesome built in interface or customize it as you want. 


## Native features
Here's all native features that CamerAwesome provides to the flutter side.

| Features                                 | Android |  iOS  |
| :--------------------------------------- | :-----: | :---: |
| 🔖 Ask permissions                       |    ✅    |   ✅   |
| 🎥 Record video                          |    ✅    |   ✅   |
| 📹 Multi camera                          |    ✅    |   ✅   |
| 🔈 Enable/disable audio                  |    ✅    |   ✅   |
| 🎞 Take photos                           |    ✅    |   ✅   |
| 🌆 Photo live filters                    |    ✅    |   ✅   |
| 🌤 Exposure level                        |    ✅    |   ✅   |
| 📡 Broadcast live image stream           |    ✅    |   ✅   |
| 🧪 Image analysis (barcode scan & more.) |    ✅    |   ✅   |
| 👁 Zoom                                  |    ✅    |   ✅   |
| 📸 Device flash support                  |    ✅    |   ✅   |
| ⌛️ Auto focus                            |    ✅    |   ✅   |
| 📲 Live switching camera                 |    ✅    |   ✅   |
| 😵‍💫 Camera rotation stream                |    ✅    |   ✅   |
| 🤐 Background auto stop                  |    ✅    |   ✅   |
| 🔀 Sensor type switching                 |    ⛔️    |   ✅   |
| 🪞 Enable/disable front camera mirroring |    ✅    |   ✅   |


After [installing](getting_started/installing) CamerAwesome, take a look at the [Awesome built-in UI](getting_started/awesome-ui) guide.


================================================
FILE: docs/appendix/license.mdx
================================================
# Licence

Copyright © Apparence.io <br/>
Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:<br/>
<br/>
The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.<br/>
The Software is provided “as is”, without warranty of any kind, express or implied, including but not limited to the warranties of merchantability, fitness for a particular purpose and noninfringement. In no event shall the authors or copyright holders be liable for any claim, damages or other liability, whether in an action of contract, tort or otherwise, arising from, out of or in connection with the software or the use or other dealings in the Software.


================================================
FILE: docs/appendix/roadmap.mdx
================================================
## 🚀 Roadmap

### Both platforms
- [ ] Adjust exposure in awesome UI (flutter)
- [ ] Timer before taking a photo (flutter)
- [ ] Multiple camera photo mode
- [ ] Rework quality options (use preset quality & not resolution)
- [ ] Add Web support.
- [ ] Add Linux support.
- [ ] Add Windows support.
- [ ] Add macOS support.
- [ ] Add custom filter.

### Android
- [ ] Include cameraX extensions (https://github.com/android/camera-samples/tree/main/CameraXExtensions)
- [ ] Change sensor type
- [ ] Add video settings
- [ ] Return list of all available cameras

### iOS
- [ ] Fix patrol tests.
- [x] Add correction brightness.

## ✔️ Done
- [x] Preview alignment & padding
- [x] Built-in widgets theming
- [x] Apply Preview filter
- [x] Apply filter on image
- [x] Add filters.
- [x] Use Pigeon.
- [x] Cropped ratio support
- [x] Tests E2E using `patrol`
- [x] Pause/Resume a video recording in awesome UI
- [x] Lock UI while recording a video (user should not change the camera sensor nor the camera mode)
- [x] Customize preview fit
- [x] Tap to focus on a point.
- [x] Add analysis mode.
- [x] Return list of all available cameras (iOS).
- [x] Change sensor type (iOS).
- [x] Add video settings (iOS).
- [x] Return list of all available cameras (iOS).


================================================
FILE: docs/getting_started/awesome-ui.mdx
================================================
# 👌 Awesome built-in UI

CamerAwesome comes with a full built UI that you can use as is.

Use `CameraAwesomeBuilder.awesome()` to get a complete ready-to-use camera experience within your app.

Here is a concrete example using **better_open_file** to display the last media captured:

```dart
CameraAwesomeBuilder.awesome(
  saveConfig: SaveConfig.photoAndVideo(),
  onMediaTap: (mediaCapture) {
    OpenFile.open(mediaCapture.filePath);
  },
)
```

![Base Awesome UI](/img/base_awesome_ui.jpg)

## 📁 Camera captures configuration

`CameraAwesomeBuilder` requires a `SaveConfig` parameter.
You can create one with one of the following factories:

- `SaveConfig.photo()` if you only want to take photos
- `SaveConfig.video()` to only take videos
- `SaveConfig.photoAndVideo()` if you want to switch between photo and video modes.

These factories don't require additional parameters, but you might want to customize a few things, like where to save your files.

Here is a complete example to overwrite the default behaviors:

```dart
SaveConfig.photoAndVideo(
  // 1.
  initialCaptureMode: CaptureMode.photo,
  // 2.
  photoPathBuilder: (sensors) async {
    ...
  },
  // 3.
  videoPathBuilder: (sensors) async {
    ...
  },
  // 4.
  videoOptions: VideoOptions(
    enableAudio: true,
    ios: CupertinoVideoOptions(
      fps: 10,
      // TODODOC Add other possble params
    ),
    android: AndroidVideoOptions(
      bitrate: 6000000,
      quality: VideoRecordingQuality.fhd,
      fallbackStrategy: QualityFallbackStrategy.lower,
    ),
  ),
  // 5.
  exifPreferences: ExifPreferences(saveGPSLocation: true),
  // 6.
  mirrorFrontCamera: true,
)
```

Let's break it down:

1. When using `photoAndVideo` mode, you can choose which mode to start with. Here we start with photo mode (default).
2. You can customize the path where your photos will be saved.
3. You can also customize where to save your videos.
4. The video recording can be customized using `VideoOptions`. Note that each platform has its own settings.
5. You can also enable or disable the GPS location in the EXIF data of your photos.
6. Set if you want the front camera pictures & videos to be mirrored like in the preview.

A `photoPathBuilder` could look like this:

```dart
SaveConfig.photoAndVideo(
  photoPathBuilder: (sensors) async {
    // 1.
    final Directory extDir = await getTemporaryDirectory();
    final testDir = await Directory('${extDir.path}/camerawesome').create(recursive: true);

    // 2.
    if (sensors.length == 1) {
      final String filePath =
          '${testDir.path}/${DateTime.now().millisecondsSinceEpoch}.jpg';
      // 3.
      return SingleCaptureRequest(filePath, sensors.first);
    } else {
      // 4.
      return MultipleCaptureRequest(
        {
          for (final sensor in sensors)
            sensor:
                '${testDir.path}/${sensor.position == SensorPosition.front ? 'front_' : "back_"}${DateTime.now().millisecondsSinceEpoch}.jpg',
        },
      );
    }
  },
  // Other params
  ...
)
```

There are 4 steps in this code:

1. Create a directory where photos will be saved (here we use the temporary directory, using `path_provider`).
2. Since `CamerAwesome` supports taking pictures with both front and back cameras at the same time, we need to detect if there is only one picture to take or several ones.
3. If there is only one sensor used, we can build a `SingleCaptureRequest` with the file path and the sensor.
4. If there are several sensors, we need to build a `MultipleCaptureRequest` with a map of file paths and sensors. In this case, we create a different path based on wether it's the front or back sensor that takes the picture.

The same logic goes for videos but we replace the `.jpg` extension with `.mp4`.

## 📷 Initial camera configuration

You can set the initial camera configuration using a `SensorConfig`.

```dart
CameraAwesomeBuilder.awesome(
  sensorConfig: SensorConfig.single(
    aspectRatio: CameraAspectRatios.ratio_4_3,
    flashMode: FlashMode.auto,
    sensor: Sensor.position(SensorPosition.back),
    zoom: 0.0,
  ),
)
```

| Parameter       | Description                                      |
| --------------- | ------------------------------------------------ |
| **aspectRatio** | Initial aspect ratio of photos and videos taken  |
| **flashMode**   | The initial flash mode                           |
| **sensor**      | The initial camera sensor (Back or Front)        |
| **zoom**        | A value between 0.0 (no zoom) and 1.0 (max zoom) |

Note: you might also notice the `SensorConfig.multiple()` constructor which lets you specify several sensors.
This feature is in beta, but you can take a look at the [dedicated documentation](/getting_started/multicam).

`CameraAwesomeBuilder` also provides a few more parameters:

- `enablePhysicalButton` to enable the volume buttons to take pictures or record videos
- `filter` to set an initial filter to the pictures

## 🎨 Customize the built-in UI

Several parameters let you customize the built-in UI.

### Theming

You can customize the look and feel of CamerAwesome's built-in UI by setting your own theme.

For example, buttons are black with a white icon and they bounce when you tap them.

You can completely change them with `AwesomeTheme`:

Example:

```dart
CameraAwesomeBuilder.awesome(
  theme: AwesomeTheme(
    // Background color of the bottom actions
    bottomActionsBackgroundColor: Colors.deepPurple.withValues(alpha: 0.5),
    // Buttons theme
    buttonTheme: AwesomeButtonTheme(
      // Background color of the button
      backgroundColor: Colors.deepPurple.withValues(alpha: 0.5),
      // Size of the icon
      iconSize: 32,
      // Padding around the icon
      padding: const EdgeInsets.all(18),
      // Color of the icon
      foregroundColor: Colors.lightBlue,
      // Tap visual feedback (ripple, bounce...)
      buttonBuilder: (child, onTap) {
        return ClipOval(
          child: Material(
            color: Colors.transparent,
            shape: const CircleBorder(),
            child: InkWell(
              splashColor: Colors.deepPurple,
              highlightColor: Colors.deepPurpleAccent.withValues(alpha: 0.5),
              onTap: onTap,
              child: child,
            ),
          ),
        );
      },
    ),
  ),
);
```

![Custom theme](/img/custom_theme.gif)

Let's see what happens in the above animation:

- In the first part, the default theme of CamerAwesome is used. Note that the Flash button bounces when tapped.
- Then, the code is changed to show the custom theme above with a hot reload.
- Finally, the UI is updated with the new purple theme and you can see that even the Flash button has a different tap effect: it is now a ripple.

If you are using the built-in UI - even partially, it is recommended to use `AwesomeTheme` in the rest of your Camera UI to stay consistent.

You can access the current `AwesomeTheme` by using `AwesomeThemeProvider`:

```dart
final myTheme = AwesomeThemeProvider.of(context).theme
```

**Tip:** If you don't have a `BuildContext` to access the parent's `AwesomeTheme`, wrap your widget within a `Builder` widget.

See also [Theming](/widgets/theming) and `custom_theme.dart` for an example with a custom theme.

### Place widgets in your UI

The built-in UI is separated in 3 areas:

![Awesome UI parts](/img/awesome_ui_parts.webp)

1. Top actions built with `topActionsBuilder`
2. Middle content built with `middleContentBuilder`
3. Bottom actions built with `bottomActionsBuilder`

You can customize each builder to return the elements that you want in each part.
Feel free to reuse the included build-in widgets (aspect ratio button, flash button...).

Here is an example of what you can do:

```dart
CameraAwesomeBuilder.awesome(
  // Other parameters
  ...
  // Set an AwesomeTheme that you might reuse in your UI
  theme: AwesomeTheme(
    bottomActionsBackgroundColor: Colors.cyan.withValues(alpha: 0.5),
    buttonTheme: AwesomeButtonTheme(
      backgroundColor: Colors.cyan.withValues(alpha: 0.5),
      iconSize: 20,
      foregroundColor: Colors.white,
      padding: const EdgeInsets.all(16),
      // Tap visual feedback (ripple, bounce...)
      buttonBuilder: (child, onTap) {
        return ClipOval(
          child: Material(
            color: Colors.transparent,
            shape: const CircleBorder(),
            child: InkWell(
              splashColor: Colors.cyan,
              highlightColor: Colors.cyan.withValues(alpha: 0.5),
              onTap: onTap,
              child: child,
            ),
          ),
        );
      },
    ),
  ),
  // Show the filter button on the top part of the screen
  topActionsBuilder: (state) => AwesomeTopActions(
    padding: EdgeInsets.zero,
    state: state,
    children: [
      Expanded(
        child: AwesomeFilterWidget(
          state: state,
          filterListPosition: FilterListPosition.aboveButton,
          filterListPadding: const EdgeInsets.only(top: 8),
        ),
      ),
    ],
  ),
  // Show some Text with same background as the bottom part
  middleContentBuilder: (state) {
    return Column(
      children: [
        const Spacer(),
        // Use a Builder to get a BuildContext below AwesomeThemeProvider widget
        Builder(builder: (context) {
          return Container(
            // Retrieve your AwesomeTheme's background color
            color: AwesomeThemeProvider.of(context)
                .theme
                .bottomActionsBackgroundColor,
            child: const Align(
              alignment: Alignment.bottomCenter,
              child: Padding(
                padding: EdgeInsets.only(bottom: 10, top: 10),
                child: Text(
                  "Take your best shot!",
                  style: TextStyle(
                    color: Colors.white,
                    fontWeight: FontWeight.bold,
                    fontStyle: FontStyle.italic,
                  ),
                ),
              ),
            ),
          );
        }),
      ],
    );
  },
  // Show Flash button on the left and Camera switch button on the right
  bottomActionsBuilder: (state) => AwesomeBottomActions(
    state: state,
    left: AwesomeFlashButton(
      state: state,
    ),
    right: AwesomeCameraSwitchButton(
      state: state,
      scale: 1.0,
      onSwitchTap: (state) {
        state.switchCameraSensor(
          aspectRatio: state.sensorConfig.aspectRatio,
        );
      },
    ),
  ),
)
```

![Custom awesome UI](/img/custom_awesome_ui.jpg)

| Parameter                | Description                                             |
| ------------------------ | ------------------------------------------------------- |
| **topActionsBuilder**    | Top part of the built-in UI                             |
| **middleContentBuilder** | Content between top and bottom parts of the built-in UI |
| **bottomActionsBuilder** | Bottom part of the built-in UI                          |

See `custom_awesome_ui.dart` for an example using these parameters.

### Camera preview positioning

Use these parameters to adjust where you want to place the camera preview in your UI.

```dart
CameraAwesomeBuilder.awesome(
  previewPadding: const EdgeInsets.all(20),
  previewAlignment: Alignment.center,
  // Other parameters
  ...
)
```

![Custom preview positioning](/img/camera_preview_position.jpg)

By default, preview is centered with no padding.

| Parameter            | Description                    |
| -------------------- | ------------------------------ |
| **previewPadding**   | Add padding around the preview |
| **previewAlignment** | Align the preview              |

You can see these parameters in use in `custom_awesome_ui.dart` example.

### Add additional content on top of the preview

If you'd like to place elements on top of the preview, like you would do it in a Stack, use the `previewDecoratorBuilder`.

This feature can be used combined with image analysis to show face filters (like snapchat ones) for example:

![Face filter using previewDecoratorBuilder](/img/face_filter.jpg)

| Parameter                   | Description                        |
| --------------------------- | ---------------------------------- |
| **previewDecoratorBuilder** | Add a widget on top of the preview |

That's what is used in `ai_analysis_faces.dart` to draw contours of the detected face.

## 📝 Complete example

Here is an example showing the complete list of parameters you can set to customize your camera experience:

```dart
CameraAwesomeBuilder.awesome(
  // Bottom actions (take photo, switch camera...)
  bottomActionsBuilder: (state) {
    return AwesomeBottomActions(
      state: state,
      onMediaTap: _handleMediaTap,
    );
  },
  // Clicking on volume buttons will capture photo/video depending on the current mode
  enablePhysicalButton: true,
  // Filter to apply on the preview
  filter: AwesomeFilter.AddictiveRed,
   // Image analysis configuration
  imageAnalysisConfig: AnalysisConfig(
        androidOptions: const AndroidAnalysisOptions.nv21(
            width: 1024,
        ),
        autoStart: true,
  ),
  // Middle content (filters, photo/video switcher...)
  middleContentBuilder: (state) {
    // Use this to add widgets on the middle of the preview
    return Column(
      children: [
        const Spacer(),
        AwesomeFilterWidget(state: state),
        Builder(
          builder: (context) => Container(
            color: AwesomeThemeProvider.of(context)
                .theme
                .bottomActionsBackgroundColor,
            height: 8,
          ),
        ),
        AwesomeCameraModeSelector(state: state),
      ],
    );
  },
  // Handle image analysis
  onImageForAnalysis: (analysisImage) {
    // Do some stuff with the image (see example)
    return processImage(analysisImage);
  },
  onMediaTap: (mediaCapture) {
    // Hande tap on the preview of the last media captured
    print('Tap on ${mediaCapture.filePath}');
  },
  // Handle gestures on the preview, such as tap to focus or scale to zoom
  onPreviewTapBuilder: (state) => OnPreviewTap(
    onTap: (position, flutterPreviewSize, pixelPreviewSize) {
      // Handle tap to focus (default) or take a photo for instance
      // ...
    },
    onTapPainter: (position) {
      // Tap feedback, here we just show a circle
      return Positioned(
        left: position.dx - 25,
        top: position.dy - 25,
        child: IgnorePointer(
          child: Container(
            decoration: BoxDecoration(
              shape: BoxShape.circle,
              border: Border.all(color: Colors.white, width: 2),
            ),
            width: 50,
            height: 50,
          ),
        ),
      );
    },
    // Duration during which the feedback should be shown
    tapPainterDuration: const Duration(seconds: 2),
  ),
  // Handle scale gestures on the preview
  onPreviewScaleBuilder: (state) => OnPreviewScale(
    onScale: (scale) {
      // Do something with the scale value, set zoom for instance
      state.sensorConfig.setZoom(scale);
    },
  ),
  // Alignment of the preview
  previewAlignment: Alignment.center,
    // Add your own decoration on top of the preview
  previewDecoratorBuilder: (state, preview) {
    // This will be shown above the preview (in a Stack)
    // It could be used in combination with MLKit to draw filters on faces for example
    return PreviewDecorationWiget(preview.rect);
  },
  // Preview fit of the camera
  previewFit: CameraPreviewFit.fitWidth,
  // Padding around the preview
  previewPadding: const EdgeInsets.all(20),
  // Show a progress indicator while loading the camera
  progressIndicator: const Center(
    child: SizedBox(
      width: 100,
      height: 100,
      child: CircularProgressIndicator(),
    ),
  ),
   // Define if you want to take photos, videos or both and where to save them
  saveConfig: SaveConfig.photoAndVideo(
    initialCaptureMode: CaptureMode.photo,
    mirrorFrontCamera: true,
    photoPathBuilder: (sensors) async {
      // Return a valid file path (must be a jpg file)
      return SingleCaptureRequest('some/image/file/path.jpg', sensors.first);
    },
    videoPathBuilder: (sensors) async {
      // Return a valid file path (must be a mp4 file)
      return SingleCaptureRequest('some/image/file/path.mp4', sensors.first);
    },
  ),
  // Sensor initial configuration
  sensorConfig: SensorConfig.single(
    aspectRatio: CameraAspectRatios.ratio_4_3,
    flashMode: FlashMode.auto,
    sensor: Sensor.position(SensorPosition.back),
    zoom: 0.0,
  ),
  // CamerAwesome theme used to customize the built-in UI
  theme: AwesomeTheme(
    // Background color of the bottom actions
    bottomActionsBackgroundColor: Colors.deepPurple.withValues(alpha: 0.5),
    // Buttons theme
    buttonTheme: AwesomeButtonTheme(
      // Background color of the buttons
      backgroundColor: Colors.deepPurple.withValues(alpha: 0.5),
      // Buttons icon size
      iconSize: 32,
      // Padding around icons
      padding: const EdgeInsets.all(18),
      // Buttons icon color
      foregroundColor: Colors.lightBlue,
      // Tap visual feedback (ripple, bounce...)
      buttonBuilder: (child, onTap) {
        return ClipOval(
          child: Material(
            color: Colors.transparent,
            shape: const CircleBorder(),
            child: InkWell(
              splashColor: Colors.deepPurple,
              highlightColor: Colors.deepPurpleAccent.withValues(alpha: 0.5),
              onTap: onTap,
              child: child,
            ),
          ),
        );
      },
    ),
  ),
  // Top actions (flash, timer...)
  topActionsBuilder: (state) {
    return AwesomeTopActions(state: state);
  },
  // default filter
  defaultFilter: AwesomeFilter.None,
  // list of photo filters (default to awesome filter) 
  // put null or empty list for hiding filters 
  availableFilters: awesomePresetFiltersList,
)
```

### 🔬 Full list of properties

| Method                      | Comment                                                                                                                                                                                     |
| --------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **aspectRatio**             | Initial aspect ratio of photos and videos taken                                                                                                                                             |
| **bottomActionsBuilder**    | A widget builder used to show buttons on the bottom of the preview.<br/>`AwesomeBottomActions` by default.                                                                                  |
| **enablePhysicalButton**    | When set to true, volume buttons will capture pictures/videos depending on the current mode.                                                                                                |
| **filter**                  | Initial preview filter which will be applied to the photo                                                                                                                                   |
| **imageAnalysisConfig**     | Image format, resolution and autoStart (start analysis immediately or later)                                                                                                                |
| **middleContentBuilder**    | A widget builder used to add widgets above the middle part of the preview (between bottom and top actions).<br/>Shows the filter selector by default.                                       |
| **onImageForAnalysis**      | Callback that will provide an image stream for AI analysis                                                                                                                                  |
| **onMediaTap**              | Choose what you want to do when user tap on the last media captured                                                                                                                         |
| **onPreviewTapBuilder**     | Customize the behavior when the camera preview is tapped (tap to focus by default)                                                                                                          |
| **onPreviewScaleBuilder**   | Customize what to do when the user makes a pinch (pinch to zoom by default)                                                                                                                 |
| **previewAlignment**        | Alignment of the preview                                                                                                                                                                    |
| **previewDecoratorBuilder** | A widget builder used to draw elements around or on top of the preview                                                                                                                      |
| **previewFit**              | One of fitWidth, fitHeight, contain, cover                                                                                                                                                  |
| **previewPadding**          | Padding around the preview                                                                                                                                                                  |
| **progressIndicator**       | Widget to show when loading                                                                                                                                                                 |
| **saveConfig**              | Define if you want to take photos, videos or both and where to save them. You can also set exif preferences, decide to mirror or not front camera outputs and set video recording settings. |
| **sensorConfig**            | The initial sensor configuration: aspect ratio, flash mode, which sensor to use and initial zoom.                                                                                           |
| **theme**                   | Theme used to customize the built-in UI                                                                                                                                                     |
| **topActionsBuilder**       | A widget builder used to show buttons on the top of the preview.<br/>`AwesomeTopActions` by default.                                                                                        |

## 🔨 Need more customization? Other use cases?

If the separation between top, middle and bottom content doesn't suit your needs, you can also provide your own UI using `CameraAwesomeBuilder.custom()` builder.

See [🎨 Creating a custom UI](/getting_started/custom-ui) documentation.

If you only want to display the preview and not taking pictures or videos, you can also use `CameraAwesomeBuilder.previewOnly()` constructor.
You can check [Reading barcodes](image_analysis/reading_barcodes) and [Detecting faces](image_analysis/detecting_faces) examples to see how to use it.

If you are only interested in image analysis, you can also use `CameraAwesomeBuilder.analysisOnly()` which will **not** provide a camera preview behind your `builder`.
In `analysis_image_filter.dart` and `analysis_image_filter_picker.dart`, this constructor is used to draw a camera preview with filter effects applied to each image (image distortion, greyscale, etc...).
You can find more about this example in the [Image analysis formats and conversions documentation](/image_analysis/image_format/conversions).



================================================
FILE: docs/getting_started/custom-ui.mdx
================================================
# 🎨 Creating your own UI

If `CameraAwesomeBuilder.awesome()` doesn't fit your needs in terms of layout, you can create your own UI using the `CameraAwesomeBuilder.custom()` constructor.

The camera preview will be visible behind what you will provide to this builder.

```dart
CameraAwesomeBuilder.custom(
  saveConfig: SaveConfig.photoAndVideo(),
  builder: (cameraState, previewSize, previewRect) {
    // Return your UI (a Widget)
    return cameraState.when(
      onPreparingCamera: (state) => const Center(child: CircularProgressIndicator()),
      onPhotoMode: (state) => TakePhotoUI(state),
      onVideoMode: (state) => RecordVideoUI(state, recording: false),
      onVideoRecordingMode: (state) => RecordVideoUI(state, recording: true),
    );
  },
)
```

You can find more examples on the `example` folder.

## Properties

| Method                    | Comment                                                                                                                                                                                     |
| ------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **aspectRatio**           | Initial aspect ratio of photos and videos taken                                                                                                                                             |
| **builder**               | Create your own interface using the builder method.                                                                                                                                         |
| **enablePhysicalButton**  | When set to true, volume buttons will capture pictures/videos depending on the current mode.                                                                                                |
| **filter**                | Initial preview filter which will be applied to the photo                                                                                                                                   |
| **imageAnalysisConfig**   | Image format, resolution and autoStart (start analysis immediately or later)                                                                                                                |
| **onImageForAnalysis**    | Callback that will provide an image stream for AI analysis                                                                                                                                  |
| **onPreviewTapBuilder**   | Customize the behavior when the camera preview is tapped (tap to focus by default)                                                                                                          |
| **onPreviewScaleBuilder** | Customize what to do when the user makes a pinch (pinch to zoom by default)                                                                                                                 |
| **previewAlignment**      | Alignment of the preview                                                                                                                                                                    |
| **previewFit**            | One of fitWidth, fitHeight, contain, cover                                                                                                                                                  |
| **previewPadding**        | Padding around the preview                                                                                                                                                                  |
| **progressIndicator**     | Widget to show when loading                                                                                                                                                                 |
| **saveConfig**            | Define if you want to take photos, videos or both and where to save them. You can also set exif preferences, decide to mirror or not front camera outputs and set video recording settings. |
| **sensorConfig**          | The initial sensor configuration: aspect ratio, flash mode, which sensor to use and initial zoom.                                                                                           |
| **theme**                 | Theme used to customize the built-in UI                                                                                                                                                     |

## Builder method

The `builder` method is the main method here.

```dart
typedef CameraLayoutBuilder = Widget Function(CameraState cameraModeState, PreviewSize previewSize, Rect previewRect);
```

CamerAwesome works using a state pattern to make sure you can only call methods available on the current camera state.
The magic is that you don't have to do anything apart calling some methods using the camera state.

```dart
state.when(
    onAnalysisOnlyMode: (analysisCameraState) => analysisCameraState.startAnalysis(),
    onPhotoMode: (photoCameraState) => photoCameraState.takePhoto(),
    onVideoMode: (videoCameraState) => videoCameraState.startRecording(),
    onVideoRecordingMode: (videoRecordingCameraState) => videoRecordingCameraState.stopRecording(),
    onPreparingCamera: (preparingCameraState) => Loader(),
    onPreviewMode: (previewCameraState) => previewModeState.focus(),
);
```

`previewSize` and `previewRect` are additional parameters that might be used to position your UI around or on top of the camera preview.

### CamerAwesome has 6 different states

- **PreparingCameraState** : camera is starting
- **PhotoCameraState** : camera is ready to take a photo
- **VideoCameraState** : camera is ready to take a video
- **VideoRecordingCameraState** : camera is taking a video
- **PreviewCameraState** : camera is in preview only mode
- **AnalysisCameraState** : camera is in analysis only mode

Here is a schema showing the interactions between states:
![Camera states interactions](/img/camera_states_interactions.png)

As you can see, after the initial `PreparingCameraState`, the new state is either `PhotoCameraState` or `VideoCameraState`.
A `VideoRecordingCameraState` replaces the `VideoCameraState` when a recording starts. You can't start two recording at the same time thanks to this.
When the recording stops, a `VideoCameraState` replaces it again.

### You don't have to worry about state management here

`CameraAwesomeBuilder` calls the `builder` method each time you switch between camera states.
This way, you can react to these changes easily in your `builder` 👌

## Creating my own widget

`CameraState` lets you build a reactive UI by providing you streams and setters to the various properties around the camera.
It should let you create everything you need in a reactive way without worrying about the camera flow.

You can get inspiration on how we built every widgets.

**Example**

```dart
class AwesomeFlashButton extends StatelessWidget {
  final CameraState state;

  const AwesomeFlashButton({
    super.key,
    required this.state,
  });

  @override
  Widget build(BuildContext context) {
    return StreamBuilder<SensorConfig>(
      // Listen to the current SensorConfig. It might change when switching between front and back cameras.
      stream: state.sensorConfig$,
      builder: (_, sensorConfigSnapshot) {
        if (!sensorConfigSnapshot.hasData) {
          return const SizedBox.shrink();
        }
        final sensorConfig = sensorConfigSnapshot.requireData;
        return StreamBuilder<FlashMode>(
          // Listen to the currently selected flash mode
          stream: sensorConfig.flashMode$,
          builder: (context, snapshot) {
            if (!snapshot.hasData) {
              return Container();
            }
            return _FlashButton.from(
              // Build your button differently based on the current Flash mode, with different icons for instance
              flashMode: snapshot.requireData,
              onTap: () => sensorConfig.switchCameraFlash(),
            );
          },
        );
      },
    );
  }
}
```

In the snippet above, the widget first listens to `CameraState.sensorConfig$` to get the most up-to-date `SensorConfig`. It might change when switching between back and front camera for instance.

Once you have your `SensorConfig`, you can listen to their properties like the flash mode.

Since we want to listen to its changes, we use `flashMode$`, which is a `Stream` of the current flash mode.

> 👌 Every getter terminated with the $ are streams in CamerAwesome.
>
> The equivalent without $ is the current value. You should not store these in variables since they may change over time.

## Using provided widgets

You can find common widgets that you may want to use in the **Widgets** section.

Many of them use the `AwesomeTheme` provided to `CameraAwesomeBuilder`.
Feel free to customize it and reuse this theme in your own UI.
See the [theme](/theme) section for more details.

Instead of handling it yourself, using the built-in widgets can let you rotate your buttons automatically when the phone rotates with [AwesomeOrientedWidget](/widgets/awesome_oriented_widget) for instance.

Check also built-in [buttons](/widgets/buttons) and the [camera mode selector](/widgets/camera_mode_selector).

## Setting and reading camera properties

If you need more customization, you can find details on how to access and update the properties of the camera below.

Note that we recommend to access properties via their `Stream` whenever possible.
If you need it to build your UI, just use it with a `StreamBuilder`.

### Camera sensor properties and methods

`CameraState` gives access to the current `SensorConfig` (via a Stream or a getter).
You will use this object to get or set different sensor related properties.

See the tables below for each use case.

**Flash**

| Use case                                 | Code                                     |
| ---------------------------------------- | ---------------------------------------- |
| **Switch** between different flash modes | `state.sensorConfig.switchCameraFlash()` |
| **Set** a specific flash mode            | `state.sensorConfig.setFlashMode()`      |
| **Get** current flash mode               | `state.sensorConfig.flashMode`           |
| **Stream** of the current flash mode     | `state.sensorConfig.flashMode$`          |

**Aspect ratio**

| Use case                                  | Code                                     |
| ----------------------------------------- | ---------------------------------------- |
| **Switch** between different aspect ratio | `state.sensorConfig.switchCameraRatio()` |
| **Set** a specific aspect ratio           | `state.sensorConfig.setAspectRatio()`    |
| **Get** current aspect ratio              | `state.sensorConfig.aspectRatio`         |
| **Stream** of the current aspect ratio    | `state.sensorConfig.aspectRatio$`        |

**Zoom**

| Use case                             | Code                           | Comment                                                     |
| ------------------------------------ | ------------------------------ | ----------------------------------------------------------- |
| **Set** a specific zoom value        | `state.sensorConfig.setZoom()` | Zoom value must be between 0.0 (no zoom) and 1.0 (max zoom) |
| **Get** current zoom value           | `state.sensorConfig.zoom`      |                                                             |
| **Stream** of the current zoom value | `state.sensorConfig.zoom$`     |                                                             |

**Brightness**

| Use case                                   | Code                                 |
| ------------------------------------------ | ------------------------------------ |
| **Set** a specific brightness              | `state.sensorConfig.setBrightness()` |
| **Get** current brightness value           | `state.sensorConfig.brightness`      |
| **Stream** of the current brightness value | `state.sensorConfig.brightness$`     |

### Methods and properties available to any CameraState

If you want to access more than just the current `SensorConfig`, you can explore what the different `CameraStates` provide.

First of all, they all give you the following features:

| Use case                                   | Code                         |
| ------------------------------------------ | ---------------------------- |
| **Switch** between FRONT and BACK camera   | `state.switchCameraSensor()` |
| **Get** current Sensor configuration       | `state.sensorConfig`         |
| **Stream** of current Sensor configuration | `state.sensorConfig$`        |
| **Get** original SaveConfig                | `state.saveConfig`           |

More features are available depending on which `CameraState` is in use.

### PhotoCameraState properties and methods

**Take a photo**

| Use case     | Code                |
| ------------ | ------------------- |
| Take a photo | `state.takePhoto()` |

**Toggle to save (or not) the location when taking photos**

| Use case                      | Code                           |
| ----------------------------- | ------------------------------ |
| **Set** saveGpsLocation       | `state.saveGpsLocation = true` |
| **Get** saveGpsLocation       | `state.saveGpsLocation`        |
| **Stream** of saveGpsLocation | `state.saveGpsLocation$`       |

### VideoCameraState properties and methods

In this state, you didn't start recording yet.

| Use case                       | Code                     | Comment                                                                                               |
| ------------------------------ | ------------------------ | ----------------------------------------------------------------------------------------------------- |
| Start recording a video        | `state.startRecording()` | This will push a `VideoRecordingCameraState`                                                          |
| Enable/Disable audio recording | `state.enableAudio()`    | Must be set before starting a recording. Once started, it can't be changed for the current recording. |

### VideoRecordingCameraState properties and methods

In this state, the video recording has started.

| Use case           | Code                      | Comment                                                    |
| ------------------ | ------------------------- | ---------------------------------------------------------- |
| Pause a recording  | `state.pauseRecording()`  | A paused recording must not be paused again                |
| Resume a recording | `state.resumeRecording()` | A recording not paused should not call `resumeRecording()` |
| Stop a recording   | `state.stopRecording()`   | This will push a `VideoCameraState`                        |



================================================
FILE: docs/getting_started/installing.mdx
================================================
## 📖 Installation

### Add the package in your pubspec.yaml

```yaml
dependencies:
    camerawesome: ^2.0.0
    ...
```

**Please check the last version on [pub.dev](https://pub.dev/packages/camerawesome)**

### ✔️ Platform specific setup

#### iOS Setup

Add these permissions on `ios/Runner/Info.plist` file:

```xml
<key>NSCameraUsageDescription</key>
<string>Your own description</string>

<key>NSMicrophoneUsageDescription</key>
<string>To enable microphone access when recording video</string>

<key>NSLocationWhenInUseUsageDescription</key>
<string>To enable GPS location access for Exif data</string>
```

#### Android Setup

Change the minimum SDK version to 21 (or higher) in `android/app/build.gradle`

```
minSdkVersion 21
```

In order to be able to take pictures or record videos, you may need additional permissions depending on the Android version and where you want to save them.
Read more about it in the [official documentation](https://developer.android.com/training/data-storage).

> `WRITE_EXTERNAL_STORAGE` is not included in the plugin starting with version 1.4.0.

To record videos with audio, add this permission to your `AndroidManifest.xml`:

```xml
<manifest xmlns:android="http://schemas.android.com/apk/res/android"
    package="com.example.yourpackage">
    <uses-permission android:name="android.permission.RECORD_AUDIO" />

    <!-- Other declarations -->
</manifest>
```

You may also want to save location of your pictures in exif metadata. In this case, add below permissions:

```xml
<manifest xmlns:android="http://schemas.android.com/apk/res/android"
    package="com.example.yourpackage">
    <uses-permission android:name="android.permission.ACCESS_FINE_LOCATION" />
    <uses-permission android:name="android.permission.ACCESS_COARSE_LOCATION" />

    <!-- Other declarations -->
</manifest>
```

<details>
<summary>⚠️ Overriding Android dependencies</summary>

Some of the dependencies used by CamerAwesome can be overriden if you have a conflict.
Change these variables to define which version you want to use:

```gradle
buildscript {
    ext.kotlin_version = '1.7.10'
    ext {
        // You can override these variables
        compileSdkVersion = 33
        minSdkVersion = 24 // 21 minimum
        playServicesLocationVersion = "20.0.0"
        exifInterfaceVersion = "1.3.4"
    }
    // ...
}
```

Only change these variables if you are sure of what you are doing.

For example, setting the Play Services Location version might help you when you have conflicts with other plugins.
The below line shows an example of these conflicts:

```
java.lang.IncompatibleClassChangeError: Found interface com.google.android.gms.location.ActivityRecognitionClient, but class was expected
```

</details>

### Import the package in your Flutter app

```dart
import 'package:camerawesome/camerawesome_plugin.dart';
```

You are now ready to use CamerAwesome 👌



================================================
FILE: docs/getting_started/multicam.mdx
================================================
# 📷 📷 Multiple cameras at once (🚧 BETA)

To enable concurrent cameras feature, you need to give `CameraAwesomeBuilder` a `SensorConfig` with multiple sensors:

```dart
CameraAwesomeBuilder.awesome(
    // 1.
    sensorConfig: SensorConfig.multiple(
        // 2.
        sensors: [
            Sensor.position(SensorPosition.back),
            Sensor.position(SensorPosition.front),
        ],
        // 3.
        flashMode: FlashMode.auto,
        aspectRatio: CameraAspectRatios.ratio_16_9,
    ),
    // Other params
)
```

The main points of interest are the following:

1. Instead of using the `SensorConfig.single` constructor, use `SensorConfig.multiple`.
2. This constructor lets you define a list of sensors instead of a single one.
3. Then, you can set regular sensor parameters like `flashMode` or `aspectRatio`.

## Feature support

Not all devices support the concurrent cameras feature. Keep in mind that it can be resource intensive.

Check the following method to determine if the feature is supported on the current device:

```dart
final isSupported = await CamerawesomePlugin.isMultiCamSupported()
```

## Customizing the picture-in-picture preview

The `pictureInPictureConfigBuilder` parameter lets you customize the preview of the additional sensors.

A `PictureInPictureConfigBuilder` is a function that is called with the index of the sensor and the sensor itself as parameters and returns a `PictureInPictureConfig` object.

Here is a sample code taken from the `multi_camera.dart` example:

```dart
CameraAwesomeBuilder.awesome(
 pictureInPictureConfigBuilder: (index, sensor) {
      const width = 200.0;
      return PictureInPictureConfig(
        // 1.
        isDraggable: false,
        // 2.
        startingPosition: Offset(
          screenSize.width - width - 20.0 * index,
          screenSize.height - 356,
        ),
        // 3.
        sensor: sensor,
        // 4.
        onTap: (){
          print('on preview tap');
        },
        // 5.
        pictureInPictureBuilder: (preview, aspectRatio) {
          return SizedBox(
            width: width,
            height: width,
            child: ClipPath(
              clipper: _MyCustomPipClipper(
                width: width,
                height: width * aspectRatio,
                shape: shape,
              ),
              child: SizedBox(
                width: width,
                // 6.
                child: preview,
              ),
            ),
          );
        },
      );
    },
)
```

Let's break it down:

1. Define if you want the preview to be draggable or not using the `isDraggable` parameter.
2. Choose the `startingPosition` of the preview. You may adjust it depending on the index of the sensor.
3. Set for which `sensor` this preview is.
4. Add an `onTap` callback.
5. Customize how you want the preview to be displayed using the `pictureInPictureBuilder`. This builder must display the `preview` widget. You may also use the `aspectRatio` of the preview to adjust the size of the widget.

## Get the list of sensors

You can get the list of all the sensors available on iOS with:

```dart
final sensorDeviceData = await CamerawesomePlugin.getSensors();
```

## Maximum number of concurrent cameras

Although the code lets you define any number of sensors, each platform has its limits regarding the number of cameras you can open simultaneously.

| Platform | Max number of cameras |
| -------- | --------------------- |
| Android  | 2                     |
| iOS      | 3                     |

Providing more cameras may result in unexpected behaviour.


## Capturing multiple pictures

You can capture multiple pictures at once with the regular `takePhoto()` method:

```dart
await photoCameraState.takePhoto();
```

Then, listen to `cameraState.captureState$` in order to retrieve the last medias captured.

A `MediaCapture` object contains a `CaptureRequest` which might be either a `SingleCaptureRequest` or a `MultipleCaptureRequest`, depending on the number of sensors used.

You can use the `when` operator to deal with this or directly cast it to one of the mentionned classes.

Here is an example which handles the preview tap:

```dart
CameraAwesomeBuilder.awesome(
    ...
    onMediaTap: (mediaCapture) {
        mediaCapture.captureRequest.when(
        // 1.
        single: (single) => OpenFile.open(single.file?.path),
        // 2.
        multiple: (multiple) => Navigator.of(context).pushNamed(
                '/gallery',
                arguments: multiple,
            ),
        );
    },
)
```
In this example, we use the `when` operator to handle each case:
1. If it's a `SingleCaptureRequest`, we open the file directly.
2. If it's a `MultipleCaptureRequest`, we navigate to a new page and pass the `MultipleCaptureRequest` object as an argument. This page could be used to display all the pictures taken for instance.



## Capturing multiple videos

Concurrent camera video recording support is not ready yet.


## Limitations

### Sensor settings

Sensor settinigs like `flashMode` or `aspectRatio` are only applied to the first sensor in the list (let's call it the main sensor).

### Sensors used on Android

The sensors used are not necessarly the ones given in the list of sensors.

There is a concept of pairs of concurrent cameras on this platform which implies that only some specific pairs will be compatible with each other.

For now, the sensors used are always one from the front and one from the back of the device.

### Analysis mode with concurrent cameras

This feature is not ready yet and might not be as good as you would expect: it would require even more resources.

### Differences between pictures taken and Preview

The preview shows the additional sensors as picture-in-picture.

This is not what is captured by CamerAwesome: instead, a picture for each sensor is individually captured.

For now, you are responsible to merge them into one picture (or use a `Widget` to position each image as you want).


## 🗣️ Feedback

If you are using this feature or have any feedback regarding it, please share it with us in a [new issue](https://github.com/Apparence-io/CamerAwesome/issues/new/choose).


================================================
FILE: docs/image_analysis/detecting_faces.mdx
================================================
## Detecting faces

![Read barcodes](/img/face_ai.gif)

With MLKit, detecting faces is very similar to scanning barcodes.
Instead of a ```BarcodeScanner```, you need to use a ```FaceDetector```.
However, if you want to draw a mask on top of the face for example, things get more complicated.
Let's see how it can be done with CamerAwesome in ```ai_analysis_faces.dart```.

First, you need to use MLKit's face detection library:
``` dart
google_mlkit_face_detection: ^0.5.5
```

Next you'll need a `FaceDetector`:
``` dart
final options = FaceDetectorOptions(
  enableContours: true,
  enableClassification: true,
  enableLandmarks: true,
);
late final faceDetector = FaceDetector(options: options);
```

MLKit is now ready, let's setup CamerAwesome.

### CamerAwesome setup

``` dart
// 1.
CameraAwesomeBuilder.previewOnly(
  // 2.
  previewFit: CameraPreviewFit.contain,
  sensorConfig: SensorConfig.single(
    sensor: Sensor.position(SensorPosition.front),
    aspectRatio: CameraAspectRatios.ratio_1_1,
  ),
  // 3.
  onImageForAnalysis: (img) => _analyzeImage(img),
  // 4.
  imageAnalysisConfig: AnalysisConfig(
    androidOptions: const AndroidAnalysisOptions.nv21(
      width: 250,
    ),
    maxFramesPerSecond: 5, // depending on your phone performances
  ),
  // 5.
  builder: (state, previewSize, previewRect) {
    return _MyPreviewDecoratorWidget(
      cameraState: state,
      faceDetectionStream: _faceDetectionController,
      previewSize: previewSize,
      previewRect: previewRect,
    );
  },
)
```

1. In this example, we just want to draw face contours above the preview. There is no need for other elements in the UI, so we use the `previewOnly()` constructor.
2. (Optional) Next arguments determine that we want an aspect ratio of 1:1 that is contained in the screen (CameraPreviewFit.contain) and that the starting sensor is the front one.
3. ```onImageForAnalysis``` is called whenever a new `AnalysisImage` is available. That's where you might want to try to detect faces.
4. Provide your ```AnalysisConfig```. You don't need an high resolution to detect faces and it will be much faster with a low resolution. For good performances, we also set maxFramesPerSecond to 20.
7. Draw your UI in the `builder`. If you want to draw above the preview, `previewSize` and `previewRect` will be useful.

Now let's see how the face detection is done.


### Detecting faces in an AnalysisImage

Most complicated part of the detection is to convert an `AnalysisImage` to an `InputImage`.
To do that, we've created an extension function in the example project:
```dart
import 'package:camerawesome/camerawesome_plugin.dart';
import 'package:google_mlkit_face_detection/google_mlkit_face_detection.dart';

extension MLKitUtils on AnalysisImage {
  InputImage toInputImage() {
    // 1.
    final planeData =
        when(nv21: (img) => img.planes, bgra8888: (img) => img.planes)?.map(
      (plane) {
        return InputImagePlaneMetadata(
          bytesPerRow: plane.bytesPerRow,
          height: height,
          width: width,
        );
      },
    ).toList();

    // 2.
    return when(nv21: (image) {
      // 3.
      return InputImage.fromBytes(
          bytes: image.bytes,
          metadata: InputImageMetadata(
            rotation: inputImageRotation,
            format: InputImageFormat.nv21,
            size: image.size,
            bytesPerRow: image.planes.first.bytesPerRow,
          ),
        );
    }, bgra8888: (image) {
      // 4. 
      final inputImageData = InputImageData(
        size: size,
        imageRotation: inputImageRotation,
        inputImageFormat: inputImageFormat,
        planeData: planeData,
      );

      // 5.
      return InputImage.fromBytes(
        bytes: image.bytes,
        inputImageData: inputImageData,
      );
    })!;
  }

  InputImageRotation get inputImageRotation =>
      InputImageRotation.values.byName(rotation.name);

  InputImageFormat get inputImageFormat {
    switch (format) {
      case InputAnalysisImageFormat.bgra8888:
        return InputImageFormat.bgra8888;
      case InputAnalysisImageFormat.nv21:
        return InputImageFormat.nv21;
      default:
        return InputImageFormat.yuv420;
    }
  }
}
```

Let's break down the above code:
1. Get the planes from the `AnalysisImage` and map them to `InputImagePlaneMetadata`.
2. Use the `when()` function to handle both Android and iOS cases.
3. Convert the `Nv21Image` to MLKit's `InputImage`
4. Define the `InputImageData` for the `InputImage` on iOS.
5. Convert the `Bgra8888Image` to MLKit's `InputImage` thanks to `InputImageData`.


Thanks to this extension function, we can now easily convert an `AnalysisImage` to an `InputImage` and use it to detect faces:
``` dart
Future _analyzeImage(AnalysisImage img) async {
  // 1.
  final inputImage = img.toInputImage();

  try {
    // 2.
    _faceDetectionController.add(
      FaceDetectionModel(
        faces: await faceDetector.processImage(inputImage),
        absoluteImageSize: inputImage.inputImageData!.size,
        rotation: 0,
        imageRotation: img.inputImageRotation,
        croppedSize: img.croppedSize,
      ),
    );
  } catch (error) {
    debugPrint("...sending image resulted error $error");
  }
}
```

This code snippet is quite short:
1. Use our extension function to convert `AnalysisImage` to `InputImage`.
2. Give the `InputImage` to MLKit's `FaceDetector` to eventually detect faces. Next, wrap these in a model that is added to a stream.

The result of the face detection will be handled by a `StreamBuilder`: each time a new result is emitted, the UI will be updated with this result.


### Drawing face contours

Now that our model is ready, we can focus on the UI code.

First, take a look at `_MyPreviewDecoratorWidget`:

``` dart
class _MyPreviewDecoratorWidget extends StatelessWidget {
  final CameraState cameraState;
  final Stream<FaceDetectionModel> faceDetectionStream;
  final Preview preview;

  const _MyPreviewDecoratorWidget({
    required this.cameraState,
    required this.faceDetectionStream,
    required this.preview,
  });

  @override
  Widget build(BuildContext context) {
    return IgnorePointer(
      child: StreamBuilder(
        // 1.
        stream: cameraState.sensorConfig$,
        builder: (_, snapshot) {
          if (!snapshot.hasData) {
            return const SizedBox();
          } else {
            return StreamBuilder<FaceDetectionModel>(
              // 2.
              stream: faceDetectionStream,
              builder: (_, faceModelSnapshot) {
                if (!faceModelSnapshot.hasData) return const SizedBox();
                // this is the transformation needed to convert the image to the preview
                // Android mirrors the preview but the analysis image is not
                final canvasTransformation = faceModelSnapshot.data!.img
                    ?.getCanvasTransformation(preview);
                // 3.
                return CustomPaint(
                  painter: FaceDetectorPainter(
                    model: faceModelSnapshot.requireData,
                    canvasTransformation: canvasTransformation,
                    preview: preview,
                  ),
                );
              },
            );
          }
        },
      ),
    );
  }
}
```
Here is a break through of the above code:
1. Listen to the `sensorConfig$` stream. This will be used to determine if it's the front or back camera.
Indeed, image analysis on Android is not mirrored but the preview is. In this case, we will need to make adjustments.
2. Listen to `faceDetectionStream`. Results of the face detection are emitted to this stream and we want to rebuild the UI each time there is a new result.
3. Widgets are not sufficient enough to be able to draw contours out of the box. A CustomPainter is the way to go here.

Now let's see how our `FaceDetectorPainter` works by looking at its `paint()` method.

``` dart
@override
void paint(Canvas canvas, Size size) {
  if (preview == null || model.img == null) {
      return;
    }
    // 1
    // We apply the canvas transformation to the canvas so that the barcode
    // rect is drawn in the correct orientation. (Android only)
    if (canvasTransformation != null) {
      canvas.save();
      canvas.applyTransformation(canvasTransformation!, size);
    }
    // 2
    for (final Face face in model.faces) {
      // 3  
      Map<FaceContourType, Path> paths = {
        for (var fct in FaceContourType.values) fct: Path()
      };
      // 4
      face.contours.forEach((contourType, faceContour) {
        if (faceContour != null) {
          // 5
          paths[contourType]!.addPolygon(
              faceContour.points
                  .map(
                    (element) => preview!.convertFromImage(
                      Offset(element.x.toDouble(), element.y.toDouble()),
                      model.img!,
                    ),
                  )
                  .toList(),
              true);
          // 6
          for (var element in faceContour.points) {
            var position = preview!.convertFromImage(
              Offset(element.x.toDouble(), element.y.toDouble()),
              model.img!,
            );
            canvas.drawCircle(
              position,
              4,
              Paint()..color = Colors.blue,
            );
          }
        }
      });
      // 7
      paths.removeWhere((key, value) => value.getBounds().isEmpty);
      // 8
      for (var p in paths.entries) {
        canvas.drawPath(
            p.value,
            Paint()
              ..color = Colors.orange
              ..strokeWidth = 2
              ..style = PaintingStyle.stroke);
      }
    }
    // if you want to draw again without canvas transformation, use this:
    if (canvasTransformation != null) {
      canvas.restore();
    }
}
```
Here is a short break through of the above code:
1. On Android, image rotation and the fact that the image is not mirrored on the front camera implies to make some calculations on the canvas to position properly the elements.
2. Handle each face detected
3. Initialize a map of each contourType to an empty Path that will be eventually filled later.
4. Iterate over the contours of the face.
5. Add points of the contour to a Path as a polygon. In a real world scenario, you might want to proceed differently.
6. Draw a circle at each contour's points coordinates in blue.
7. Filter the contours not found.
8. Draw the contours that we found in orange.


## Canvas applyTransformation

```dart
canvas applyTransformation(canvasTransformation!, size);
```

Here what it does

Let's explain what's going on:
1. The image from image analysis might not reflect what is seen on the camera preview due to a difference between their aspect ratio.
For instance, image analysis could return a picture in 600x800 (ratio 3:4) while the camera preview could be at 1000x1000.
The croppedSize is the part of the imageAnalysis that is visible in the cameraPreview. It would be 600x600 for the above example.
`imageDiffX` and `imageDiffY` are here to transpose MLKit's points coordinates in the part of the image analysis that is visible in the camera preview.
2. absoluteImageSize width and height are inverted on Android.
3. Transposition of the point coordinates in the image analysis to its cropped equivalent.
4. Ratio between the preview and the cropped image size. Example: a point at (300, 300) of the image analysis which size is 600x600 would become (500, 500) for a preview size of 1000x1000.
5. The painter size might be taller or wider than the preview size (or its equivalent, croppedSize * ratio). In these cases, the preview is centered. The position should be translated accordingly.


Drawing on top of the preview is definitively the more complicated part here, but as you saw it is still doable ✌️

See the [complete example](https://github.com/Apparence-io/camera_awesome/blob/master/example/lib/ai_analysis_faces.dart) if you need more details.



================================================
FILE: docs/image_analysis/image_analysis_configuration.mdx
================================================
# Image analysis configuration

CamerAwesome providess a stream of images that you can use to make image analysis.

The stream differ slightly between depending on the platform:

- on **Android**, it is provided by the imageAnalysis use case of CameraX. It may be different that what you see on the camera preview (lower resolution, different aspect ratio, image not mirrored for front camera).
- on **iOS**, the image analysis stream and the preview come from the same source. In order to not struggle too much with performance, this mode will reduce preview resolution to be able to make analysis on each image.

Image analysis implies a lot of calculations, even if you make them with a package like MLKit.
Most of the time, your analysis can be done with a low resolution image and it will be much easier to do.

Note also that trying to analyze too much images at the same time might have unexpected behaviours.

In order to deal with these issues, you can provide your own `AnalysisConfig`:

```dart
CameraAwesomeBuilder.awesome(
    // Other parameters...
    onImageForAnalysis: (AnalysisImage img) {
        // Handle image analysis
    },
    imageAnalysisConfig: AnalysisConfig(
        // 1.
        androidOptions: const AndroidAnalysisOptions.nv21(
            width: 250,
        ),
        // 2.
        autoStart: true,
        // 3.
        cupertinoOptions: const CupertinoAnalysisOptions.bgra8888(),
        // 4.
        maxFramesPerSecond: 20,
    ),
)
```

Here is an explanation of the above settings:

1. `androidOptions` is the Android specific configuration used for image analysis, with a given format (as its `nv21()` constructor suggests) and a target `width`.
2. `autoStart` is a boolean that tells if the image analysis should start immediately or not. If it is `false`, you will have to start it manually by calling `analysisController.start()` (see below).
3. `cupertinoOptions`is the iOS specific configuration for image analysis. It only supports BGRA_8888 and the resolution is not configurable.
4. `maxFramesPerSecond` is the maximum number of images sent for analysis per second. If you set it to `null`, it will send as much images as possible.

This configuration is done to detect faces. `width` and `maxFramesPerSecond` are quite low to get good performances.

## Doing video recording and image analysis at the same time

⚠️ On Android, some devices don't support video recording and image analysis at the same time.

- If they don't, image analysis will be ignored.
- You can check if a device has this capability by using `CameraCharacteristics .isVideoRecordingAndImageAnalysisSupported(Sensors.back)`.

You can find more details about this in the [official documentation](https://developer.android.com/training/camerax/architecture) of CameraX.

## Starting and stopping image analysis manually

You can access the `analysisController` from a `CameraState`.

| AnalysisController method and parameters | Description                                                                                                           |
| ---------------------------------------- | --------------------------------------------------------------------------------------------------------------------- |
| enabled                                  | Toggle to determine if image analysis is enabled or not. An image analysis must have been set or it will return false |
| start()                                  | Start image analysis. Your callback will be called with the new images provided for analysis.                         |
| stop()                                   | Stop image analysis.                                                                                                  |

You can `start()` and `stop()` image analysis as many times as you want.

> An alternative way to stop doing image analysis is to do nothing in your callback when you don't want to analyze anymore.
> However, images are still retrieved on the native side, converted and transferred to Dart, making it very inefficient.
> We recommend you to stop analysis if you don't want to analyze anything by calling `analysisController.stop()`.

After having configured your image analysis, you can start making the actual analysis thanks to `onImageForAnalysis`.

The `example` folder contains three examples using MLKit:

- `ai_analysis_barcode.dart` reads barcodes, with ability to pause and resume image analysis
- `preview_overlay_example.dart` draws a rect around the detected barcode and detects if it's within an area
- `ai_analysis_faces.dart` detects if there is a face on the camera feed and draws its contours when there is one

A detailed explanation of each example is available in [Reading barcodes](/image_analysis/reading_barcodes) and [Detecting faces](/image_analysis/detecting_faces).

See also details on the [AnalysisImage format and conversions](/image_analysis/image_format_conversions).

## iOS preview mode only publishing

If you want to use the preview-only feature on iOS, you are not required to set the microphone description permission in your `Info.plist` file.
However, keep in mind that the App Store has the ability to detect if your app is utilizing the AVAudioSession API (which is included by default in the CamerAwesome plugin).

If your app does not plan to use the microphone at all and you want to use the preview-only feature, you can add the following to your `Podfile`:
```
post_install do |installer|
  installer.pods_project.targets.each do |target|
    flutter_additional_ios_build_settings(target)
    
    # ADD THE NEXT SECTION
    target.build_configurations.each do |config|
      config.build_settings['GCC_PREPROCESSOR_DEFINITIONS'] ||= [
        '$(inherited)',
        'AUDIO_SESSION_MICROPHONE=0'
      ]
    end
    
  end
end
```

This piece of code will remove all occurrences of the microphone API in the iOS project, and you will be able to pass the review without any problems.


================================================
FILE: docs/image_analysis/image_format_conversions.mdx
================================================
# Image analysis formats

`onImageForAnalysis` is triggered every time a new image is available for analysis.
`AnalysisImage` is an abstract class and its implementation will depend on the platform and on the `AnalysisConfig` you have defined.
You can use the `when()` helper function to decide what to do depending on the format:

```dart
final Widget? result = img.when(jpeg: (JpegImage image) {
    return handleJpeg(image);
}, yuv420: (Yuv420Image image) {
    return handleYuv420( image);
}, nv21: (Nv21Image image) {
    return handleNv21(image);
}, bgra8888: (Bgra8888Image image) {
    return handleBgra8888(image);
})
```

In the above example, handle methods are expected to return a `Widget`.
You can ommit any of the format if you'd like (that's why `result` might be `null`).

# Displaying an AnalysisImage

An `Image` widget can be used to display an `AnalysisImage` but you may need to convert it to a format that can be displayed first.

## AnalysisImage conversion

The easiest conversion to display an `AnalysisImage` is to convert it to JPEG.

Let's check how to convert each `AnalysisImage implementation.

### JpegImage

This implementation is already in JPEG, so you can simply display it without additional conversion:

```dart
final Widget? result = img.when(jpeg: (JpegImage image) {
    return Image.memory(image.bytes);
})
```

However, image analysis is often done on other image formats such as NV21 so you will probably end up using an other format.

### YUV_420_888 and NV21

You might succeed in converting these formats in pure dart, but performances will probably not be good and it's not easy to do.
Instead, use `toJpeg()` on `Yuv420Image` and `Nv21Image` instances to convert them to `JpegImage`.
This method will make the conversion on the native side so it returns a `Future`.

Example:

```dart
final Widget? result = img.when(nv21: (Nv21Image image) {
    return FutureBuilderr<JpegImage>(
        future: image.toJpeg(),
        builder: (BuildContext context, AsyncSnapshot<JpegImage> snapshot) {
            if (snapshot.hasData) {
                return Image.memory(snapshot.data!.bytes);
            } else {
                return SizedBox();
            }
        },
    );
});
```

### Bgra8888Image

The iOS format is rather simple to convert in dart with the use of the `image` package:

```dart
Uint8List convertToJpeg(Bgra8888Image image){
    return imglib.encodeJpg(
        imglib.Image.fromBytes(
            width: image.width,
            height: image.height,
            bytes: image.planes[0].bytes.buffer,
            order: imglib.ChannelOrder.bgra,
        ),
        quality: 100,
    );
}
```

An alternative using native conversion is to simply call `toJpeg()` on the `Bgra8888Image` instance.
It might be better in terms of performances but it returns a `Future`, like `Nv21Image.toJpeg()` and `Yuv420Image.toJpeg()`.

# Example usage

`image` package provides a [variety of effects](https://github.com/brendan-duncan/image/blob/main/doc/filters.md) that you can apply on an image.

In the below example, we will apply a billboard effect to each analysis image and display the result.
Since we display images one after the other, it will look like a camera preview with a filter applied on it:

![Billboard effect](myimage.gif)


Let's start with a basic CamerAwesome setup:

```dart
class CameraPage extends StatefulWidget {
  const CameraPage({super.key});

  @override
  State<CameraPage> createState() => _CameraPageState();
}

class _CameraPageState extends State<CameraPage> {
  // 1.
  final _imageStreamController = StreamController<AnalysisImage>();

  @override
  void dispose() {
    _imageStreamController.close();
    super.dispose();
  }

  @override
  Widget build(BuildContext context) {
    return Scaffold(
      // 2.
      body: CameraAwesomeBuilder.analysisOnly(
        sensorConfig: SensorConfig.single(
          sensor: Sensor.position(SensorPosition.front),
          aspectRatio: CameraAspectRatios.ratio_1_1,
        ),
        // 3.
        onImageForAnalysis: (img) async => _imageStreamController.add(img),
        imageAnalysisConfig: AnalysisConfig(
          androidOptions: const AndroidAnalysisOptions.yuv420(
            width: 150,
          ),
          maxFramesPerSecond: 30,
        ),
        builder: (state, previewSize, previewRect) {
          // 4.
          return CameraPreviewDisplayer(
            analysisImageStream: _imageStreamController.stream,
          );
        },
      ),
    );
  }
}
```

This part is quite simple but let's explain it a bit:

1. We create a `StreamController` to send each `AnalysisImage` to a widget that will display them.
2. The `CameraAwesomeBuilder.analysisOnly` builder is the most appropriate since the image analysis itself will be used to display a preview of the camera. We don't need an other camera preview.
3. `onImageForAnalysis` is called every time a new image is available for analysis. This example simply adds the image to the stream.
4. `CameraPreviewDisplayer` is where we'll handle the stream of images (given as argument).

Now let's create the `CameraPreviewDisplayer` widget:

```dart
class CameraPreviewDisplayer extends StatefulWidget {
  final Stream<AnalysisImage> analysisImageStream;

  const CameraPreviewDisplayer({
    super.key,
    required this.analysisImageStream,
  });

  @override
  State<CameraPreviewDisplayer> createState() => _CameraPreviewDisplayerState();
}

class _CameraPreviewDisplayerState extends State<CameraPreviewDisplayer> {
  // 1.
  Uint8List? _cachedJpeg;

  @override
  Widget build(BuildContext context) {
    return Center(
      // 2.
      child: StreamBuilder<AnalysisImage>(
        stream: widget.analysisImageStream,
        builder: (_, snapshot) {
          if (!snapshot.hasData) {
            return const SizedBox.shrink();
          }

          final img = snapshot.requireData;
          // 3.
          return img.when(jpeg: (image) {
            // 4.
            _cachedJpeg = _applyFilterOnBytes(image.bytes);

            return ImageAnalysisPreview(
              currentJpeg: _cachedJpeg!,
              width: image.width.toDouble(),
              height: image.height.toDouble(),
            );
          }, yuv420: (Yuv420Image image) {
            // 5.
            return FutureBuilder<JpegImage>(
                future: image.toJpeg(),
                builder: (_, snapshot) {
                  if (snapshot.data == null && _cachedJpeg == null) {
                    return const Center(
                      child: CircularProgressIndicator(),
                    );
                  } else if (snapshot.data != null) {
                    // 6.
                    _cachedJpeg = _applyFilterOnBytes(
                      snapshot.data!.bytes,
                    );
                  }
                  return ImageAnalysisPreview(
                    currentJpeg: _cachedJpeg!,
                    width: image.width.toDouble(),
                    height: image.height.toDouble(),
                  );
                });
          }, nv21: (Nv21Image image) {
            // 7.
            return FutureBuilder<JpegImage>(
                future: image.toJpeg(),
                builder: (_, snapshot) {
                  if (snapshot.data == null && _cachedJpeg == null) {
                    return const Center(
                      child: CircularProgressIndicator(),
                    );
                  } else if (snapshot.data != null) {
                    _cachedJpeg = _applyFilterOnBytes(
                      snapshot.data!.bytes,
                    );
                  }
                  return ImageAnalysisPreview(
                    currentJpeg: _cachedJpeg!,
                    width: image.width.toDouble(),
                    height: image.height.toDouble(),
                  );
                });
          }, bgra8888: (Bgra8888Image image) {
            // 8.
            _cachedJpeg = _applyFilterOnImage(
              imglib.Image.fromBytes(
                width: image.width,
                height: image.height,
                bytes: image.planes[0].bytes.buffer,
                order: imglib.ChannelOrder.bgra,
              ),
            );

            return ImageAnalysisPreview(
              currentJpeg: _cachedJpeg!,
              width: image.width.toDouble(),
              height: image.height.toDouble(),
            );
            // We handle all formats so we're sure there won't be a null value
          })!;
        },
      ),
    );
  }

  Uint8List _applyFilterOnBytes(Uint8List bytes) {
    return _applyFilterOnImage(imglib.decodeJpg(bytes)!);
  }

  Uint8List _applyFilterOnImage(imglib.Image image) {
    // 9.
    return imglib.encodeJpg(
      imglib.billboard(image),
      quality: 70,
    );
  }
}
```

There was a bit more code here! Let's explain it:

1. The last image treated is saved in `_cachedJpeg`. It works as a kind of cache: if one conversion is still pending, we show the last one instead.
2. Since we have a stream of images to display, we use a `StreamBuilder`.
3. An `AnalysisImage` can have multiple formats. The `when` method let use handle all of them.
4. If the image is already in `JPEG`, we can apply the filter directly on the bytes.
5. If the image is in `yuv420` format, we need to convert it to `jpeg` first. We use the `toJpeg` method to do so. This method is asynchronous so we need to use a `FutureBuilder` to wait for the result.
6. Once the `AsyncSnapshot` has data, apply the filter on it and save the result in `_cachedJpeg`.
7. The `Nv21Image` handler works exactly the same as `Yuv420Image`.
8. If the image is in `bgra8888` format, we can convert it easily in dart and apply the filter on the result.
9. Use the `image` librare (aliased as "imglib" here) to apply a billboard effect on the image.

Finally, let's create the `ImageAnalysisPreview` widget:

```dart
class ImageAnalysisPreview extends StatelessWidget {
  final double width;
  final double height;
  final Uint8List currentJpeg;

  const ImageAnalysisPreview({
    super.key,
    required this.currentJpeg,
    required this.width,
    required this.height,
  });

  @override
  Widget build(BuildContext context) {
    return Container(
      color: Colors.black,
      child: Transform.scale(
        // 1.
        scaleX: -1,
        child: Transform.rotate(
          // 2.
          angle: 3 / 2 * pi,
          child: SizedBox.expand(
            child: Image.memory(
              currentJpeg,
              // 3.
              gaplessPlayback: true,
              fit: BoxFit.cover,
            ),
          ),
        ),
      ),
    );
  }
}
```

Let's break it down:

1. On Android, image analysis for the front camera is not flipped (like in the preview). Flip it back with `Transform.scale`.
2. Rotate the image to have the same orientation as the camera preview.
3. `gaplessPlayback` avoids flickering when the image is updated.

Full source code is available in `example/analysis_image_filter.dart`.
A more elaborate example where you can choose which filter to apply is also available in `example/analysis_image_filter_picker.dart`



================================================
FILE: docs/image_analysis/reading_barcodes.mdx
================================================
## Reading barcodes

![Read barcodes](/img/barcode_ai.gif)

First thing you need to do is adding MLKit's barcode scanner to your pubspec.yaml:

```yaml
google_mlkit_barcode_scanning: ^0.12.0
```

You will need a `BarcodeScanner`. You can get one with:

```dart
final _barcodeScanner = BarcodeScanner(formats: [BarcodeFormat.all]);
```

You might not want to read all `BarcodeFormat`, so feel free to be more selective.

Now you can set up CamerAwesome.

### Setting up CamerAwesome

This is the builder of the `ai_analysis_barcode.dart` example:

```dart
// 1.
CameraAwesomeBuilder.previewOnly(
  // 2.
  imageAnalysisConfig: AnalysisConfig(
    androidOptions: const AndroidAnalysisOptions.nv21(
      width: 1024,
    ),
    maxFramesPerSecond: 5,
    autoStart: false,
  ),
  // 3.
  onImageForAnalysis: (img) => _processImageBarcode(img),
  // 4.
  builder: (cameraModeState, preview) {
    return _BarcodeDisplayWidget(
      barcodesStream: _barcodesStream,
      scrollController: _scrollController,
      // 5.
      analysisController: cameraModeState.analysisController!,
    );
  },
)
```

Let's go through this code snippet:
1. We use the `previewOnly()` constructor since we don't need to take pictures or videos. We only need to analyze images.
2. The `AnalysisConfig` for this example includes a `maxFramesPerSecond` to a low value since we only need to read barcodes. It doesn't make sense to read 10s of barcodes per second. It reduces the number of calculations greatly.
3. We set the `onImageForAnalysis` callback to our `_processImageBarcode` method.
4. We build our interface with the `builder`. Here, we simply made a widget that displays all the barcodes scanned in a `ListView`.
5. The `analysisController` will be used to pause and resume image analysis.

## Extract barcodes from AnalysisImage

Once you get an `AnalysisImage`, you need to convert it to an object that MLKit can handle then you can do your processing.

```dart
Future _processImageBarcode(AnalysisImage img) async {
  // 1.
  final inputImage = img.toInputImage();

  try {
    // 2.
    var recognizedBarCodes = await _barcodeScanner.processImage(inputImage);
    // 3.   
    for (Barcode barcode in recognizedBarCodes) {
      debugPrint("Barcode: [${barcode.format}]: ${barcode.rawValue}");
      // 4.
      _addBarcode("[${barcode.format.name}]: ${barcode.rawValue}");
    }
  } catch (error) {
    debugPrint("...sending image resulted error $error");
  }
}
```
Here is an explanation of the above code:
1. Convert the `AnalysisImage` to an `InputImage` with `toInputImage()`,a function from an extension that we'll detail later.
2. Process the image with the `BarcodeScanner` and get the barcodes.
3. Loop through the barcodes.
4. Add the barcode to the list of barcodes.

The extension function `toInputImage()` is the following:

```dart
import 'package:camerawesome/camerawesome_plugin.dart';
import 'package:google_mlkit_face_detection/google_mlkit_face_detection.dart';

extension MLKitUtils on AnalysisImage {
  InputImage toInputImage() {
    // 1.
    final planeData =
        when(nv21: (img) => img.planes, bgra8888: (img) => img.planes)?.map(
      (plane) {
        return InputImagePlaneMetadata(
          bytesPerRow: plane.bytesPerRow,
          height: height,
          width: width,
        );
      },
    ).toList();

    // 2.
    return when(nv21: (image) {
      // 3.
      return InputImage.fromBytes(
        bytes: image.bytes,
        metadata: InputImageMetadata(
            rotation: inputImageRotation,
            format: InputImageFormat.nv21,
            size: image.size,
            bytesPerRow: image.planes.first.bytesPerRow,
        ),
      );
    }, bgra8888: (image) {
      // 4. 
      final inputImageData = InputImageData(
        size: size,
        imageRotation: inputImageRotation,
        inputImageFormat: inputImageFormat,
        planeData: planeData,
      );

      // 5.
      return InputImage.fromBytes(
        bytes: image.bytes,
        inputImageData: inputImageData,
      );
    })!;
  }

  InputImageRotation get inputImageRotation =>
      InputImageRotation.values.byName(rotation.name);

  InputImageFormat get inputImageFormat {
    switch (format) {
      case InputAnalysisImageFormat.bgra8888:
        return InputImageFormat.bgra8888;
      case InputAnalysisImageFormat.nv21:
        return InputImageFormat.nv21;
      default:
        return InputImageFormat.yuv420;
    }
  }
}
```

Let's break down this extension function:
1. Get the planes from the `AnalysisImage` and map them to `InputImagePlaneMetadata`.
2. Use the `when()` function to handle both Android and iOS cases.
3. Convert the `Nv21Image` to MLKit's `InputImage`
4. Define the `InputImageData` for the `InputImage` on iOS.
5. Convert the `Bgra8888Image` to MLKit's `InputImage` thanks to `InputImageData`.

There are also helper function to map enums between CamerAwesome and MLKit.

Detecting barcodes is quite straightforward with CamerAwesome and MLKit.
You just need to convert `AnalysisImage` to an `InputImage` as it is done in this example and MLKit will handle the rest. 👌

## Pause and Resume Image Analysis

You may want to pause and resume image analysis.
To do that, CamerAwesome provides an `AnalysisController` through `CameraState`:

```dart
// 1.
CameraAwesomeBuilder.custom(
    builder: (cameraState, preview) {
        // 2.
        print("Image analysis enabled: ${cameraState.analysisController?.enabled}");

        return SomeWidget(
            analysisController: cameraState.analysisController,
        );
    },
)
```

The above code shows a simple way to access the `AnalysisController`:

1. We used the `custom()` constructor here but you can also get one from the `awesome()` constructor. Note that it may be null if no image analysis callback has been set.
2. Here, we only print if the image analysis is enabled or not thanks to `analysisController`.

Then, you could have a toggle widget like this:

```dart
CheckboxListTile(
    value: widget.analysisController.enabled,
    onChanged: (_) async {
        // 1.
        if (widget.analysisController?.enabled == true) {
          // 2.
          await widget.analysisController.stop();
        } else {
          // 3.
          await widget.analysisController.start();
        }
        setState(() {});
    },
    title: Text("Enable barcode scan"),
)
```

As a toggle, it swap between image analysis enabled and disabled:

1. First, check if it was enabled or disabled
2. If it was enabled, disable image analysis
3. Otherwise, enable it

See the [complete example](https://github.com/Apparence-io/camera_awesome/blob/master/example/lib/ai_analysis_barcode.dart) if you need more details.

## Scan area

While scanning barcodes, you may want to limit the area where to look for barcodes.
This is possible with CamerAwesome.
Let's see how.

### Showing the scan area

First of all, you must show the area in which you want to scan barcodes.
You can use `CameraAwesomeBuilder.awesome()` to do that thanks to the `previewDecoratorBuilder` parameter:

```dart
CameraAwesomeBuilder.awesome(
    previewDecoratorBuilder: (state, preview) {
        return BarcodePreviewOverlay(
          state: state,
          previewSize: preview.size,
          previewRect: preview.rect,
          barcodes: _barcodes,
          analysisImage: _image,
        );
    },
    // Other parameters
)
```

> Other builders of `CameraAwesomeBuilder` provide a similar feature with their `builder` parameter.

The above code is taken from `preview_overlay_example.dart`.

The `previewDecoratorBuilder` gives you three important parameters:

- the `state` of the camera
- the `previewSize` of the camera, before any clipping due to the current aspectRatio
- the `previewRect` of the camera, after any clipping due to the aspectRatio

> The native preview may not respect the selected aspect ratio, so we clip it to correct this behaviour.
> That's why there are two distinct parameters.

Then, pass these parameters to your decorator widget along with the `AnalysisImage` and the barcodes detected.

The `BarcodePreviewOverlay` results in the following:
![Barcode scan area](/img/barcode_overlay.gif)

As you can see, there are several elements.
Some of them are widgets, others are drawn on the canvas thanks to a `CustomPainter`.

The `CustomPainter` draws the following:

- a semi-transparent black overlay with a cut in the center: this is the scan area
- there's also a red line and a border around the scan area
- a blue rectangle around the detected barcode

In the other hand, all the text is written with simple `Text` widgets.

The `build()` method of the `BarcodePreviewOverlay` mainly consists of the following:

```dart
// 1.
IgnorePointer(
  ignoring: true,
  child: Padding(
    // 2.
    padding: EdgeInsets.only(
      top: widget.previewRect.top,
      left: widget.previewRect.left,
      right: _screenSize.width - widget.previewRect.right,
      bottom: _screenSize.height - widget.previewRect.bottom,
    ),
    child: Stack(children: [
      // 3.
      Positioned.fill(
        child: CustomPaint(
          painter: BarcodeFocusAreaPainter(
            scanArea: _scanArea.size,
            barcodeRect: _barcodeRect,
            canvasScale: _canvasScale,
            canvasTranslate: _canvasTranslate,
          ),
        ),
      ),
      // 4.
      Positioned(
        // 5.
        top: widget.previewSize.height / 2 + _scanArea.size.height / 2 + 10,
        left: 0,
        right: 0,
        child: Column(children: [
          Text(
            _barcodeRead ?? "",
            textAlign: TextAlign.center,
            style: const TextStyle(
              color: Colors.white,
              fontSize: 20,
            ),
          ),
          if (_barcodeInArea != null)
            Container(
              margin: const EdgeInsets.only(top: 8),
              color: _barcodeInArea! ? Colors.green : Colors.red,
              child: Text(
                _barcodeInArea! ? "Barcode in area" : "Barcode not in area",
                textAlign: TextAlign.center,
                style: const TextStyle(
                  color: Colors.white,
                  fontSize: 20,
                ),
              ),
            ),
        ]),
      ),
    ]),
  ),
)
```

Let's break down the above code:

1. We use an `IgnorePointer` to ignore all the touch events. Otherwise, our widget would block the touch events on the camera preview, including the tap to focus feature.
2. Thanks to the previewRect, we know how our preview is positioned on the screen. By adding padding corresponding to the `previewRect` boundaries, we ensure that the `child` is exactly the same size as the preview.
3. We draw the scan area on the canvas thanks to a `CustomPaint` widget.
4. We add some text widgets to show the barcode read and if it is in the scan area.
5. Since the scan area is in the middle of the screen, it's easy to position elements below it.
   `widget.previewSize.height / 2` is the middle of the screen, `_scanArea.size.height / 2` is half the height of the scan area, and the last `+ 10` is just a margin.

The scan area is calculated based on the `previewRect` for this example:

```dart
_scanArea = Rect.fromCenter(
  center: widget.previewRect.center,
  width: widget.previewRect.width * 0.7,
  height: widget.previewRect.height * 0.3,
);
```

This implies that the scan area will change slightly if the aspect ratio changes (`previewRect` changes with the aspect ratio).

We'll not dive into the code of the `CustomPainter` here, but you can check the code yourself: it's quite simple.

In fact, the most complicated part comes from the parameters given to `BarcodeFocusAreaPainter`.

### How to know if a barcode is within the scan area?

Whenever we get a new list of barcodes detected by MLKit, we iterate over them to know if one of them is within the scan area.
This happens in the `didUpdateWidget()` method:

```dart
@override
void didUpdateWidget(covariant BarcodePreviewOverlay oldWidget) {
    if (widget.barcodes != oldWidget.barcodes ||
        widget.analysisImage != oldWidget.analysisImage &&
            widget.analysisImage != null) {
      _refreshScanArea();
      _detectBarcodeInArea(widget.analysisImage!, widget.barcodes);
    }
    super.didUpdateWidget(oldWidget);
}
```

`_detectBarcodeInArea()` holds the logic to know if a barcode is within the scan area.

Let's see the code:

```dart
 Future _detectBarcodeInArea(AnalysisImage img, List<Barcode> barcodes) async {
    try {
      String? barcodeRead;
      _barcodeInArea = null;

      // The canvas transformation is needed to display the barcode rect correctly on android
      canvasTransformation = img.getCanvasTransformation(widget.preview);

      for (Barcode barcode in barcodes) {
        if (barcode.cornerPoints.isEmpty) {
          continue;
        }

        barcodeRead = "[${barcode.format.name}]: ${barcode.rawValue}";
        // For simplicity we consider the barcode to be a Rect. Due to
        // perspective, it might not be in reality. You could build a Path
        // from the 4 corner points instead.
        final topLeftOffset = barcode.cornerPoints[0];
        final bottomRightOffset = barcode.cornerPoints[2];
        var topLeftOff = widget.preview.convertFromImage(
          topLeftOffset.toOffset(),
          img,
        );
        var bottomRightOff = widget.preview.convertFromImage(
          bottomRightOffset.toOffset(),
          img,
        );

        _barcodeRect = Rect.fromLTRB(
          topLeftOff.dx,
          topLeftOff.dy,
          bottomRightOff.dx,
          bottomRightOff.dy,
        );

        // Approximately detect if the barcode is in the scan area by checking
        // if the center of the barcode is in the scan area.
        if (_scanArea.contains(
          _barcodeRect!.center.translate(
            (_screenSize.width - widget.preview.previewSize.width) / 2,
            (_screenSize.height - widget.preview.previewSize.height) / 2,
          ),
        )) {
          // Note: for a better detection, you should calculate the area of the
          // intersection between the barcode and the scan area and compare it
          // with the area of the barcode. If the intersection is greater than
          // a certain percentage, then the barcode is in the scan area.
          _barcodeInArea = true;
          // Only handle one good barcode in this example
          break;
        } else {
          _barcodeInArea = false;
        }

        if (_barcodeInArea != null && mounted) {
          setState(() {
            _barcodeRead = barcodeRead;
          });
        }
      }
    } catch (error, stacktrace) {
      debugPrint("...sending image resulted error $error $stacktrace");
    }
  }
```

1. Depending on which Sensor we use and the image rotation of the screen, the canvas needs extra transformations to match the coordinates of the different elements we want to draw.
2. Iterate over the barcodes detected by MLKit and check if one of them is within the scan area.
3. We calculate the top left and bottom right corners of the barcode. More on this below.
4. We create a `Rect` from these two points.
5. We check if the center of the barcode is within the scan area.
6. If a barcode is within the scan area, we update the state to show it.

To help you converting an analysis image to the preview coordinates we provides functions. 

```dart
/// this method is used to convert a point from an image to the preview
/// according to the current preview size and the image size
/// also in case of Android, it will flip the point if required
Offset convertFromImage(
    Offset point,
    AnalysisImage img,
);
```

The AnalysisImage also provide you the required transformation 
(It seems that the 180 / 270 cases are not working on all phones right now)
```dart
abstract class AnalysisImage {
    ...
  // Symmetry for Android since native image analysis is not mirrored but preview is
  // It also handles device rotation
  CanvasTransformation? getCanvasTransformation(
    Preview preview,
  );
```

With those two functions you can just draw your points without doing any math. 


### From MLKit coordinates to screen coordinates

MLKit takes an `AnalysisImage` as input, and returns a list of `Barcode` objects.
On Android, you can set a different resolution for your image analysis than the preview, which means that they might not have the same aspect ratio!

Here is an example:

![Coordinates conversion](/img/coordinates_conversion.png)

In the picture above, there are 3 parts:

1. The image analysis, in 4:3 for this example
2. The preview you can see, in 16:9 here
3. The screen, which has a wider aspect ratio than the preview

In fact, the image analysis seems to have the same width as the preview, but it's because we enlarged it. In reality, on Android your image analysis might be at 352x288 for example to get good performances.

In our example, the point A is the coordinate of one of the corners from a barcode that MLKit detected.
That's the coordinate we need to translate to screen coordinates.

However, a camera might not see the same things in a 4:3 image and a 16:9 image.
On Android, the `AnalysisImage` contains a `cropRect` property that correspond to the area both image have in common.

![Crop rect](/img/crop_rect_example.png)

The image analysis now goes outside the screen because it has been zoomed to match what is seen on the preview image.

The purple part is what corresponds to the `cropRect` area of the image analysis.

The method `_croppedPosition` transforms the coordinates of the image analysis (in red) to the coordinates of the preview image (in blue).

```dart
Offset _croppedPosition(
  Point<int> element, {
  required Size analysisImageSize,
  required Size croppedSize,
  required Size screenSize,
  // 1.
  required double ratio,
  // 2.
  required bool flipXY,
}) {
  // 3.
  num imageDiffX;
  num imageDiffY;
  if (Platform.isIOS) {
    imageDiffX = analysisImageSize.width - croppedSize.width;
    imageDiffY = analysisImageSize.height - croppedSize.height;
  } else {
    // 4.
    imageDiffX = analysisImageSize.height - croppedSize.width;
    imageDiffY = analysisImageSize.width - croppedSize.height;
  }

  // 5. Apply the imageDiff to the element position
  return (Offset(
            (flipXY ? element.y : element.x).toDouble() - (imageDiffX / 2),
            (flipXY ? element.x : element.y).toDouble() - (imageDiffY / 2),
          ) *
          ratio)
      .translate(
    // 6. If screenSize is bigger than croppedSize, move the element to half the difference
    (screenSize.width - (croppedSize.width * ratio)) / 2,
    (screenSize.height - (croppedSize.height * ratio)) / 2,
  );
}
```

Let's break it down a bit:

1. The ratio parameter is the one between croppedSize and previewSize
2. flipXY is a property that depends on the current Sensor and image rotation
3. Determine how much the image is cropped
4. Width and height are inverted on Android
5. Apply the imageDiff to the element position
6. If screenSize is bigger than croppedSize, translate the position to half their difference

Additionaly, we translate the result of `_croppedPosition` because we used a `Padding` to place elements on top of the preview:

```dart
_croppedPosition(
  ...
).translate(-widget.previewRect.left, -widget.previewRect.top)
```
The translate value matches the left and top padding we've set.

You can find the complete example of this demo in `preview_overlay_example.dart`.



================================================
FILE: docs/img/awesome_ui_parts.webp
================================================
[Non-text file]


================================================
FILE: docs/migration_guides/from_1_to_2.mdx
================================================
# Migrating from 1.x.x to 2.x.x

CamerAwesome 2.0.0 is a major release that brings a lot of new features and improvements.

The most important change is that you can use several sensors concurrently which implied several API changes to CamerAwesome.

This guide will help you to migrate your code from 1.x.x to 2.x.x.

## Breaking changes

The initial settings of the `CameraAwesomeBuilder` have been moved to either `SaveConfig` or `SensorConfig`.

See the code diff below:

```diff
CameraAwesomeBuilder.awesome(
-   sensor: Sensors.back,
-   flashMode: FlashMode.auto,
-   aspectRatio: CameraAspectRatios.ratio_4_3,
-   mirrorFrontCamera: true,
-   zoom: 0.0,
+   sensorConfig: SensorConfig.single(
+       sensor: Sensor.position(SensorPosition.back),
+       flashMode: FlashMode.auto,
+       aspectRatio: CameraAspectRatios.ratio_4_3,
+       zoom: 0.0,
+   ),
-   exifPreferences: ExifPreferences(saveGPSLocation: true),
-   enableAudio: true,
    saveConfig: SaveConfig.photoAndVideo(
        initialCaptureMode: CaptureMode.photo,
+       photoPathBuilder: (sensors) async {
+         final Directory extDir = await getTemporaryDirectory();
+         final testDir = await Directory(
+           '${extDir.path}/camerawesome',
+         ).create(recursive: true);
+         if (sensors.length == 1) {
+           final String filePath =
+               '${testDir.path}/${DateTime.now().millisecondsSinceEpoch}.jpg';
+           return SingleCaptureRequest(filePath, sensors.first);
+         } else {
+           // Separate pictures taken with front and back camera
+           return MultipleCaptureRequest(
+             {
+               for (final sensor in sensors)
+                 sensor:
+                     '${testDir.path}/${sensor.position == SensorPosition.front ? 'front_' : "back_"}${DateTime.now().millisecondsSinceEpoch}.jpg',
+             },
+           );
+         }
+       },
+       videoPathBuilder: (sensors) async {
+           // same logic as photoPathBuilder
+       },
+       videoOptions: VideoOptions(
+         enableAudio: true,
+         ios: CupertinoVideoOptions(
+           fps: 10,
+         ),
+         android: AndroidVideoOptions(
+           bitrate: 6000000,
+           quality: VideoRecordingQuality.fhd,
+           fallbackStrategy: QualityFallbackStrategy.lower,
+         ),
+       ),
+       exifPreferences: ExifPreferences(saveGPSLocation: true),
+       mirrorFrontCamera: true,
     ),
    ...
)
```


## Changelog

- ✨ Added multi-camera feature, allowing users to display multiple camera previews simultaneously. Note that this feature is currently in beta, and we do not recommend using it in production.
- ✨ Users can now pass options (such as bitrate, fps, and quality) when recording a video.
- ✨🍏 Implemented brightness and exposure level settings on iOS / iPadOS.
- ✨🤖 Added zoom indicator UI.
- ✨🤖 Video recording is now mirrored if `mirrorFrontCamera` is set to true.
- ♻️🍏 Completely reworked the code for increased clarity and performance.
- 🐛 Fixed patrol tests.
- 🐛 Fixed the use of capture button parameter in awesome bottom actions (thanks to @juliuszmandrosz).
- 📝 Added Chinese README.md (thanks to @chyiiiiiiiiiiii).



================================================
FILE: docs/widgets/awesome_buttons.mdx
================================================
## Built-in Buttons

You may want different icons, additional features or just arrange the screen differently.
In this case, you will use `CameraAwesomeBuilder.custom()` instead of `CameraAwesomeBuiilder.awesome()`.

Yet, if some of our buttons suit your needs, feel free to use them!

### AwesomeMediaPreview

Shows a clickable preview of a media captured using CamerAwesome.

_This widget rotates when the camera rotates._

Below example shows how to show the last captured media using `AwesomeMediaPreview`.

```dart
StreamBuilder<MediaCapture?>(
  stream: state.captureState$,
  builder: (context, snapshot) {
    if (!snapshot.hasData) {
      return Container(width: 72, height: 72);
    }
    return SizedBox(
      width: 72,
      child: AwesomeMediaPreview(
        mediaCapture: snapshot.requireData,
        onMediaTap: onMediaTap,
      ),
    );
  },
)
```

Note: video preview is not implemented in this widget since it would imply a dependency to other packages.

You can find an alternative widget that auto plays the last video recorded in the `example` project at `widgets/custom_media_preview.dart`.

It uses the `video_player` package under the hood.

See its usage in `custom_ui_example_3.dart`.

### AwesomeBouncingWidget

This widget is used to make its child bounce when pressed.

```dart
AwesomeBouncingWidget(
  // On tap callback. Set it to null to disable the button
  onTap: () {
    // Do something
  },
  // Opacity of the widget when disabled
  disabledOpacity: 0.3,
  // Duration of the bounce animation
  duration: const Duration(milliseconds: 100),
  // Wether or not the widget should vibrate when pressed
  vibrationEnabled: true,
  // Your child widget
  child: MyChild(),
)
```

### AwesomeFlashButton

![Switch between flash modes](/img/flash.gif)

Choose the Flash mode of the camera.

Supported flash modes are:

- FlashMode.none
- FlashMode.on
- FlashMode.auto
- FlashMode.always

_This button rotates when the camera rotates._

**Full example:**

```dart
AwesomeFlashButton(
  // Current CameraState
  state: state,
  // You can provide your own icon builder with a custom icon for each flash mode for example.
  iconBuilder: (flashMode) {
    switch (flashMode) {
      case FlashMode.none:
        return const Icon(Icons.flash_off);
      case FlashMode.on:
        return const Icon(Icons.flash_on);
      case FlashMode.auto:
        return const Icon(Icons.flash_auto);
      case FlashMode.always:
        return const Icon(Icons.flashlight_on);
    }
  },
  // You can provide a custom theme to the button. If you don't, it will use the theme from CameraAwesomeBuilder
  theme: AwesomeTheme(
    buttonTheme: AwesomeButtonTheme(
      iconSize: 28,
      padding: const EdgeInsets.all(8),
      foregroundColor: Colors.black,
      backgroundColor: Colors.white,
    ),
  ),
  onFlashTap: (sensorConfig, flashMode) {
    // You may want to update your custom UI or save the flash mode in the settings of your app for example
    doSomethingCustom();

    // Finally, update the flash mode of the sensor config
    sensorConfig.setFlashMode(flashMode);
  },
)
```

Note: `state` is the only required parameter. Other ones have default values.

### AwesomeCameraSwitchButton

Switch between Front and Back camera.

_This button rotates when the camera rotates._

```dart
AwesomeCameraSwitchButton(
  // Current CameraState
  state: state,
  // You can set a scale value for this button to make it look smaller or bigger than the other ones
  scale: 1.5,
  // You can provide a custom theme to the button. If you don't, it will use the theme from CameraAwesomeBuilder
  theme: AwesomeTheme(
    buttonTheme: AwesomeButtonTheme(
      iconSize: 28,
      padding: const EdgeInsets.all(8),
      foregroundColor: Colors.black,
      backgroundColor: Colors.white,
    ),
  ),
  // Change the switch camera behaviour logic
  onSwitchTap: (state) {
    // Aspect ratio is reset by default when switching cameras (it goes back to 4:3).
    // You can change this behaviour by overriding the aspect ratio in the switchCameraSensor method.
    state.switchCameraSensor(
      aspectRatio: state.sensorConfig.aspectRatio,
    );
  },
  // Set the icon you want to display for the switch camera button
  iconBuilder: () {
    return MyCustomIcon();
  },
)
```

Note: `state` is the only required parameter. Other ones have default values.

⚠️ By default, the SensorConfig goes back to a `SensorConfig` with the default values of its constructor:

```dart
 SensorConfig({
  required this.sensor,
  FlashMode flash = FlashMode.none,
  SensorType type = SensorType.wideAngle,
  this.captureDeviceId,
  CameraAspectRatios aspectRatio = CameraAspectRatios.ratio_4_3,
  double currentZoom = 0.0,
})
```

This behaviour may be improved in a future update.

### AwesomeCaptureButton

![Capture button](/img/capture_button.gif)

Take a photo when in Photo mode or start/stop recording when in Video mode.

```dart
AwesomeCaptureButton(state: state)
```

### AwesomeAspectRatioButton

Change the aspect ratio of the camera using this widget.

Supported ratios are:

- 4:3
- 16:9
- 1:1

_This button rotates when the camera rotates._

```dart
AwesomeAspectRatioButton(
  state: state,
  // Custom icon builder
  iconBuilder: (aspectRatio) {
    switch (aspectRatio) {
      case CameraAspectRatios.ratio_16_9:
        return const Icon(Icons.crop_16_9);
      case CameraAspectRatios.ratio_4_3:
        return const Icon(Icons.crop_7_5);
      case CameraAspectRatios.ratio_1_1:
        return const Icon(Icons.crop_square);
      default:
        return const SizedBox();
    }
  },
  // You can provide a custom theme to the button. If you don't, it will use the theme from CameraAwesomeBuilder
  theme: AwesomeTheme(
    buttonTheme: AwesomeButtonTheme(
      iconSize: 28,
      padding: const EdgeInsets.all(8),
      foregroundColor: Colors.black,
      backgroundColor: Colors.white,
    ),
  ),
  onAspectRatioTap: (sensorConfig, aspectRatio) {
    // Do something custom
    doSomething();

    // Set the aspect ratio
    sensorConfig.setAspectRatio(aspectRatio);
  },
)
```

Note: `state` is the only required parameter. Other ones have default values.

### AwesomeLocationButton

When taking a photo, enable or disable the save of the current location in the exif metadata.

_This button rotates when the camera rotates._

```dart
AwesomeLocationButton(
  // Current CameraState
  state: state,
  // Change the icon based on saveGpsLocation
  iconBuilder: (saveGpsLocation) {
    if (saveGpsLocation) {
      return const Icon(Icons.location_pin);
    } else {
      return const Icon(Icons.location_off_outlined);
    }
  },
  // You can provide a custom theme to the button. If you don't, it will use the theme from CameraAwesomeBuilder
  theme: AwesomeTheme(
    buttonTheme: AwesomeButtonTheme(
      iconSize: 28,
      padding: const EdgeInsets.all(8),
      foregroundColor: Colors.black,
      backgroundColor: Colors.white,
    ),
  ),
  // Change the location button behaviour logic
  onLocationTap: (state, saveGpsLocation) {
    // Your custom logic
    doSomethingCustom();

    // Default logic
    state.shouldSaveGpsLocation(saveGpsLocation);
  },
)
```

### AwesomePauseResumeButton

Pause or resume a video recording with this button.
This widget is designed to work with a `VideoRecordingCameraState`.

_This button rotates when the camera rotates._

```dart
AwesomePauseResumeButton(
  state: videoRecordingCameraState,
  // You can provide a custom theme to the button. If you don't, it will use the theme from CameraAwesomeBuilder
  theme: AwesomeTheme(
    buttonTheme: AwesomeButtonTheme(
      iconSize: 28,
      padding: const EdgeInsets.all(8),
      foregroundColor: Colors.black,
      backgroundColor: Colors.white,
    ),
  ),
)
```

)

````

### AwesomeSensorTypeSelector (iOS only)

![Swap sensor type](/img/sensors_type.gif)

Change between different sensor types. The widget will adapt itself to the available sensors.

Here is an example of all existing sensors on an iPhone 14 Pro:

- Wide angle.
- Ultra wide angle.
- Telephoto.
- TrueDepth (front camera).

You can call `CamerawesomePlugin.getSensors()` to get the list of available sensors.

_Elements of this widget rotate when the camera rotates._

```dart
AwesomeSensorTypeSelector(state: state)
````



================================================
FILE: docs/widgets/awesome_camera_mode_selector.mdx
================================================
## AwesomeCameraModeSelector

![Swap between photo and video](/img/swap_photo_video.gif)

Switch between PHOTO and VIDEO mode easily with this widget.

If you are using `CameraAwesomeBuilder.awesome()` builder with no customization on the UI, you are already using it behind the scene.

However, if you made your own custom UI using `CameraAwesomeBuilder.custom()` or you've set up a different `middleContentBuilder` in `CameraAwesomeBuiilder.awesome()`, you may want to use it.

Here is an example:

``` dart
CameraAwesomeBuilder.custom(
  saveConfig: ...,
  builder: (state, previewSize, previewRect) {
    return Stack(
      fit: StackFit.expand,
      children: <Widget>[
        YourCameraPreview(),
        Positioned.fill(
          child: SafeArea(
            child: Column(children: [
              // Rest of your UI, e.g.: Flash button
              AwesomeCameraModeSelector(state: state),
              // Rest of your UI, e.g.: Button to take photos
            ]),
          ),
        ),
      ],
    );
  },
  // other parameters
);
```




================================================
FILE: docs/widgets/awesome_filters.mdx
================================================
## Filters

CamerAwesome embeds a set of **filters** that can be applied to the camera preview.

There is actually **29** filters which can be applied on your pictures to make them look awesome 🎉.

We are working hard to add more filters to the plugin but you can also [contribute](https://github.com/Apparence-io/CamerAwesome/pulls) by adding your own filters 😎.

![Filters preview](/img/filters.gif)

You can add a filter directly in the camerAwesome builder:

```dart
CameraAwesomeBuilder.awesome(
  // [...]
  filter: AwesomeFilter.AddictiveRed,
),
```

Here is a list of all filters currently available:

|                                                                                                                                                    |                                                                                                                                                                |                                                                                                                                                             |                                                                                                                                                 |
| :------------------------------------------------------------------------------------------------------------------------------------------------: | :------------------------------------------------------------------------------------------------------------------------------------------------------------: | :---------------------------------------------------------------------------------------------------------------------------------------------------------: | :---------------------------------------------------------------------------------------------------------------------------------------------: |
| <img width="1604" alt="No Filter" src="https://raw.githubusercontent.com/skkallayath/photofilters/master/exampleimages/No Filter.jpg" /> No Filter | <img width="1604" alt="AddictiveBlue" src="https://raw.githubusercontent.com/skkallayath/photofilters/master/exampleimages/AddictiveBlue.jpg" /> AddictiveBlue | <img width="1604" alt="AddictiveRed" src="https://raw.githubusercontent.com/skkallayath/photofilters/master/exampleimages/AddictiveRed.jpg" /> AddictiveRed |       <img width="1604" alt="Aden" src="https://raw.githubusercontent.com/skkallayath/photofilters/master/exampleimages/Aden.jpg" /> Aden       |
|       <img width="1604" alt="Amaro" src="https://raw.githubusercontent.com/skkallayath/photofilters/master/exampleimages/Amaro.jpg" /> Amaro       |             <img width="1604" alt="Ashby" src="https://raw.githubusercontent.com/skkallayath/photofilters/master/exampleimages/Ashby.jpg" /> Ashby             |        <img width="1604" alt="Brannan" src="https://raw.githubusercontent.com/skkallayath/photofilters/master/exampleimages/Brannan.jpg" /> Brannan         | <img width="1604" alt="Brooklyn" src="https://raw.githubusercontent.com/skkallayath/photofilters/master/exampleimages/Brooklyn.jpg" /> Brooklyn |
| <img width="1604" alt="Clarendon" src="https://raw.githubusercontent.com/skkallayath/photofilters/master/exampleimages/Clarendon.jpg" /> Clarendon |             <img width="1604" alt="Crema" src="https://raw.githubusercontent.com/skkallayath/photofilters/master/exampleimages/Crema.jpg" /> Crema             |       <img width="1604" alt="Dogpatch" src="https://raw.githubusercontent.com/skkallayath/photofilters/master/exampleimages/Dogpatch.jpg" /> Dogpatch       |     <img width="1604" alt="Sutro" src="https://raw.githubusercontent.com/skkallayath/photofilters/master/exampleimages/Sutro.jpg" /> Sutro      |
|    <img width="1604" alt="Gingham" src="https://raw.githubusercontent.com/skkallayath/photofilters/master/exampleimages/Gingham.jpg" /> Gingham    |             <img width="1604" alt="Ginza" src="https://raw.githubusercontent.com/skkallayath/photofilters/master/exampleimages/Ginza.jpg" /> Ginza             |       <img width="1604" alt="X-Pro II" src="https://raw.githubusercontent.com/skkallayath/photofilters/master/exampleimages/X-Pro II.jpg" /> X-Pro II       |    <img width="1604" alt="Willow" src="https://raw.githubusercontent.com/skkallayath/photofilters/master/exampleimages/Willow.jpg" /> Willow    |
|        <img width="1604" alt="Hefe" src="https://raw.githubusercontent.com/skkallayath/photofilters/master/exampleimages/Hefe.jpg" /> Hefe         |           <img width="1604" alt="Hudson" src="https://raw.githubusercontent.com/skkallayath/photofilters/master/exampleimages/Hudson.jpg" /> Hudson            |        <img width="1604" alt="Inkwell" src="https://raw.githubusercontent.com/skkallayath/photofilters/master/exampleimages/Inkwell.jpg" /> Inkwell         |    <img width="1604" alt="Walden" src="https://raw.githubusercontent.com/skkallayath/photofilters/master/exampleimages/Walden.jpg" /> Walden    |
|        <img width="1604" alt="Juno" src="https://raw.githubusercontent.com/skkallayath/photofilters/master/exampleimages/Juno.jpg" /> Juno         |              <img width="1604" alt="Lark" src="https://raw.githubusercontent.com/skkallayath/photofilters/master/exampleimages/Lark.jpg" /> Lark               |           <img width="1604" alt="Lo-Fi" src="https://raw.githubusercontent.com/skkallayath/photofilters/master/exampleimages/Lo-Fi.jpg" /> Lo-Fi            |  <img width="1604" alt="Stinson" src="https://raw.githubusercontent.com/skkallayath/photofilters/master/exampleimages/Stinson.jpg" /> Stinson   |
|     <img width="1604" alt="Ludwig" src="https://raw.githubusercontent.com/skkallayath/photofilters/master/exampleimages/Ludwig.jpg" /> Ludwig      |              <img width="1604" alt="Moon" src="https://raw.githubusercontent.com/skkallayath/photofilters/master/exampleimages/Moon.jpg" /> Moon               |        <img width="1604" alt="Slumber" src="https://raw.githubusercontent.com/skkallayath/photofilters/master/exampleimages/Slumber.jpg" /> Slumber         |    <img width="1604" alt="Sierra" src="https://raw.githubusercontent.com/skkallayath/photofilters/master/exampleimages/Sierra.jpg" /> Sierra    |
|  <img width="1604" alt="Perpetua" src="https://raw.githubusercontent.com/skkallayath/photofilters/master/exampleimages/Perpetua.jpg" /> Perpetua   |             <img width="1604" alt="Reyes" src="https://raw.githubusercontent.com/skkallayath/photofilters/master/exampleimages/Reyes.jpg" /> Reyes             |

_Preview picture are taken from the [photofilters](https://pub.dev/packages/photofilters) library, thanks to [@skkallayath](https://github.com/skkallayath) for the awesome work!_



================================================
FILE: docs/widgets/awesome_oriented_widget.mdx
================================================
## AwesomeOrientedWidget

Use this Widget to rotate automatically elements of your UI when the phone rotates.

![Rotate icons automatically when camera rotates](/img/auto_rotate.gif)

Note how icons rotate when the phone rotates.

### Usage

```dart
AwesomeOrientedWidget(
  child: YourWidget(),
  rotateWithDevice: true,
)
```



================================================
FILE: docs/widgets/layout.mdx
================================================
# Layout

CamerAwesome uses custom widgets to place elements in the built-in UI.

You can also reuse them if you'd like.

## Base layout

`AwesomeCameraLayout` separates the UI in 3 parts:
![Awesome UI parts](/img/awesome_ui_parts.webp)

In Awesome UI, these parts are meant to be:

1. Top actions: secondary actions like flash, aspect ratio, etc.
2. Middle content: additional elements like filters or text indications for example.
3. Bottom actions: main actions such as taking a picture or switching camera.

`CameraAwesomeBuilder` includes a builder for each part.

Here is the constructor of this widget:

```dart
AwesomeCameraLayout({
    super.key,
    required this.state,
    OnMediaTap? onMediaTap,
    Widget? middleContent,
    Widget? topActions,
    Widget? bottomActions,
})  : middleContent = middleContent ??
        (Column(
            children: [
            const Spacer(),
            if (state.captureMode == CaptureMode.photo)
                AwesomeFilterWidget(state: state),
            Builder(
                builder: (context) => Container(
                color: AwesomeThemeProvider.of(context)
                    .theme
                    .bottomActionsBackgroundColor,
                height: 8,
                ),
            ),
            AwesomeCameraModeSelector(state: state),
            ],
        )),
    topActions = topActions ?? AwesomeTopActions(state: state),
    bottomActions = bottomActions ??
        AwesomeBottomActions(state: state, onMediaTap: onMediaTap);
```

`topActions`, `middleContent` and `bottomActions` have default values if set to null.
The `onMediaTap` callback is used to handle the tap on the media preview in the default `bottomActions` widget.

The widget is then mostly a `Column` with `middleContent` being wrapped in an `Expanded`.

Feel free to use it in your custom UI.

## Top actions

`AwesomeTopActions` is a `Row` with an additional padding and default children.

Use it like this:

```dart
AwesomeTopActions(
    // CameraState is required
    state: state,
    // Add your own children
    children: [
        MyFirstWidget(state: state),
        MySecondWidget(state: state),
    ],
    // Override default padding
    padding: const EdgeInsets.all(8),
)
```

You may want to show different children based on the current state:

```dart
AwesomeTopActions(
  // CameraState is required
  state: state,
  // Add your own children
  children: state is PhotoCameraState
      ? [
          MyFirstWidget(state: state),
          MySecondWidget(state: state),
        ]
      : [
          MyThirdWidget(state: state),
        ],
  // Override default padding
  padding: const EdgeInsets.all(8),
)
```

## Bottom actions

Unlike `AwesomeTopActions`, `AwesomeBottomActions` is designed to include the main action of your Camera UI: taking a picture or recording a video.

Due to that, you can only set which widget you want besides that main action and customize the main action button:
```dart
AwesomeBottomActions(
  // CameraState is required
  state: state,
  // You can set your own capture button
  captureButton: MyCaptureButton(
    state: state,
  ),
  // Padding around the bottom actions
  padding: const EdgeInsets.only(
    bottom: 16,
    left: 8,
    right: 8,
  ),
  // Widget to the left of the captureButton
  left: AwesomeFlashButton(
    state: state,
  ),
  // Widget to the right of the captureButton
  right: AwesomeCameraSwitchButton(
    state: state,
    scale: 1.0,
    onSwitchTap: (state) {
      state.switchCameraSensor(
        aspectRatio: state.sensorConfig.aspectRatio,
      );
    },
  ),
  // Callback used by default values. Don't specify it if you override left and right widgets.
  onMediaTap: null,
);
```

`left`, `captureButton` and `right` widgets are then placed in a `Row`.

`left` and `right` are wrapped in an `Expanded` widget and centered within it.


================================================
FILE: docs/widgets/theming.mdx
================================================
# 🎨 Theming

You can customize the look and feel of CamerAwesome's built-in widgets by setting your own theme.

It can be used both in `CameraAwesomeBuilder.awesome()` and `CameraAwesomeBuilder.custom()` constructors.

Included buttons (Flash button, aspect ratio...) are black with a white icon and they bounce when you tap them.

You can completely change them if you set a new `AwesomeTheme`:

Example:

```dart
CameraAwesomeBuilder.awesome(
  theme: AwesomeTheme(
    // Background color of the bottom actions
    bottomActionsBackgroundColor: Colors.deepPurple.withValues(alpha: 0.5),
    // Buttons theme
    buttonTheme: AwesomeButtonTheme(
      // Background color of the button
      backgroundColor: Colors.deepPurple.withValues(alpha: 0.5),
      // Size of the icon
      iconSize: 32,
      // Padding around the icon
      padding: const EdgeInsets.all(18),
      // Color of the icon
      foregroundColor: Colors.lightBlue,
      // Tap visual feedback (ripple, bounce...)
      buttonBuilder: (child, onTap) {
        return ClipOval(
          child: Material(
            color: Colors.transparent,
            shape: const CircleBorder(),
            child: InkWell(
              splashColor: Colors.deepPurple,
              highlightColor: Colors.deepPurpleAccent.withValues(alpha: 0.5),
              onTap: onTap,
              child: child,
            ),
          ),
        );
      },
    ),
  ),
);
```

![Custom theme](/img/custom_theme.gif)

Let's see what happens in the above animation:
- In the first part, the default theme of CamerAwesome is used. Note that the Flash button bounces when tapped.
- Then, the code is changed to show the custom theme above with a hot reload.
- Finally, the UI is updated with the new purple theme and you can see that even the Flash button has a different tap effect: it is now a ripple.

If you are using the built-in widgets - even partially, it is recommended to use `AwesomeTheme` in the rest of your Camera UI to stay consistent.

You can access the current `AwesomeTheme` by using `AwesomeThemeProvider`:

```dart
final myTheme = AwesomeThemeProvider.of(context).theme
```

**Tips:**
- If you don't have a `BuildContext` to access the parent's `AwesomeTheme`, wrap your widget within a `Builder` widget.
- Use `AwesomeTheme.copyWith()` to make a copy of an `AwesomeTheme` and change only the properties you want to change. The same goes for `AwesomeButtonTheme`.

See `custom_theme.dart` for an example with a custom theme.



================================================
FILE: docs/widgets/widgets.mdx
================================================
## CamerAwesome widgets

CamerAwesome comes with a list of pre-built widgets to ease Camera integration in your app.

Here is a table of all the widgets, with a description and screenshot when appropriate.

| Widget                    | Description                                                                                                                                                                                                                       | Screenshot |
| ------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------- | --- |
| CameraAwesomeBuilder      | Main widget with which you should use CamerAwesome. Use either `CameraAwesomeBuilder.awesome()` to use CamerAwesome UI with a few customization or `CameraAwesomeBuilder.awesome()` if you want to entirely build your camera UI. |            |     |
| AwesomeCameraLayout       | Layout used by CameraAwesomeBuilder.                                                                                                                                                                                              |            |     |
| AwesomeCameraActionsRow   |                                                                                                                                                                                                                                   |            |
| AwesomeBottomActions      |                                                                                                                                                                                                                                   |            |
| AwesomeFilterWidget       | Expandable list of filters to use with CamerAwesome in picture mode.                                                                                                                                                              |            |
| AwesomeAspectRatioButton  | Button used to change aspect ratio.<br/>You can customize its behaviour and its look.                                                                                                                                             |            |
| AwesomeCameraSwitchButton | Button used to switch between front and back camera.<br/>You can customize its behaviour and its look.                                                                                                                            |            |
| AwesomeFlashButton        | Button used to switch between flash modes (none, auto, on, always).<br/>You can customize its behaviour and its look.                                                                                                             |            |
| AwesomeLocationButton     | Button used to toggle if location should be save in Exif metadata when taking a picture.<br/>You can customize its behaviour and its look.                                                                                        |            |
| AwesomePauseResumeButton  | Button used to pause or resume a vide recording.<br/>You can customize its behaviour and its look.                                                                                                                                |            |
| AwesomeCameraModeSelector | PageView used to switch between picture mode and video recording mode.<br/>You can customize its behaviour and its look.                                                                                                          |            |
| AwesomeMediaPreview       | Preview of the last media captured.<br/>You can customize its behaviour and its look.                                                                                                                                             |            |
| AwesomeSensorTypeSelector | Selector of sensor types (only iOS).<br/>You can customize its behaviour and its look.                                                                                                                                            |            |
| AwesomeOrientedWidget     | Its child rotates automatically with the camera.<br/>It can be disabled.                                                                                                                                                          |            |
| AwesomeZoomSelector       | Displays the current Zoom and allows to switch to min/max zoom (when min zoom < 1.0)                                                                                                                                              |



================================================
FILE: example/README.md
================================================
# camera_app

A new Flutter project.

## Getting Started

This project is a starting point for a Flutter application.

A few resources to get you started if this is your first Flutter project:

- [Lab: Write your first Flutter app](https://docs.flutter.dev/get-started/codelab)
- [Cookbook: Useful Flutter samples](https://docs.flutter.dev/cookbook)

For help getting started with Flutter development, view the
[online documentation](https://docs.flutter.dev/), which offers tutorials,
samples, guidance on mobile development, and a full API reference.



================================================
FILE: example/analysis_options.yaml
================================================
# This file configures the analyzer, which statically analyzes Dart code to
# check for errors, warnings, and lints.
#
# The issues identified by the analyzer are surfaced in the UI of Dart-enabled
# IDEs (https://dart.dev/tools#ides-and-editors). The analyzer can also be
# invoked from the command line by running `flutter analyze`.

# The following line activates a set of recommended lints for Flutter apps,
# packages, and plugins designed to encourage good coding practices.
include: package:flutter_lints/flutter.yaml

linter:
  # The lint rules applied to this project can be customized in the
  # section below to disable rules from the `package:flutter_lints/flutter.yaml`
  # included above or to enable additional rules. A list of all available lints
  # and their documentation is published at
  # https://dart-lang.github.io/linter/lints/index.html.
  #
  # Instead of disabling a lint rule for the entire project in the
  # section below, it can also be suppressed for a single line of code
  # or a specific dart file by using the `// ignore: name_of_lint` and
  # `// ignore_for_file: name_of_lint` syntax on the line or in the file
  # producing the lint.
  rules:
    # avoid_print: false  # Uncomment to disable the `avoid_print` rule
    # prefer_single_quotes: true  # Uncomment to enable the `prefer_single_quotes` rule

# Additional information about this file can be found at
# https://dart.dev/guides/language/analysis-options



================================================
FILE: example/pubspec.lock
================================================
# Generated by pub
# See https://dart.dev/tools/pub/glossary#lockfile
packages:
  archive:
    dependency: transitive
    description:
      name: archive
      sha256: cb6a278ef2dbb298455e1a713bda08524a175630ec643a242c399c932a0a1f7d
      url: "https://pub.dev"
    source: hosted
    version: "3.6.1"
  args:
    dependency: transitive
    description:
      name: args
      sha256: "7cf60b9f0cc88203c5a190b4cd62a99feea42759a7fa695010eb5de1c0b2252a"
      url: "https://pub.dev"
    source: hosted
    version: "2.5.0"
  async:
    dependency: transitive
    description:
      name: async
      sha256: d2872f9c19731c2e5f10444b14686eb7cc85c76274bd6c16e1816bff9a3bab63
      url: "https://pub.dev"
    source: hosted
    version: "2.12.0"
  boolean_selector:
    dependency: transitive
    description:
      name: boolean_selector
      sha256: "8aab1771e1243a5063b8b0ff68042d67334e3feab9e95b9490f9a6ebf73b42ea"
      url: "https://pub.dev"
    source: hosted
    version: "2.1.2"
  camerawesome:
    dependency: "direct main"
    description:
      path: ".."
      relative: true
    source: path
    version: "2.2.0"
  carousel_slider:
    dependency: transitive
    description:
      name: carousel_slider
      sha256: "7b006ec356205054af5beaef62e2221160ea36b90fb70a35e4deacd49d0349ae"
      url: "https://pub.dev"
    source: hosted
    version: "5.0.0"
  characters:
    dependency: transitive
    description:
      name: characters
      sha256: f71061c654a3380576a52b451dd5532377954cf9dbd272a78fc8479606670803
      url: "https://pub.dev"
    source: hosted
    version: "1.4.0"
  clock:
    dependency: transitive
    description:
      name: clock
      sha256: fddb70d9b5277016c77a80201021d40a2247104d9f4aa7bab7157b7e3f05b84b
      url: "https://pub.dev"
    source: hosted
    version: "1.1.2"
  collection:
    dependency: transitive
    description:
      name: collection
      sha256: "2f5709ae4d3d59dd8f7cd309b4e023046b57d8a6c82130785d2b0e5868084e76"
      url: "https://pub.dev"
    source: hosted
    version: "1.19.1"
  colorfilter_generator:
    dependency: transitive
    description:
      name: colorfilter_generator
      sha256: ccc2995e440b1d828d55d99150e7cad64624f3cb4a1e235000de3f93cf10d35c
      url: "https://pub.dev"
    source: hosted
    version: "0.0.8"
  convert:
    dependency: transitive
    description:
      name: convert
      sha256: "0f08b14755d163f6e2134cb58222dd25ea2a2ee8a195e53983d57c075324d592"
      url: "https://pub.dev"
    source: hosted
    version: "3.1.1"
  cross_file:
    dependency: "direct main"
    description:
      name: cross_file
      sha256: "7caf6a750a0c04effbb52a676dce9a4a592e10ad35c34d6d2d0e4811160d5670"
      url: "https://pub.dev"
    source: hosted
    version: "0.3.4+2"
  crypto:
    dependency: transitive
    description:
      name: crypto
      sha256: ff625774173754681d66daaf4a448684fb04b78f902da9cb3d308c19cc5e8bab
      url: "https://pub.dev"
    source: hosted
    version: "3.0.3"
  csslib:
    dependency: transitive
    description:
      name: csslib
      sha256: "706b5707578e0c1b4b7550f64078f0a0f19dec3f50a178ffae7006b0a9ca58fb"
      url: "https://pub.dev"
    source: hosted
    version: "1.0.0"
  dispose_scope:
    dependency: transitive
    description:
      name: dispose_scope
      sha256: "48ec38ca2631c53c4f8fa96b294c801e55c335db5e3fb9f82cede150cfe5a2af"
      url: "https://pub.dev"
    source: hosted
    version: "2.1.0"
  equatable:
    dependency: transitive
    description:
      name: equatable
      sha256: "567c64b3cb4cf82397aac55f4f0cbd3ca20d77c6c03bedbc4ceaddc08904aef7"
      url: "https://pub.dev"
    source: hosted
    version: "2.0.7"
  exif:
    dependency: "direct dev"
    description:
      name: exif
      sha256: a7980fdb3b7ffcd0b035e5b8a5e1eef7cadfe90ea6a4e85ebb62f87b96c7a172
      url: "https://pub.dev"
    source: hosted
    version: "3.3.0"
  fake_async:
    dependency: transitive
    description:
      name: fake_async
      sha256: "6a95e56b2449df2273fd8c45a662d6947ce1ebb7aafe80e550a3f68297f3cacc"
      url: "https://pub.dev"
    source: hosted
    version: "1.3.2"
  ffi:
    dependency: transitive
    description:
      name: ffi
      sha256: "493f37e7df1804778ff3a53bd691d8692ddf69702cf4c1c1096a2e41b4779e21"
      url: "https://pub.dev"
    source: hosted
    version: "2.1.2"
  flutter:
    dependency: "direct main"
    description: flutter
    source: sdk
    version: "0.0.0"
  flutter_lints:
    dependency: "direct dev"
    description:
      name: flutter_lints
      sha256: "9e8c3858111da373efc5aa341de011d9bd23e2c5c5e0c62bccf32438e192d7b1"
      url: "https://pub.dev"
    source: hosted
    version: "3.0.2"
  flutter_test:
    dependency: "direct dev"
    description: flutter
    source: sdk
    version: "0.0.0"
  flutter_web_plugins:
    dependency: transitive
    description: flutter
    source: sdk
    version: "0.0.0"
  google_mlkit_barcode_scanning:
    dependency: "direct main"
    description:
      name: google_mlkit_barcode_scanning
      sha256: b38505df2d3fdf7830979d60fee55039c2f442d189b2e06fcb2fe494ba65d0db
      url: "https://pub.dev"
    source: hosted
    version: "0.14.1"
  google_mlkit_commons:
    dependency: transitive
    description:
      name: google_mlkit_commons
      sha256: "8f40fbac10685cad4715d11e6a0d86837d9ad7168684dfcad29610282a88e67a"
      url: "https://pub.dev"
    source: hosted
    version: "0.11.0"
  google_mlkit_face_detection:
    dependency: "direct main"
    description:
      name: google_mlkit_face_detection
      sha256: f336737d5b8a86797fd4368f42a5c26aeaa9c6dcc5243f0a16b5f6f663cfb70a
      url: "https://pub.dev"
    source: hosted
    version: "0.13.1"
  google_mlkit_text_recognition:
    dependency: "direct main"
    description:
      name: google_mlkit_text_recognition
      sha256: "96173ad4dd7fd06c660e22ac3f9e9f1798a517fe7e48bee68eeec83853224224"
      url: "https://pub.dev"
    source: hosted
    version: "0.15.0"
  html:
    dependency: transitive
    description:
      name: html
      sha256: "3a7812d5bcd2894edf53dfaf8cd640876cf6cef50a8f238745c8b8120ea74d3a"
      url: "https://pub.dev"
    source: hosted
    version: "0.15.4"
  http:
    dependency: transitive
    description:
      name: http
      sha256: b9c29a161230ee03d3ccf545097fccd9b87a5264228c5d348202e0f0c28f9010
      url: "https://pub.dev"
    source: hosted
    version: "1.2.2"
  http_parser:
    dependency: transitive
    description:
      name: http_parser
      sha256: "178d74305e7866013777bab2c3d8726205dc5a4dd935297175b19a23a2e66571"
      url: "https://pub.dev"
    source: hosted
    version: "4.1.2"
  image:
    dependency: "direct main"
    description:
      name: image
      sha256: "2237616a36c0d69aef7549ab439b833fb7f9fb9fc861af2cc9ac3eedddd69ca8"
      url: "https://pub.dev"
    source: hosted
    version: "4.2.0"
  json_annotation:
    dependency: transitive
    description:
      name: json_annotation
      sha256: "1ce844379ca14835a50d2f019a3099f419082cfdd231cd86a142af94dd5c6bb1"
      url: "https://pub.dev"
    source: hosted
    version: "4.9.0"
  leak_tracker:
    dependency: transitive
    description:
      name: leak_tracker
      sha256: c35baad643ba394b40aac41080300150a4f08fd0fd6a10378f8f7c6bc161acec
      url: "https://pub.dev"
    source: hosted
    version: "10.0.8"
  leak_tracker_flutter_testing:
    dependency: transitive
    description:
      name: leak_tracker_flutter_testing
      sha256: f8b613e7e6a13ec79cfdc0e97638fddb3ab848452eff057653abd3edba760573
      url: "https://pub.dev"
    source: hosted
    version: "3.0.9"
  leak_tracker_testing:
    dependency: transitive
    description:
      name: leak_tracker_testing
      sha256: "6ba465d5d76e67ddf503e1161d1f4a6bc42306f9d66ca1e8f079a47290fb06d3"
      url: "https://pub.dev"
    source: hosted
    version: "3.0.1"
  lints:
    dependency: transitive
    description:
      name: lints
      sha256: cbf8d4b858bb0134ef3ef87841abdf8d63bfc255c266b7bf6b39daa1085c4290
      url: "https://pub.dev"
    source: hosted
    version: "3.0.0"
  matcher:
    dependency: transitive
    description:
      name: matcher
      sha256: dc58c723c3c24bf8d3e2d3ad3f2f9d7bd9cf43ec6feaa64181775e60190153f2
      url: "https://pub.dev"
    source: hosted
    version: "0.12.17"
  material_color_utilities:
    dependency: transitive
    description:
      name: material_color_utilities
      sha256: f7142bb1154231d7ea5f96bc7bde4bda2a0945d2806bb11670e30b850d56bdec
      url: "https://pub.dev"
    source: hosted
    version: "0.11.1"
  matrix2d:
    dependency: transitive
    description:
      name: matrix2d
      sha256: "188718dd3bc2a31e372cfd0791b0f77f4f13ea76164147342cc378d9132949e7"
      url: "https://pub.dev"
    source: hosted
    version: "1.0.4"
  meta:
    dependency: "direct main"
    description:
      name: meta
      sha256: e3641ec5d63ebf0d9b41bd43201a66e3fc79a65db5f61fc181f04cd27aab950c
      url: "https://pub.dev"
    source: hosted
    version: "1.16.0"
  open_filex:
    dependency: "direct main"
    description:
      name: open_filex
      sha256: "9976da61b6a72302cf3b1efbce259200cd40232643a467aac7370addf94d6900"
      url: "https://pub.dev"
    source: hosted
    version: "4.7.0"
  path:
    dependency: transitive
    description:
      name: path
      sha256: "75cca69d1490965be98c73ceaea117e8a04dd21217b37b292c9ddbec0d955bc5"
      url: "https://pub.dev"
    source: hosted
    version: "1.9.1"
  path_provider:
    dependency: "direct main"
    description:
      name: path_provider
      sha256: "50c5dd5b6e1aaf6fb3a78b33f6aa3afca52bf903a8a5298f53101fdaee55bbcd"
      url: "https://pub.dev"
    source: hosted
    version: "2.1.5"
  path_provider_android:
    dependency: transitive
    description:
      name: path_provider_android
      sha256: "490539678396d4c3c0b06efdaab75ae60675c3e0c66f72bc04c2e2c1e0e2abeb"
      url: "https://pub.dev"
    source: hosted
    version: "2.2.9"
  path_provider_foundation:
    dependency: transitive
    description:
      name: path_provider_foundation
      sha256: f234384a3fdd67f989b4d54a5d73ca2a6c422fa55ae694381ae0f4375cd1ea16
      url: "https://pub.dev"
    source: hosted
    version: "2.4.0"
  path_provider_linux:
    dependency: transitive
    description:
      name: path_provider_linux
      sha256: f7a1fe3a634fe7734c8d3f2766ad746ae2a2884abe22e241a8b301bf5cac3279
      url: "https://pub.dev"
    source: hosted
    version: "2.2.1"
  path_provider_platform_interface:
    dependency: transitive
    description:
      name: path_provider_platform_interface
      sha256: "88f5779f72ba699763fa3a3b06aa4bf6de76c8e5de842cf6f29e2e06476c2334"
      url: "https://pub.dev"
    source: hosted
    version: "2.1.2"
  path_provider_windows:
    dependency: transitive
    description:
      name: path_provider_windows
      sha256: bd6f00dbd873bfb70d0761682da2b3a2c2fccc2b9e84c495821639601d81afe7
      url: "https://pub.dev"
    source: hosted
    version: "2.3.0"
  patrol:
    dependency: "direct dev"
    description:
      name: patrol
      sha256: "2bb991db06b5e1eb2ec5c803067c41316d94d01dda93ddf16f5342073d791a20"
      url: "https://pub.dev"
    source: hosted
    version: "3.14.0"
  patrol_finders:
    dependency: transitive
    description:
      name: patrol_finders
      sha256: "4c6d78e00362fd15d7c21cfac110e501d08ada7d77000bad139b0c3c2e27ccaf"
      url: "https://pub.dev"
    source: hosted
    version: "2.7.0"
  patrol_log:
    dependency: transitive
    description:
      name: patrol_log
      sha256: "98b2701400c7a00b11533ab942bdeb44c3c714746e7cdb12e6cb93b6d06361da"
      url: "https://pub.dev"
    source: hosted
    version: "0.3.0"
  petitparser:
    dependency: transitive
    description:
      name: petitparser
      sha256: c15605cd28af66339f8eb6fbe0e541bfe2d1b72d5825efc6598f3e0a31b9ad27
      url: "https://pub.dev"
    source: hosted
    version: "6.0.2"
  platform:
    dependency: transitive
    description:
      name: platform
      sha256: "5d6b1b0036a5f331ebc77c850ebc8506cbc1e9416c27e59b439f917a902a4984"
      url: "https://pub.dev"
    source: hosted
    version: "3.1.6"
  plugin_platform_interface:
    dependency: transitive
    description:
      name: plugin_platform_interface
      sha256: "4820fbfdb9478b1ebae27888254d445073732dae3d6ea81f0b7e06d5dedc3f02"
      url: "https://pub.dev"
    source: hosted
    version: "2.1.8"
  rxdart:
    dependency: "direct main"
    description:
      name: rxdart
      sha256: "5c3004a4a8dbb94bd4bf5412a4def4acdaa12e12f269737a5751369e12d1a962"
      url: "https://pub.dev"
    source: hosted
    version: "0.28.0"
  shelf:
    dependency: transitive
    description:
      name: shelf
      sha256: e7dd780a7ffb623c57850b33f43309312fc863fb6aa3d276a754bb299839ef12
      url: "https://pub.dev"
    source: hosted
    version: "1.4.2"
  sky_engine:
    dependency: transitive
    description: flutter
    source: sdk
    version: "0.0.0"
  source_span:
    dependency: transitive
    description:
      name: source_span
      sha256: "254ee5351d6cb365c859e20ee823c3bb479bf4a293c22d17a9f1bf144ce86f7c"
      url: "https://pub.dev"
    source: hosted
    version: "1.10.1"
  sprintf:
    dependency: transitive
    description:
      name: sprintf
      sha256: "1fc9ffe69d4df602376b52949af107d8f5703b77cda567c4d7d86a0693120f23"
      url: "https://pub.dev"
    source: hosted
    version: "7.0.0"
  stack_trace:
    dependency: transitive
    description:
      name: stack_trace
      sha256: "8b27215b45d22309b5cddda1aa2b19bdfec9df0e765f2de506401c071d38d1b1"
      url: "https://pub.dev"
    source: hosted
    version: "1.12.1"
  stream_channel:
    dependency: transitive
    description:
      name: stream_channel
      sha256: "969e04c80b8bcdf826f8f16579c7b14d780458bd97f56d107d3950fdbeef059d"
      url: "https://pub.dev"
    source: hosted
    version: "2.1.4"
  string_scanner:
    dependency: transitive
    description:
      name: string_scanner
      sha256: "921cd31725b72fe181906c6a94d987c78e3b98c2e205b397ea399d4054872b43"
      url: "https://pub.dev"
    source: hosted
    version: "1.4.1"
  term_glyph:
    dependency: transitive
    description:
      name: term_glyph
      sha256: "7f554798625ea768a7518313e58f83891c7f5024f88e46e7182a4558850a4b8e"
      url: "https://pub.dev"
    source: hosted
    version: "1.2.2"
  test_api:
    dependency: transitive
    description:
      name: test_api
      sha256: fb31f383e2ee25fbbfe06b40fe21e1e458d14080e3c67e7ba0acfde4df4e0bbd
      url: "https://pub.dev"
    source: hosted
    version: "0.7.4"
  typed_data:
    dependency: transitive
    description:
      name: typed_data
      sha256: facc8d6582f16042dd49f2463ff1bd6e2c9ef9f3d5da3d9b087e244a7b564b3c
      url: "https://pub.dev"
    source: hosted
    version: "1.3.2"
  vector_math:
    dependency: transitive
    description:
      name: vector_math
      sha256: "80b3257d1492ce4d091729e3a67a60407d227c27241d6927be0130c98e741803"
      url: "https://pub.dev"
    source: hosted
    version: "2.1.4"
  video_player:
    dependency: "direct main"
    description:
      name: video_player
      sha256: "4a8c3492d734f7c39c2588a3206707a05ee80cef52e8c7f3b2078d430c84bc17"
      url: "https://pub.dev"
    source: hosted
    version: "2.9.2"
  video_player_android:
    dependency: transitive
    description:
      name: video_player_android
      sha256: "4de50df9ee786f5891d3281e1e633d7b142ef1acf47392592eb91cba5d355849"
      url: "https://pub.dev"
    source: hosted
    version: "2.6.0"
  video_player_avfoundation:
    dependency: transitive
    description:
      name: video_player_avfoundation
      sha256: d1e9a824f2b324000dc8fb2dcb2a3285b6c1c7c487521c63306cc5b394f68a7c
      url: "https://pub.dev"
    source: hosted
    version: "2.6.1"
  video_player_platform_interface:
    dependency: transitive
    description:
      name: video_player_platform_interface
      sha256: "236454725fafcacf98f0f39af0d7c7ab2ce84762e3b63f2cbb3ef9a7e0550bc6"
      url: "https://pub.dev"
    source: hosted
    version: "6.2.2"
  video_player_web:
    dependency: transitive
    description:
      name: video_player_web
      sha256: "6dcdd298136523eaf7dfc31abaf0dfba9aa8a8dbc96670e87e9d42b6f2caf774"
      url: "https://pub.dev"
    source: hosted
    version: "2.3.2"
  vm_service:
    dependency: transitive
    description:
      name: vm_service
      sha256: "0968250880a6c5fe7edc067ed0a13d4bae1577fe2771dcf3010d52c4a9d3ca14"
      url: "https://pub.dev"
    source: hosted
    version: "14.3.1"
  web:
    dependency: transitive
    description:
      name: web
      sha256: d43c1d6b787bf0afad444700ae7f4db8827f701bc61c255ac8d328c6f4d52062
      url: "https://pub.dev"
    source: hosted
    version: "1.0.0"
  xdg_directories:
    dependency: transitive
    description:
      name: xdg_directories
      sha256: faea9dee56b520b55a566385b84f2e8de55e7496104adada9962e0bd11bcff1d
      url: "https://pub.dev"
    source: hosted
    version: "1.0.4"
  xml:
    dependency: transitive
    description:
      name: xml
      sha256: b015a8ad1c488f66851d762d3090a21c600e479dc75e68328c52774040cf9226
      url: "https://pub.dev"
    source: hosted
    version: "6.5.0"
sdks:
  dart: ">=3.7.0-0 <4.0.0"
  flutter: ">=3.24.0"



================================================
FILE: example/pubspec.yaml
================================================
name: camera_app
description: A Camera App using CameraAwesome.
publish_to: "none" # Remove this line if you wish to publish to pub.dev
version: 1.0.1

environment:
  sdk: ">=3.1.0 <4.0.0"
  flutter: ">=3.3.0"

dependencies:
  cross_file: ^0.3.4+2
  flutter:
    sdk: flutter
  camerawesome:
    path: ../
  path_provider: ^2.1.5
  google_mlkit_face_detection: ^0.13.1
  google_mlkit_barcode_scanning: ^0.14.1
  google_mlkit_text_recognition: ^0.15.0
  image: ^4.1.3
  open_filex: ^4.7.0
  meta: ^1.10.0
  rxdart: ^0.28.0
  video_player: ^2.9.2

dev_dependencies:
  exif: ^3.3.0
  flutter_test:
    sdk: flutter
  patrol: ^3.14.0
  flutter_lints: ^3.0.1

flutter:
  uses-material-design: true
  disable-swift-package-manager: true
  # assets:
  #   - images/a_dot_burr.jpeg
  #   - images/a_dot_ham.jpeg

# patrol:
#   android:
#     package_name: com.example.camera_app
#   ios:
#     bundle_id:



================================================
FILE: example/.gitignore
================================================
# Miscellaneous
*.class
*.lock
*.log
*.pyc
*.swp
.DS_Store
.atom/
.buildlog/
.history
.svn/

# IntelliJ related
*.iml
*.ipr
*.iws
.idea/

# Visual Studio Code related
.classpath
.project
.settings/
.vscode/

# Flutter repo-specific
/bin/cache/
/bin/internal/bootstrap.bat
/bin/internal/bootstrap.sh
/bin/mingit/
/dev/benchmarks/mega_gallery/
/dev/bots/.recipe_deps
/dev/bots/android_tools/
/dev/devicelab/ABresults*.json
/dev/docs/doc/
/dev/docs/flutter.docs.zip
/dev/docs/lib/
/dev/docs/pubspec.yaml
/dev/integration_tests/**/xcuserdata
/dev/integration_tests/**/Pods
/packages/flutter/coverage/
version
analysis_benchmark.json


# packages file containing multi-root paths
.packages.generated

# Flutter/Dart/Pub related
**/doc/api/
.dart_tool/
.flutter-plugins
.flutter-plugins-dependencies
**/generated_plugin_registrant.dart
.packages
.pub-cache/
.pub/
build/
flutter_*.png
linked_*.ds
unlinked.ds
unlinked_spec.ds

# Android related
**/android/**/gradle-wrapper.jar
.gradle/
**/android/captures/
**/android/gradlew
**/android/gradlew.bat
**/android/local.properties
**/android/**/GeneratedPluginRegistrant.java
**/android/key.properties
*.jks
/app/.cxx/** 


# iOS/XCode related
**/ios/**/*.mode1v3
**/ios/**/*.mode2v3
**/ios/**/*.moved-aside
**/ios/**/*.pbxuser
**/ios/**/*.perspectivev3
**/ios/**/*sync/
**/ios/**/.sconsign.dblite
**/ios/**/.tags*
**/ios/**/.vagrant/
**/ios/**/DerivedData/
**/ios/**/Icon?
**/ios/**/Pods/
**/ios/**/.symlinks/
**/ios/**/profile
**/ios/**/xcuserdata
**/ios/.generated/
**/ios/Flutter/.last_build_id
**/ios/Flutter/App.framework
**/ios/Flutter/Flutter.framework
**/ios/Flutter/Flutter.podspec
**/ios/Flutter/Generated.xcconfig
**/ios/Flutter/ephemeral
**/ios/Flutter/app.flx
**/ios/Flutter/app.zip
**/ios/Flutter/flutter_assets/
**/ios/Flutter/flutter_export_environment.sh
**/ios/ServiceDefinitions.json
**/ios/Runner/GeneratedPluginRegistrant.*

# macOS
**/Flutter/ephemeral/
**/Pods/
**/macos/Flutter/GeneratedPluginRegistrant.swift
**/macos/Flutter/ephemeral
**/xcuserdata/

# Windows
**/windows/flutter/generated_plugin_registrant.cc
**/windows/flutter/generated_plugin_registrant.h

# Linux
**/linux/flutter/generated_plugin_registrant.cc
**/linux/flutter/generated_plugin_registrant.h

# Coverage
coverage/

# Symbols
app.*.symbols

# Exceptions to above rules.
!**/ios/**/default.mode1v3
!**/ios/**/default.mode2v3
!**/ios/**/default.pbxuser
!**/ios/**/default.perspectivev3
!/packages/flutter_tools/test/data/dart_dependencies_test/**/.packages
!/dev/ci/**/Gemfile.lock


================================================
FILE: example/.metadata
================================================
# This file tracks properties of this Flutter project.
# Used by Flutter tool to assess capabilities and perform upgrades etc.
#
# This file should be version controlled and should not be manually edited.

version:
  revision: "17025dd88227cd9532c33fa78f5250d548d87e9a"
  channel: "stable"

project_type: app

# Tracks metadata for the flutter migrate command
migration:
  platforms:
    - platform: root
      create_revision: 17025dd88227cd9532c33fa78f5250d548d87e9a
      base_revision: 17025dd88227cd9532c33fa78f5250d548d87e9a
    - platform: android
      create_revision: 17025dd88227cd9532c33fa78f5250d548d87e9a
      base_revision: 17025dd88227cd9532c33fa78f5250d548d87e9a
    - platform: ios
      create_revision: 17025dd88227cd9532c33fa78f5250d548d87e9a
      base_revision: 17025dd88227cd9532c33fa78f5250d548d87e9a
    - platform: linux
      create_revision: 17025dd88227cd9532c33fa78f5250d548d87e9a
      base_revision: 17025dd88227cd9532c33fa78f5250d548d87e9a
    - platform: macos
      create_revision: 17025dd88227cd9532c33fa78f5250d548d87e9a
      base_revision: 17025dd88227cd9532c33fa78f5250d548d87e9a
    - platform: web
      create_revision: 17025dd88227cd9532c33fa78f5250d548d87e9a
      base_revision: 17025dd88227cd9532c33fa78f5250d548d87e9a
    - platform: windows
      create_revision: 17025dd88227cd9532c33fa78f5250d548d87e9a
      base_revision: 17025dd88227cd9532c33fa78f5250d548d87e9a

  # User provided section

  # List of Local paths (relative to this file) that should be
  # ignored by the migrate tool.
  #
  # Files that are not part of the templates will be ignored by default.
  unmanaged_files:
    - 'lib/main.dart'
    - 'ios/Runner.xcodeproj/project.pbxproj'



================================================
FILE: example/android/gradle.properties
================================================
org.gradle.jvmargs=-Xmx4G -XX:MaxMetaspaceSize=2G -XX:+HeapDumpOnOutOfMemoryError
android.useAndroidX=true
android.enableJetifier=true



================================================
FILE: example/android/.gitignore
================================================
gradle-wrapper.jar
/.gradle
/captures/
/gradlew
/gradlew.bat
/local.properties
GeneratedPluginRegistrant.java

# Remember to never publicly share your keystore.
# See https://flutter.dev/to/reference-keystore
key.properties
**/*.keystore
**/*.jks



================================================
FILE: example/android/app/src/debug/AndroidManifest.xml
================================================
<manifest xmlns:android="http://schemas.android.com/apk/res/android">
    <!-- The INTERNET permission is required for development. Specifically,
         the Flutter tool needs it to communicate with the running application
         to allow setting breakpoints, to provide hot reload, etc.
    -->
    <uses-permission android:name="android.permission.INTERNET"/>
</manifest>



================================================
FILE: example/android/app/src/main/AndroidManifest.xml
================================================
<manifest xmlns:android="http://schemas.android.com/apk/res/android">
    <uses-permission android:name="android.permission.RECORD_AUDIO" />
    <uses-permission android:name="android.permission.ACCESS_FINE_LOCATION" />
    <uses-permission android:name="android.permission.ACCESS_COARSE_LOCATION" />
    <uses-permission android:name="android.permission.CAMERA" />

    <application
        android:label="camera_app"
        android:name="${applicationName}"
        android:icon="@mipmap/ic_launcher">
        <activity
            android:name=".MainActivity"
            android:exported="true"
            android:launchMode="singleTop"
            android:taskAffinity=""
            android:theme="@style/LaunchTheme"
            android:configChanges="orientation|keyboardHidden|keyboard|screenSize|smallestScreenSize|locale|layoutDirection|fontScale|screenLayout|density|uiMode"
            android:hardwareAccelerated="true"
            android:windowSoftInputMode="adjustResize">
            <!-- Specifies an Android theme to apply to this Activity as soon as
                 the Android process has started. This theme is visible to the user
                 while the Flutter UI initializes. After that, this theme continues
                 to determine the Window background behind the Flutter UI. -->
            <meta-data
              android:name="io.flutter.embedding.android.NormalTheme"
              android:resource="@style/NormalTheme"
              />
            <intent-filter>
                <action android:name="android.intent.action.MAIN"/>
                <category android:name="android.intent.category.LAUNCHER"/>
            </intent-filter>
        </activity>
        <!-- Don't delete the meta-data below.
             This is used by the Flutter tool to generate GeneratedPluginRegistrant.java -->
        <meta-data
            android:name="flutterEmbedding"
            android:value="2" />
    </application>
    <!-- Required to query activities that can process text, see:
         https://developer.android.com/training/package-visibility and
         https://developer.android.com/reference/android/content/Intent#ACTION_PROCESS_TEXT.

         In particular, this is used by the Flutter engine in io.flutter.plugin.text.ProcessTextPlugin. -->
    <queries>
        <intent>
            <action android:name="android.intent.action.PROCESS_TEXT"/>
            <data android:mimeType="text/plain"/>
        </intent>
    </queries>
</manifest>



================================================
FILE: example/android/app/src/main/kotlin/com/example/camera_app/MainActivity.kt
================================================
package com.example.camera_app

import io.flutter.embedding.android.FlutterActivity

class MainActivity: FlutterActivity()



================================================
FILE: example/android/app/src/main/res/drawable/launch_background.xml
================================================
<?xml version="1.0" encoding="utf-8"?>
<!-- Modify this file to customize your launch splash screen -->
<layer-list xmlns:android="http://schemas.android.com/apk/res/android">
    <item android:drawable="@android:color/white" />

    <!-- You can insert your own image assets here -->
    <!-- <item>
        <bitmap
            android:gravity="center"
            android:src="@mipmap/launch_image" />
    </item> -->
</layer-list>



================================================
FILE: example/android/app/src/main/res/drawable-v21/launch_background.xml
================================================
<?xml version="1.0" encoding="utf-8"?>
<!-- Modify this file to customize your launch splash screen -->
<layer-list xmlns:android="http://schemas.android.com/apk/res/android">
    <item android:drawable="?android:colorBackground" />

    <!-- You can insert your own image assets here -->
    <!-- <item>
        <bitmap
            android:gravity="center"
            android:src="@mipmap/launch_image" />
    </item> -->
</layer-list>








================================================
FILE: example/android/app/src/main/res/values/styles.xml
================================================
<?xml version="1.0" encoding="utf-8"?>
<resources>
    <!-- Theme applied to the Android Window while the process is starting when the OS's Dark Mode setting is off -->
    <style name="LaunchTheme" parent="@android:style/Theme.Light.NoTitleBar">
        <!-- Show a splash screen on the activity. Automatically removed when
             the Flutter engine draws its first frame -->
        <item name="android:windowBackground">@drawable/launch_background</item>
    </style>
    <!-- Theme applied to the Android Window as soon as the process has started.
         This theme determines the color of the Android Window while your
         Flutter UI initializes, as well as behind your Flutter UI while its
         running.

         This Theme is only used starting with V2 of Flutter's Android embedding. -->
    <style name="NormalTheme" parent="@android:style/Theme.Light.NoTitleBar">
        <item name="android:windowBackground">?android:colorBackground</item>
    </style>
</resources>



================================================
FILE: example/android/app/src/main/res/values-night/styles.xml
================================================
<?xml version="1.0" encoding="utf-8"?>
<resources>
    <!-- Theme applied to the Android Window while the process is starting when the OS's Dark Mode setting is on -->
    <style name="LaunchTheme" parent="@android:style/Theme.Black.NoTitleBar">
        <!-- Show a splash screen on the activity. Automatically removed when
             the Flutter engine draws its first frame -->
        <item name="android:windowBackground">@drawable/launch_background</item>
    </style>
    <!-- Theme applied to the Android Window as soon as the process has started.
         This theme determines the color of the Android Window while your
         Flutter UI initializes, as well as behind your Flutter UI while its
         running.

         This Theme is only used starting with V2 of Flutter's Android embedding. -->
    <style name="NormalTheme" parent="@android:style/Theme.Black.NoTitleBar">
        <item name="android:windowBackground">?android:colorBackground</item>
    </style>
</resources>



================================================
FILE: example/android/app/src/profile/AndroidManifest.xml
================================================
<manifest xmlns:android="http://schemas.android.com/apk/res/android">
    <!-- The INTERNET permission is required for development. Specifically,
         the Flutter tool needs it to communicate with the running application
         to allow setting breakpoints, to provide hot reload, etc.
    -->
    <uses-permission android:name="android.permission.INTERNET"/>
</manifest>



================================================
FILE: example/android/gradle/wrapper/gradle-wrapper.properties
================================================
distributionBase=GRADLE_USER_HOME
distributionPath=wrapper/dists
zipStoreBase=GRADLE_USER_HOME
zipStorePath=wrapper/dists
distributionUrl=https\://services.gradle.org/distributions/gradle-8.3-all.zip



================================================
FILE: example/integration_test/bundled_test.dart
================================================
// FIXME -> Patrol 1.1.0 -> 3.X

// import 'package:flutter_test/flutter_test.dart';

// import 'photo_test.dart' as photo_test;
// import 'ui_test.dart' as ui_test;
// import 'video_test.dart' as video_test;

// void main() {
//   group("Bundled tests > ", () {
//     ui_test.main();
//     video_test.main();
//     photo_test.main();
//   });
// }



================================================
FILE: example/integration_test/common.dart
================================================
// FIXME -> Patrol 1.1.0 -> 3.X
// import 'dart:io';

// import 'package:camerawesome/camerawesome_plugin.dart';
// import 'package:meta/meta.dart';
// import 'package:path_provider/path_provider.dart';
// import 'package:patrol/patrol.dart';

// @isTest
// void patrol(
//   String description,
//   Future<void> Function(PatrolTester) callback, {
//   bool? skip,
// }) {
//   patrolTest(
//     description,
//     skip: skip,
//     callback,
//   );
// }

// Future<void> allowPermissionsIfNeeded(PatrolTester $) async {
//   if (await $.native.isPermissionDialogVisible()) {
//     await $.native.grantPermissionWhenInUse();
//   }
//   if (await $.native.isPermissionDialogVisible()) {
//     await $.native.grantPermissionWhenInUse();
//   }
//   if (await $.native.isPermissionDialogVisible()) {
//     await $.native.grantPermissionWhenInUse();
//   }
//   if (await $.native.isPermissionDialogVisible()) {
//     await $.native.grantPermissionWhenInUse();
//   }
// }

// Future<CaptureRequest> Function(List<Sensor> sensors) tempPath(
//     String pictureName) {
//   return (sensors) async {
//     final file = File(
//       '${(await getTemporaryDirectory()).path}/test/$pictureName',
//     );
//     await file.create(recursive: true);
//     return SingleCaptureRequest(file.path, sensors.first);
//   };
// }



================================================
FILE: example/integration_test/concurrent_camera_test.dart
================================================
// FIXME -> Patrol 1.1.0 -> 3.X

// import 'package:camera_app/drivable_camera.dart';
// import 'package:camerawesome/camerawesome_plugin.dart';
// import 'package:flutter_test/flutter_test.dart';

// import 'common.dart';

// main() {
//   patrol('Concurrent > Basic run', ($) async {
//     await $.pumpWidgetAndSettle(
//       DrivableCamera(
//         sensors: [
//           Sensor.position(SensorPosition.back),
//           Sensor.position(SensorPosition.front)
//         ],
//         saveConfig: SaveConfig.photoAndVideo(
//           photoPathBuilder: tempPath('single_photo_back.jpg'),
//           videoPathBuilder: tempPath('single_video_back.mp4'),
//         ),
//       ),
//     );
//     await allowPermissionsIfNeeded($);
//     await $.pumpAndSettle();
//     // await $(AwesomeCaptureButton).tap(andSettle: false);

//     await $.pump(const Duration(seconds: 2));
//     await $.pump();
//     expect($(AwesomeCaptureButton), findsOneWidget);
//   });
// }



================================================
FILE: example/integration_test/photo_test.dart
================================================
// FIXME -> Patrol 1.1.0 -> 3.X

// // ignore_for_file: avoid_print
// import 'dart:io';

// import 'package:camera_app/drivable_camera.dart';
// import 'package:camerawesome/camerawesome_plugin.dart';
// import 'package:flutter_test/flutter_test.dart';

// import 'common.dart';

// // To run it, you have to use `patrol drive` instead of `flutter test`.
// void main() {
//   photoTests();
// }

// void photoTests() {
//   for (var sensor in SensorPosition.values) {
//     patrol(
//       'Take pictures > single picture ${sensor.name} camera',
//       ($) async {
//         final sensors = [Sensor.position(sensor)];
//         await $.pumpWidgetAndSettle(
//           DrivableCamera(
//             sensors: sensors,
//             saveConfig: SaveConfig.photo(
//               pathBuilder: tempPath('single_photo_back.jpg'),
//             ),
//           ),
//         );

//         await allowPermissionsIfNeeded($);

//         final request = await tempPath('single_photo_back.jpg')(sensors);
//         final filePath = request.when(single: (single) => single.file!.path);
//         await $(AwesomeCaptureButton).tap();

//         expect(File(filePath).existsSync(), true);
//         // File size should be quite high (at least more than 100)
//         expect(File(filePath).lengthSync(), greaterThan(100));
//       },
//     );

//     patrol(
//       'Take pictures > multiple picture ${sensor.name} camera',
//       ($) async {
//         int idxPicture = 0;
//         const picturesToTake = 3;
//         final sensors = [Sensor.position(sensor)];
//         await $.pumpWidgetAndSettle(
//           DrivableCamera(
//             sensors: sensors,
//             saveConfig: SaveConfig.photo(
//               pathBuilder: (sensors) async {
//                 final request = await tempPath(
//                     'multiple_photo_${sensor.name}_$idxPicture.jpg')(sensors);
//                 idxPicture++;
//                 return request;
//               },
//             ),
//           ),
//         );

//         await allowPermissionsIfNeeded($);

//         for (int i = 0; i < picturesToTake; i++) {
//           final request = await tempPath(
//               'multiple_photo_${sensor.name}_$idxPicture.jpg')(sensors);
//           final filePath = request.when(single: (single) => single.file!.path);
//           await $(AwesomeCaptureButton).tap();
//           expect(File(filePath).existsSync(), true);
//           // File size should be quite high (at least more than 100)
//           expect(File(filePath).lengthSync(), greaterThan(100));
//         }
//       },
//     );
//   }

//   patrol(
//     'Take pictures > One with ${SensorPosition.back} then one with ${SensorPosition.front}',
//     ($) async {
//       int idxSensor = 0;
//       final switchingSensors = [
//         SensorPosition.back,
//         SensorPosition.front,
//         SensorPosition.back,
//       ];
//       final initialSensors = [Sensor.position(SensorPosition.back)];
//       await $.pumpWidgetAndSettle(
//         DrivableCamera(
//           sensors: initialSensors,
//           saveConfig: SaveConfig.photo(
//             pathBuilder: (sensors) async {
//               final request = await tempPath(
//                       'switch_sensor_photo_${idxSensor}_${switchingSensors[idxSensor].name}.jpg')(
//                   sensors);
//               idxSensor++;
//               return request;
//             },
//           ),
//         ),
//       );

//       await allowPermissionsIfNeeded($);

//       for (int i = 0; i < switchingSensors.length; i++) {
//         final request = await tempPath(
//                 'switch_sensor_photo_${idxSensor}_${switchingSensors[idxSensor].name}.jpg')(
//             initialSensors);
//         final filePath = request.when(single: (single) => single.file!.path);
//         if (i > 0 && switchingSensors[i - 1] != switchingSensors[i]) {
//           await $.tester.pumpAndSettle();
//           final switchButton = find.byType(AwesomeCameraSwitchButton);
//           await $.tester.tap(switchButton, warnIfMissed: false);
//         }
//         await $(AwesomeCaptureButton).tap();
//         await Future.delayed(const Duration(milliseconds: 2000));

//         expect(File(filePath).existsSync(), true);
//         // File size should be quite high (at least more than 100)
//         expect(File(filePath).lengthSync(), greaterThan(100));
//       }
//     },
//   );
// }



================================================
FILE: example/integration_test/plugin_integration_test.dart
================================================
// FIXME -> Patrol 1.1.0 -> 3.X

// // This is a basic Flutter integration test.
// //
// // Since integration tests run in a full Flutter application, they can interact
// // with the host side of a plugin implementation, unlike Dart unit tests.
// //
// // For more information about Flutter integration tests, please see
// // https://flutter.dev/to/integration-testing

// import 'package:flutter_test/flutter_test.dart';
// import 'package:integration_test/integration_test.dart';

// import 'package:camerawesome/camerawesome.dart';

// void main() {
//   IntegrationTestWidgetsFlutterBinding.ensureInitialized();

//   testWidgets('getPlatformVersion test', (WidgetTester tester) async {
//     final Camerawesome plugin = Camerawesome();
//     final String? version = await plugin.getPlatformVersion();
//     // The version string depends on the host platform running the test, so
//     // just assert that some non-empty string is returned.
//     expect(version?.isNotEmpty, true);
//   });
// }



================================================
FILE: example/integration_test/ui_test.dart
================================================
// FIXME -> Patrol 1.1.0 -> 3.X

// // ignore_for_file: avoid_print
// import 'dart:io';

// import 'package:camera_app/drivable_camera.dart';
// import 'package:camerawesome/camerawesome_plugin.dart';
// import 'package:camerawesome/pigeon.dart';
// import 'package:exif/exif.dart';
// import 'package:flutter/material.dart';
// import 'package:flutter_test/flutter_test.dart';

// import 'common.dart';

// // To run it, you have to use `patrol drive` instead of `flutter test`.
// void main() {
//   patrol(
//     'UI > Photo and video mode',
//     ($) async {
//       await $.pumpWidgetAndSettle(
//         DrivableCamera(
//           sensors: [Sensor.position(SensorPosition.back)],
//           saveConfig: SaveConfig.photoAndVideo(
//             photoPathBuilder: tempPath('single_photo_back.jpg'),
//             videoPathBuilder: tempPath('single_video_back.mp4'),
//           ),
//         ),
//       );

//       await allowPermissionsIfNeeded($);

//       expect($(AwesomeAspectRatioButton), findsOneWidget);
//       expect($(AwesomeFlashButton), findsOneWidget);
//       expect(
//         $(AwesomeLocationButton).$(AwesomeBouncingWidget),
//         findsOneWidget,
//       );
//       expect($(AwesomeCameraSwitchButton), findsOneWidget);
//       expect($(AwesomeMediaPreview), findsNothing);
//       // expect($(AwesomePauseResumeButton), findsNothing);
//       expect($(AwesomeCaptureButton), findsOneWidget);

//       // After the first picture taken, mediaPreview should become visible
//       await $(AwesomeCaptureButton).tap();
//       expect($(AwesomeMediaPreview), findsOneWidget);

//       expect($(AwesomeCameraModeSelector).$(PageView), findsOneWidget);

//       // Switch to video mode
//       await $.tap(find.text("VIDEO"));
//       await $.pump(const Duration(milliseconds: 3000));
//       expect($(AwesomeAspectRatioButton), findsNothing);
//       expect($(AwesomeFlashButton), findsOneWidget);
//       expect(
//         $(AwesomeLocationButton).$(AwesomeBouncingWidget),
//         findsNothing,
//       ); // Not visible anymore
//       expect($(AwesomeCameraSwitchButton), findsOneWidget);
//       // expect($(AwesomeEnableAudioButton), findsNothing);
//       expect($(AwesomeCaptureButton), findsOneWidget);
//       expect($(AwesomeMediaPreview), findsOneWidget);
//       // expect($(AwesomePauseResumeButton), findsNothing);

//       expect($(AwesomeCameraModeSelector).$(PageView), findsOneWidget);
//     },
//   );

//   patrol(
//     'UI > Photo mode elements',
//     ($) async {
//       await $.pumpWidgetAndSettle(
//         DrivableCamera(
//           sensors: [Sensor.position(SensorPosition.back)],
//           saveConfig: SaveConfig.photo(
//             pathBuilder: tempPath('single_photo_back.jpg'),
//           ),
//         ),
//       );

//       await allowPermissionsIfNeeded($);

//       expect($(AwesomeAspectRatioButton), findsOneWidget);
//       expect($(AwesomeFlashButton), findsOneWidget);
//       expect(
//         $(AwesomeLocationButton).$(AwesomeBouncingWidget),
//         findsOneWidget,
//       );
//       expect($(AwesomeCameraSwitchButton), findsOneWidget);
//       // expect($(AwesomeEnableAudioButton), findsNothing);
//       expect($(AwesomeMediaPreview), findsNothing);
//       // expect($(AwesomePauseResumeButton), findsNothing);
//       expect($(AwesomeCaptureButton), findsOneWidget);
//       expect($(AwesomeCameraModeSelector).$(PageView), findsNothing);
//     },
//   );

//   patrol(
//     'UI > Video mode elements',
//     ($) async {
//       await $.pumpWidgetAndSettle(
//         DrivableCamera(
//           sensors: [Sensor.position(SensorPosition.back)],
//           saveConfig: SaveConfig.photoAndVideo(
//             photoPathBuilder: tempPath('single_photo_back.jpg'),
//             videoPathBuilder: tempPath('single_video_back.mp4'),
//             initialCaptureMode: CaptureMode.video,
//           ),
//         ),
//       );

//       await allowPermissionsIfNeeded($);
//       await $.pump(const Duration(milliseconds: 1000));

//       // Ratio button is not visible in video mode
//       expect($(AwesomeAspectRatioButton), findsNothing);
//       expect($(AwesomeFlashButton), findsOneWidget);
//       expect($(AwesomeLocationButton).$(AwesomeBouncingWidget), findsNothing);
//       expect($(AwesomeCameraSwitchButton), findsOneWidget);
//       // TODO Add an enableAudioButton somewhere in awesome UI ?
//       // expect($(AwesomeEnableAudioButton), findsOneWidget);
//       expect($(AwesomeMediaPreview), findsNothing);
//       expect($(AwesomePauseResumeButton), findsNothing);
//       expect($(AwesomeCaptureButton), findsOneWidget);
//       expect($(AwesomeCameraModeSelector).$(PageView), findsOneWidget);

//       await $(AwesomeCaptureButton).tap(andSettle: false);
//       await allowPermissionsIfNeeded($);
//       await $.pump(const Duration(milliseconds: 2000));

//       // // Recording
//       expect($(AwesomeAspectRatioButton), findsNothing);
//       expect($(AwesomeFlashButton), findsNothing);
//       expect($(AwesomeLocationButton).$(AwesomeBouncingWidget), findsNothing);
//       expect($(AwesomeCameraSwitchButton), findsNothing);
//       // expect($(AwesomeEnableAudioButton), findsOneWidget);
//       expect($(AwesomeMediaPreview), findsNothing);
//       expect($(AwesomePauseResumeButton), findsOneWidget);
//       expect($(AwesomeCaptureButton), findsOneWidget);
//       expect($(AwesomeCameraModeSelector).$(PageView), findsNothing);

//       await $(AwesomeCaptureButton).tap(andSettle: false);
//       await $.pump(const Duration(milliseconds: 4000));

//       // Not recording
//       expect($(AwesomeAspectRatioButton), findsNothing);
//       expect($(AwesomeFlashButton), findsOneWidget);
//       expect($(AwesomeLocationButton).$(AwesomeBouncingWidget), findsNothing);
//       expect($(AwesomeCameraSwitchButton), findsOneWidget);
//       // expect($(AwesomeEnableAudioButton), findsOneWidget);

//       // Sometimes these pump work, sometimes they don't...
//       await $.pump(const Duration(milliseconds: 3000));
//       expect($(AwesomeMediaPreview), findsOneWidget);
//       expect($(AwesomePauseResumeButton), findsNothing);
//       expect($(AwesomeCaptureButton), findsOneWidget);
//       expect($(AwesomeCameraModeSelector).$(PageView), findsOneWidget);
//     },
//   );

//   // Back camera should have a flash and be able to switch between all flash modes

//   patrol(
//     'UI > Switching flash mode should work',
//     ($) async {
//       await $.pumpWidgetAndSettle(
//         DrivableCamera(
//           sensors: [Sensor.position(SensorPosition.back)],
//           saveConfig: SaveConfig.photo(
//             pathBuilder: tempPath('single_photo_back.jpg'),
//           ),
//         ),
//       );

//       await allowPermissionsIfNeeded($);

//       // FLash off by default
//       expect(
//         find
//             .byType(Icon)
//             .evaluate()
//             .where(
//               (element) => (element.widget as Icon).icon == Icons.flash_off,
//             )
//             .length,
//         equals(1),
//       );
//       final flashButton = find.byType(AwesomeFlashButton);
//       await $.tester.tap(flashButton, warnIfMissed: false);
//       await $.pump(const Duration(milliseconds: 400));
//       // FLash auto next
//       expect(
//         find
//             .byType(Icon)
//             .evaluate()
//             .where(
//               (element) => (element.widget as Icon).icon == Icons.flash_auto,
//             )
//             .length,
//         equals(1),
//       );
//       await $.tester.tap(flashButton, warnIfMissed: false);
//       await $.pump(const Duration(milliseconds: 100));
//       // FLash on next
//       expect(
//         find
//             .byType(Icon)
//             .evaluate()
//             .where((element) => (element.widget as Icon).icon == Icons.flash_on)
//             .length,
//         equals(1),
//       );
//       await $.tester.tap(flashButton, warnIfMissed: false);
//       await $.pump(const Duration(milliseconds: 100));
//       // FLash always next
//       expect(
//         find
//             .byType(Icon)
//             .evaluate()
//             .where(
//               (element) => (element.widget as Icon).icon == Icons.flashlight_on,
//             )
//             .length,
//         equals(1),
//       );
//       await $.tester.tap(flashButton, warnIfMissed: false);
//       await $.pump(const Duration(milliseconds: 100));
//       // Back to flash none
//       expect(
//         find
//             .byType(Icon)
//             .evaluate()
//             .where(
//               (element) => (element.widget as Icon).icon == Icons.flash_off,
//             )
//             .length,
//         equals(1),
//       );
//     },
//   );

//   // This group of test only works when location is enabled on the phone
//   // TODO Try to use Patrol to enable location manually on the device

//   patrol(
//     'Location > Do NOT save if not specified',
//     ($) async {
//       final sensors = [Sensor.position(SensorPosition.back)];
//       await $.pumpWidgetAndSettle(
//         DrivableCamera(
//           sensors: sensors,
//           saveConfig: SaveConfig.photo(
//             pathBuilder: tempPath('single_photo_back_no_gps.jpg'),
//           ),
//         ),
//       );

//       await allowPermissionsIfNeeded($);

//       // await $.native.openQuickSettings();
//       // await $.native.tap(Selector(text: 'Location'));
//       // await $.native.pressBack();

//       await $(AwesomeCaptureButton).tap();
//       final request = await tempPath('single_photo_back_no_gps.jpg')(sensors);
//       final filePath = request.when(single: (single) => single.file!.path);
//       final exif = await readExifFromFile(File(filePath));
//       final gpsTags = exif.entries.where(
//         (element) => element.key.contains('GPSDate'),
//       );
//       // TODO for some reason, 8 gps fields are set on android emulators even when no gps data are provided. When GPS is on, there are 11 fields instead.
//       expect(gpsTags.length, lessThan(11));
//     },
//   );

//   // This test might not pass in Firebase Test Lab because location does not seem to be activated. It works on local device.
//   // TODO Try to use Patrol to enable location manually on the device

//   patrol(
//     'Location > Save if specified',
//     ($) async {
//       final sensors = [Sensor.position(SensorPosition.back)];
//       await $.pumpWidgetAndSettle(
//         DrivableCamera(
//           sensors: sensors,
//           saveConfig: SaveConfig.photo(
//             pathBuilder: tempPath('single_photo_back_gps.jpg'),
//             exifPreferences: ExifPreferences(saveGPSLocation: true),
//           ),
//         ),
//       );

//       await allowPermissionsIfNeeded($);

//       await $(AwesomeCaptureButton).tap(andSettle: false);
//       // TODO Wait for media captured instead of a fixed duration (taking picture + retrieving locaiton might take a lot of time)
//       await $.pump(const Duration(seconds: 4));
//       final request = await tempPath('single_photo_back_gps.jpg')(sensors);
//       final filePath = request.when(single: (single) => single.file!.path);
//       final exif = await readExifFromFile(File(filePath));
//       // for (final entry in exif.entries) {
//       //   print('EXIF_PRINT > ${entry.key} : ${entry.value}');
//       // }
//       final gpsTags = exif.entries.where(
//         (element) => element.key.startsWith('GPS GPS'),
//       );
//       expect(gpsTags.length, greaterThan(0));
//     },
//   );

//   patrol(
//     'Focus > On camera preview tap, show focus indicator for 2 seconds',
//     ($) async {
//       final sensors = [Sensor.position(SensorPosition.back)];
//       await $.pumpWidgetAndSettle(
//         DrivableCamera(
//           sensors: sensors,
//           saveConfig: SaveConfig.photo(
//             pathBuilder: tempPath('single_photo_back.jpg'),
//           ),
//         ),
//       );

//       await allowPermissionsIfNeeded($);

//       expect($(AwesomeFocusIndicator), findsNothing);
//       await $(AwesomeCameraGestureDetector).tap(andSettle: false);
//       expect($(AwesomeFocusIndicator), findsOneWidget);
//       // [OnPreviewTap.tapPainterDuration] should last 2 seconds by default
//       await $.pump(const Duration(seconds: 2));
//       expect($(AwesomeFocusIndicator), findsNothing);
//     },
//   );

//   patrol(
//     'Focus > On multiple focus, last more than 2 seconds',
//     ($) async {
//       final sensors = [Sensor.position(SensorPosition.back)];
//       await $.pumpWidgetAndSettle(
//         DrivableCamera(
//           sensors: sensors,
//           saveConfig: SaveConfig.photo(
//             pathBuilder: tempPath('single_photo_back.jpg'),
//           ),
//         ),
//       );

//       await allowPermissionsIfNeeded($);

//       expect($(AwesomeFocusIndicator), findsNothing);
//       await $(AwesomeCameraGestureDetector).tap(andSettle: false);
//       expect($(AwesomeFocusIndicator), findsOneWidget);
//       await $.pump(const Duration(seconds: 1));
//       // Focus again after one sec, meaning the focus indicator should last 3 seconds total
//       await $(AwesomeCameraGestureDetector).tap(andSettle: false);
//       await $.pump(const Duration(seconds: 1));
//       expect($(AwesomeFocusIndicator), findsOneWidget);
//       await $.pump(const Duration(seconds: 1));
//       expect($(AwesomeFocusIndicator), findsNothing);
//     },
//   );
// }



================================================
FILE: example/integration_test/video_test.dart
================================================
// FIXME -> Patrol 1.1.0 -> 3.X

// // ignore_for_file: avoid_print
// import 'dart:io';

// import 'package:camera_app/drivable_camera.dart';
// import 'package:camerawesome/camerawesome_plugin.dart';
// import 'package:flutter_test/flutter_test.dart';

// import 'common.dart';

// // To run it, you have to use `patrol drive` instead of `flutter test`.
// void main() {
//   for (final sensor in SensorPosition.values) {
//     patrol(
//       'Record video >  one with ${SensorPosition.back}',
//       ($) async {
//         final sensors = [Sensor.position(sensor)];
//         await $.pumpWidgetAndSettle(
//           DrivableCamera(
//             sensors: sensors,
//             saveConfig: SaveConfig.video(
//               pathBuilder: tempPath('record_video_single_${sensor.name}.mp4'),
//             ),
//           ),
//         );
//         await allowPermissionsIfNeeded($);

//         final request =
//             await tempPath('record_video_single_${sensor.name}.mp4')(sensors);
//         final filePath = request.when(single: (single) => single.file!.path);
//         await $(AwesomeCaptureButton).tap(andSettle: false);
//         await allowPermissionsIfNeeded($);
//         await $.pump(const Duration(seconds: 3));
//         await $(AwesomeCaptureButton).tap();
//         await $.pump(const Duration(milliseconds: 2000));

//         expect(File(filePath).existsSync(), true);
//         // File size should be quite high (at least more than 100)
//         expect(File(filePath).lengthSync(), greaterThan(100));
//       },
//     );

//     patrol(
//       'Record video > multiple ${sensor.name} camera',
//       ($) async {
//         int idxVideo = 0;
//         const videosToTake = 3;
//         final sensors = [Sensor.position(sensor)];
//         await $.pumpWidgetAndSettle(
//           DrivableCamera(
//             sensors: sensors,
//             saveConfig: SaveConfig.video(
//               pathBuilder:
//                   tempPath('multiple_video_${sensor.name}_$idxVideo.mp4'),
//             ),
//           ),
//         );
//         await allowPermissionsIfNeeded($);

//         for (int i = 0; i < videosToTake; i++) {
//           final request = await tempPath(
//               'multiple_video_${sensor.name}_$idxVideo.mp4')(sensors);
//           final filePath = request.when(single: (single) => single.file!.path);
//           await $(AwesomeCaptureButton).tap(andSettle: false);
//           await allowPermissionsIfNeeded($);
//           await Future.delayed(const Duration(seconds: 3));
//           await $(AwesomeCaptureButton).tap();
//           await $.pump(const Duration(milliseconds: 1000));
//           expect(File(filePath).existsSync(), true);
//           // File size should be quite high (at least more than 100)
//           expect(File(filePath).lengthSync(), greaterThan(100));
//         }
//       },
//     );

//     patrol(
//       'Record video > Pause and resume',
//       ($) async {
//         final sensors = [Sensor.position(sensor)];
//         await $.pumpWidgetAndSettle(
//           DrivableCamera(
//             sensors: sensors,
//             saveConfig: SaveConfig.video(
//                 pathBuilder: tempPath('pause_resume_video_$sensor.mp4')),
//           ),
//         );

//         await allowPermissionsIfNeeded($);

//         final request =
//             await tempPath('pause_resume_video_$sensor.mp4')(sensors);
//         final filePath = request.when(single: (single) => single.file!.path);

//         await $(AwesomeCaptureButton).tap(andSettle: false);
//         await allowPermissionsIfNeeded($);
//         await Future.delayed(const Duration(seconds: 2));
//         await $.tester.pumpAndSettle();
//         final pauseResumeButton = find.byType(AwesomePauseResumeButton);
//         await $.tester.tap(pauseResumeButton, warnIfMissed: false);
//         await Future.delayed(const Duration(seconds: 3));
//         await $.tester.tap(pauseResumeButton, warnIfMissed: false);
//         await Future.delayed(const Duration(seconds: 1));

//         await $(AwesomeCaptureButton).tap();
//         await $.pump(const Duration(milliseconds: 1000));

//         final file = File(filePath);
//         expect(file.existsSync(), true);
//         // File size should be quite high (at least more than 100)
//         expect(file.lengthSync(), greaterThan(100));
//         // We might test that the video lasts 3 seconds (2+1) and not 6 (2+3+1)
//         // Didn't work using video_player (error in native side) neither using
//         // video_compress (metadata null)
//       },
//     );
//   }

//   patrol(
//     'Record video > One with ${SensorPosition.back} then one with ${SensorPosition.front}',
//     ($) async {
//       int idxSensor = 0;
//       final switchingSensors = [
//         SensorPosition.back,
//         SensorPosition.front,
//         SensorPosition.back,
//       ];
//       final initialSensors = [Sensor.position(SensorPosition.back)];
//       await $.pumpWidgetAndSettle(
//         DrivableCamera(
//           sensors: initialSensors,
//           saveConfig: SaveConfig.video(
//             pathBuilder: (sensors) async {
//               final path = await tempPath(
//                       'switch_sensor_video_${idxSensor}_${switchingSensors[idxSensor].name}.mp4')(
//                   sensors);
//               idxSensor++;
//               return path;
//             },
//           ),
//         ),
//       );

//       await allowPermissionsIfNeeded($);

//       for (int i = 0; i < switchingSensors.length; i++) {
//         final request = await tempPath(
//                 'switch_sensor_video_${i}_${switchingSensors[i].name}.mp4')(
//             initialSensors);
//         final filePath = request.when(single: (single) => single.file!.path);

//         if (i > 0 && switchingSensors[i - 1] != switchingSensors[i]) {
//           await $.tester.pumpAndSettle();
//           final switchButton = find.byType(AwesomeCameraSwitchButton);
//           await $.tester.tap(switchButton, warnIfMissed: false);
//           await $.pump(const Duration(milliseconds: 2000));
//         }
//         await $(AwesomeCaptureButton).tap(andSettle: false);
//         await allowPermissionsIfNeeded($);
//         await Future.delayed(const Duration(seconds: 3));
//         await $(AwesomeCaptureButton).tap(andSettle: false);
//         await $.pump(const Duration(milliseconds: 2000));

//         expect(File(filePath).existsSync(), true);
//         // File size should be quite high (at least more than 100)
//         expect(File(filePath).lengthSync(), greaterThan(100));
//       }
//     },
//   );
// }



================================================
FILE: example/ios/Podfile
================================================
# Uncomment this line to define a global platform for your project
platform :ios, '15.5'

# CocoaPods analytics sends network stats synchronously affecting flutter build latency.
ENV['COCOAPODS_DISABLE_STATS'] = 'true'

project 'Runner', {
  'Debug' => :debug,
  'Profile' => :release,
  'Release' => :release,
}

def flutter_root
  generated_xcode_build_settings_path = File.expand_path(File.join('..', 'Flutter', 'Generated.xcconfig'), __FILE__)
  unless File.exist?(generated_xcode_build_settings_path)
    raise "#{generated_xcode_build_settings_path} must exist. If you're running pod install manually, make sure flutter pub get is executed first"
  end

  File.foreach(generated_xcode_build_settings_path) do |line|
    matches = line.match(/FLUTTER_ROOT\=(.*)/)
    return matches[1].strip if matches
  end
  raise "FLUTTER_ROOT not found in #{generated_xcode_build_settings_path}. Try deleting Generated.xcconfig, then run flutter pub get"
end

require File.expand_path(File.join('packages', 'flutter_tools', 'bin', 'podhelper'), flutter_root)

flutter_ios_podfile_setup

target 'Runner' do
  use_frameworks!
  use_modular_headers!

  target 'RunnerUITests' do
    inherit! :complete
  end
  
  flutter_install_all_ios_pods File.dirname(File.realpath(__FILE__))
end

post_install do |installer|
  installer.pods_project.targets.each do |target|
    flutter_additional_ios_build_settings(target)

    target.build_configurations.each do |config|
      config.build_settings['IPHONEOS_DEPLOYMENT_TARGET'] = '13.0'
    end

    target.build_configurations.each do |config|
        config.build_settings['GCC_PREPROCESSOR_DEFINITIONS'] ||= [
          '$(inherited)',
  
          ## dart: PermissionGroup.calendar
          # 'PERMISSION_EVENTS=1',
  
          ## dart: PermissionGroup.reminders
          # 'PERMISSION_REMINDERS=1',
  
          ## dart: PermissionGroup.contacts
          'PERMISSION_CONTACTS=1',
  
          ## dart: PermissionGroup.camera
          'PERMISSION_CAMERA=1',
  
          ## dart: PermissionGroup.microphone
          'PERMISSION_MICROPHONE=1',
  
          ## dart: PermissionGroup.speech
          # 'PERMISSION_SPEECH_RECOGNIZER=1',
  
          ## dart: PermissionGroup.photos
          # 'PERMISSION_PHOTOS=1',
  
          ## dart: [PermissionGroup.location, PermissionGroup.locationAlways, PermissionGroup.locationWhenInUse]
          'PERMISSION_LOCATION=1',
  
          ## dart: PermissionGroup.notification
          # 'PERMISSION_NOTIFICATIONS=1',
  
          ## dart: PermissionGroup.mediaLibrary
          # 'PERMISSION_MEDIA_LIBRARY=1',
  
          ## dart: PermissionGroup.sensors
          # 'PERMISSION_SENSORS=1',   
  
          ## dart: PermissionGroup.bluetooth
          # 'PERMISSION_BLUETOOTH=1',
  
          ## dart: PermissionGroup.appTrackingTransparency
          # 'PERMISSION_APP_TRACKING_TRANSPARENCY=1',
  
          ## dart: PermissionGroup.criticalAlerts
          # 'PERMISSION_CRITICAL_ALERTS=1'
        ]
      end 
  end
end



================================================
FILE: example/ios/Podfile.lock
================================================
PODS:
  - better_open_file (0.0.1):
    - Flutter
  - camerawesome (0.0.1):
    - Flutter
  - CocoaAsyncSocket (7.6.5)
  - Flutter (1.0.0)
  - google_mlkit_barcode_scanning (0.14.1):
    - Flutter
    - google_mlkit_commons
    - GoogleMLKit/BarcodeScanning (~> 7.0.0)
  - google_mlkit_commons (0.11.0):
    - Flutter
    - MLKitVision
  - google_mlkit_face_detection (0.13.1):
    - Flutter
    - google_mlkit_commons
    - GoogleMLKit/FaceDetection (~> 7.0.0)
  - google_mlkit_text_recognition (0.15.0):
    - Flutter
    - google_mlkit_commons
    - GoogleMLKit/TextRecognition (~> 7.0.0)
  - GoogleDataTransport (10.1.0):
    - nanopb (~> 3.30910.0)
    - PromisesObjC (~> 2.4)
  - GoogleMLKit/BarcodeScanning (7.0.0):
    - GoogleMLKit/MLKitCore
    - MLKitBarcodeScanning (~> 6.0.0)
  - GoogleMLKit/FaceDetection (7.0.0):
    - GoogleMLKit/MLKitCore
    - MLKitFaceDetection (~> 6.0.0)
  - GoogleMLKit/MLKitCore (7.0.0):
    - MLKitCommon (~> 12.0.0)
  - GoogleMLKit/TextRecognition (7.0.0):
    - GoogleMLKit/MLKitCore
    - MLKitTextRecognition (~> 5.0.0)
  - GoogleToolboxForMac/Defines (4.2.1)
  - GoogleToolboxForMac/Logger (4.2.1):
    - GoogleToolboxForMac/Defines (= 4.2.1)
  - "GoogleToolboxForMac/NSData+zlib (4.2.1)":
    - GoogleToolboxForMac/Defines (= 4.2.1)
  - GoogleUtilities/Environment (8.0.2):
    - GoogleUtilities/Privacy
  - GoogleUtilities/Logger (8.0.2):
    - GoogleUtilities/Environment
    - GoogleUtilities/Privacy
  - GoogleUtilities/Privacy (8.0.2)
  - GoogleUtilities/UserDefaults (8.0.2):
    - GoogleUtilities/Logger
    - GoogleUtilities/Privacy
  - GTMSessionFetcher/Core (3.5.0)
  - MLImage (1.0.0-beta6)
  - MLKitBarcodeScanning (6.0.0):
    - MLKitCommon (~> 12.0)
    - MLKitVision (~> 8.0)
  - MLKitCommon (12.0.0):
    - GoogleDataTransport (~> 10.0)
    - GoogleToolboxForMac/Logger (< 5.0, >= 4.2.1)
    - "GoogleToolboxForMac/NSData+zlib (< 5.0, >= 4.2.1)"
    - GoogleUtilities/Logger (~> 8.0)
    - GoogleUtilities/UserDefaults (~> 8.0)
    - GTMSessionFetcher/Core (< 4.0, >= 3.3.2)
  - MLKitFaceDetection (6.0.0):
    - MLKitCommon (~> 12.0)
    - MLKitVision (~> 8.0)
  - MLKitTextRecognition (5.0.0):
    - MLKitCommon (~> 12.0)
    - MLKitTextRecognitionCommon (= 4.0.0)
    - MLKitVision (~> 8.0)
  - MLKitTextRecognitionCommon (4.0.0):
    - MLKitCommon (~> 12.0)
    - MLKitVision (~> 8.0)
  - MLKitVision (8.0.0):
    - GoogleToolboxForMac/Logger (< 5.0, >= 4.2.1)
    - "GoogleToolboxForMac/NSData+zlib (< 5.0, >= 4.2.1)"
    - GTMSessionFetcher/Core (< 4.0, >= 3.3.2)
    - MLImage (= 1.0.0-beta6)
    - MLKitCommon (~> 12.0)
  - nanopb (3.30910.0):
    - nanopb/decode (= 3.30910.0)
    - nanopb/encode (= 3.30910.0)
  - nanopb/decode (3.30910.0)
  - nanopb/encode (3.30910.0)
  - path_provider_foundation (0.0.1):
    - Flutter
    - FlutterMacOS
  - patrol (0.0.1):
    - CocoaAsyncSocket (~> 7.6)
    - Flutter
    - FlutterMacOS
  - PromisesObjC (2.4.0)
  - video_player_avfoundation (0.0.1):
    - Flutter
    - FlutterMacOS

DEPENDENCIES:
  - better_open_file (from `.symlinks/plugins/better_open_file/ios`)
  - camerawesome (from `.symlinks/plugins/camerawesome/ios`)
  - Flutter (from `Flutter`)
  - google_mlkit_barcode_scanning (from `.symlinks/plugins/google_mlkit_barcode_scanning/ios`)
  - google_mlkit_commons (from `.symlinks/plugins/google_mlkit_commons/ios`)
  - google_mlkit_face_detection (from `.symlinks/plugins/google_mlkit_face_detection/ios`)
  - google_mlkit_text_recognition (from `.symlinks/plugins/google_mlkit_text_recognition/ios`)
  - path_provider_foundation (from `.symlinks/plugins/path_provider_foundation/darwin`)
  - patrol (from `.symlinks/plugins/patrol/darwin`)
  - video_player_avfoundation (from `.symlinks/plugins/video_player_avfoundation/darwin`)

SPEC REPOS:
  trunk:
    - CocoaAsyncSocket
    - GoogleDataTransport
    - GoogleMLKit
    - GoogleToolboxForMac
    - GoogleUtilities
    - GTMSessionFetcher
    - MLImage
    - MLKitBarcodeScanning
    - MLKitCommon
    - MLKitFaceDetection
    - MLKitTextRecognition
    - MLKitTextRecognitionCommon
    - MLKitVision
    - nanopb
    - PromisesObjC

EXTERNAL SOURCES:
  better_open_file:
    :path: ".symlinks/plugins/better_open_file/ios"
  camerawesome:
    :path: ".symlinks/plugins/camerawesome/ios"
  Flutter:
    :path: Flutter
  google_mlkit_barcode_scanning:
    :path: ".symlinks/plugins/google_mlkit_barcode_scanning/ios"
  google_mlkit_commons:
    :path: ".symlinks/plugins/google_mlkit_commons/ios"
  google_mlkit_face_detection:
    :path: ".symlinks/plugins/google_mlkit_face_detection/ios"
  google_mlkit_text_recognition:
    :path: ".symlinks/plugins/google_mlkit_text_recognition/ios"
  path_provider_foundation:
    :path: ".symlinks/plugins/path_provider_foundation/darwin"
  patrol:
    :path: ".symlinks/plugins/patrol/darwin"
  video_player_avfoundation:
    :path: ".symlinks/plugins/video_player_avfoundation/darwin"

SPEC CHECKSUMS:
  better_open_file: 1b4512d5c323999178c1cfe6427352cd582f351b
  camerawesome: a961fa32dafc00d2f093d824311c84f849586b58
  CocoaAsyncSocket: 065fd1e645c7abab64f7a6a2007a48038fdc6a99
  Flutter: e0871f40cf51350855a761d2e70bf5af5b9b5de7
  google_mlkit_barcode_scanning: 8f5987f244a43fe1167689c548342a5174108159
  google_mlkit_commons: 2abe6a70e1824e431d16a51085cb475b672c8aab
  google_mlkit_face_detection: 754da2113a1952f063c7c5dc347ac6ae8934fb77
  google_mlkit_text_recognition: ec2122ec89bfe0d7200763336a6e4ef44810674c
  GoogleDataTransport: aae35b7ea0c09004c3797d53c8c41f66f219d6a7
  GoogleMLKit: eff9e23ec1d90ea4157a1ee2e32a4f610c5b3318
  GoogleToolboxForMac: d1a2cbf009c453f4d6ded37c105e2f67a32206d8
  GoogleUtilities: 26a3abef001b6533cf678d3eb38fd3f614b7872d
  GTMSessionFetcher: 5aea5ba6bd522a239e236100971f10cb71b96ab6
  MLImage: 0ad1c5f50edd027672d8b26b0fee78a8b4a0fc56
  MLKitBarcodeScanning: 0a3064da0a7f49ac24ceb3cb46a5bc67496facd2
  MLKitCommon: 07c2c33ae5640e5380beaaa6e4b9c249a205542d
  MLKitFaceDetection: 2a593db4837db503ad3426b565e7aab045cefea5
  MLKitTextRecognition: 3b41f3ff084a79afb214408d25d2068d77ab322c
  MLKitTextRecognitionCommon: cd44577a8c506fc6bba065096de03bec0d01a213
  MLKitVision: 45e79d68845a2de77e2dd4d7f07947f0ed157b0e
  nanopb: fad817b59e0457d11a5dfbde799381cd727c1275
  path_provider_foundation: 080d55be775b7414fd5a5ef3ac137b97b097e564
  patrol: dd82ffedfee3aba87c1d0ed2daad0b77bfb8ee1f
  PromisesObjC: f5707f49cb48b9636751c5b2e7d227e43fba9f47
  video_player_avfoundation: 2cef49524dd1f16c5300b9cd6efd9611ce03639b

PODFILE CHECKSUM: dabefdfdd2f64e884a9feb5996b51b7bfc7404d3

COCOAPODS: 1.16.2



================================================
FILE: example/ios/.gitignore
================================================
**/dgph
*.mode1v3
*.mode2v3
*.moved-aside
*.pbxuser
*.perspectivev3
**/*sync/
.sconsign.dblite
.tags*
**/.vagrant/
**/DerivedData/
Icon?
**/Pods/
**/.symlinks/
profile
xcuserdata
**/.generated/
Flutter/App.framework
Flutter/Flutter.framework
Flutter/Flutter.podspec
Flutter/Generated.xcconfig
Flutter/ephemeral/
Flutter/app.flx
Flutter/app.zip
Flutter/flutter_assets/
Flutter/flutter_export_environment.sh
ServiceDefinitions.json
Runner/GeneratedPluginRegistrant.*

# Exceptions to above rules.
!default.mode1v3
!default.mode2v3
!default.pbxuser
!default.perspectivev3



================================================
FILE: example/ios/Flutter/AppFrameworkInfo.plist
================================================
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
  <key>CFBundleDevelopmentRegion</key>
  <string>en</string>
  <key>CFBundleExecutable</key>
  <string>App</string>
  <key>CFBundleIdentifier</key>
  <string>io.flutter.flutter.app</string>
  <key>CFBundleInfoDictionaryVersion</key>
  <string>6.0</string>
  <key>CFBundleName</key>
  <string>App</string>
  <key>CFBundlePackageType</key>
  <string>FMWK</string>
  <key>CFBundleShortVersionString</key>
  <string>1.0</string>
  <key>CFBundleSignature</key>
  <string>????</string>
  <key>CFBundleVersion</key>
  <string>1.0</string>
  <key>MinimumOSVersion</key>
  <string>12.0</string>
</dict>
</plist>



================================================
FILE: example/ios/Flutter/Debug.xcconfig
================================================
#include? "Pods/Target Support Files/Pods-Runner/Pods-Runner.debug.xcconfig"
#include "Generated.xcconfig"



================================================
FILE: example/ios/Flutter/Release.xcconfig
================================================
#include? "Pods/Target Support Files/Pods-Runner/Pods-Runner.release.xcconfig"
#include "Generated.xcconfig"



================================================
FILE: example/ios/Runner/AppDelegate.swift
================================================
import UIKit
import Flutter

@main
@objc class AppDelegate: FlutterAppDelegate {
  override func application(
    _ application: UIApplication,
    didFinishLaunchingWithOptions launchOptions: [UIApplication.LaunchOptionsKey: Any]?
  ) -> Bool {
    GeneratedPluginRegistrant.register(with: self)
    return super.application(application, didFinishLaunchingWithOptions: launchOptions)
  }
}



================================================
FILE: example/ios/Runner/Info.plist
================================================
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
	<key>CADisableMinimumFrameDurationOnPhone</key>
	<true/>
	<key>CFBundleDevelopmentRegion</key>
	<string>$(DEVELOPMENT_LANGUAGE)</string>
	<key>CFBundleDisplayName</key>
	<string>camerAwesome</string>
	<key>CFBundleExecutable</key>
	<string>$(EXECUTABLE_NAME)</string>
	<key>CFBundleIdentifier</key>
	<string>$(PRODUCT_BUNDLE_IDENTIFIER)</string>
	<key>CFBundleInfoDictionaryVersion</key>
	<string>6.0</string>
	<key>CFBundleName</key>
	<string>camera_app</string>
	<key>CFBundlePackageType</key>
	<string>APPL</string>
	<key>CFBundleShortVersionString</key>
	<string>$(FLUTTER_BUILD_NAME)</string>
	<key>CFBundleSignature</key>
	<string>????</string>
	<key>CFBundleVersion</key>
	<string>$(FLUTTER_BUILD_NUMBER)</string>
	<key>LSRequiresIPhoneOS</key>
	<true/>
	<key>NSBonjourServices</key>
	<array>
		<string>_dartobservatory._tcp</string>
	</array>
	<key>NSCameraUsageDescription</key>
	<string>Your own description</string>
	<key>NSLocalNetworkUsageDescription</key>
	<string>Looking for local tcp Bonjour service</string>
	<key>NSLocationWhenInUseUsageDescription</key>
	<string>To enable GPS location access for Exif data</string>
	<key>NSMicrophoneUsageDescription</key>
	<string>To enable microphone access when recording video</string>
	<key>UIApplicationSupportsIndirectInputEvents</key>
	<true/>
	<key>UILaunchStoryboardName</key>
	<string>LaunchScreen</string>
	<key>UIMainStoryboardFile</key>
	<string>Main</string>
	<key>UIRequiresFullScreen</key>
	<true/>
	<key>UISupportedInterfaceOrientations</key>
	<array>
		<string>UIInterfaceOrientationPortrait</string>
	</array>
	<key>UISupportedInterfaceOrientations~ipad</key>
	<array>
		<string>UIInterfaceOrientationLandscapeLeft</string>
		<string>UIInterfaceOrientationLandscapeRight</string>
		<string>UIInterfaceOrientationPortrait</string>
		<string>UIInterfaceOrientationPortraitUpsideDown</string>
	</array>
	<key>UIViewControllerBasedStatusBarAppearance</key>
	<false/>
</dict>
</plist>



================================================
FILE: example/ios/Runner/Runner-Bridging-Header.h
================================================
#import "GeneratedPluginRegistrant.h"



================================================
FILE: example/ios/Runner/Assets.xcassets/AppIcon.appiconset/Contents.json
================================================
{
  "images" : [
    {
      "size" : "20x20",
      "idiom" : "iphone",
      "filename" : "Icon-App-20x20@2x.png",
      "scale" : "2x"
    },
    {
      "size" : "20x20",
      "idiom" : "iphone",
      "filename" : "Icon-App-20x20@3x.png",
      "scale" : "3x"
    },
    {
      "size" : "29x29",
      "idiom" : "iphone",
      "filename" : "Icon-App-29x29@1x.png",
      "scale" : "1x"
    },
    {
      "size" : "29x29",
      "idiom" : "iphone",
      "filename" : "Icon-App-29x29@2x.png",
      "scale" : "2x"
    },
    {
      "size" : "29x29",
      "idiom" : "iphone",
      "filename" : "Icon-App-29x29@3x.png",
      "scale" : "3x"
    },
    {
      "size" : "40x40",
      "idiom" : "iphone",
      "filename" : "Icon-App-40x40@2x.png",
      "scale" : "2x"
    },
    {
      "size" : "40x40",
      "idiom" : "iphone",
      "filename" : "Icon-App-40x40@3x.png",
      "scale" : "3x"
    },
    {
      "size" : "60x60",
      "idiom" : "iphone",
      "filename" : "Icon-App-60x60@2x.png",
      "scale" : "2x"
    },
    {
      "size" : "60x60",
      "idiom" : "iphone",
      "filename" : "Icon-App-60x60@3x.png",
      "scale" : "3x"
    },
    {
      "size" : "20x20",
      "idiom" : "ipad",
      "filename" : "Icon-App-20x20@1x.png",
      "scale" : "1x"
    },
    {
      "size" : "20x20",
      "idiom" : "ipad",
      "filename" : "Icon-App-20x20@2x.png",
      "scale" : "2x"
    },
    {
      "size" : "29x29",
      "idiom" : "ipad",
      "filename" : "Icon-App-29x29@1x.png",
      "scale" : "1x"
    },
    {
      "size" : "29x29",
      "idiom" : "ipad",
      "filename" : "Icon-App-29x29@2x.png",
      "scale" : "2x"
    },
    {
      "size" : "40x40",
      "idiom" : "ipad",
      "filename" : "Icon-App-40x40@1x.png",
      "scale" : "1x"
    },
    {
      "size" : "40x40",
      "idiom" : "ipad",
      "filename" : "Icon-App-40x40@2x.png",
      "scale" : "2x"
    },
    {
      "size" : "76x76",
      "idiom" : "ipad",
      "filename" : "Icon-App-76x76@1x.png",
      "scale" : "1x"
    },
    {
      "size" : "76x76",
      "idiom" : "ipad",
      "filename" : "Icon-App-76x76@2x.png",
      "scale" : "2x"
    },
    {
      "size" : "83.5x83.5",
      "idiom" : "ipad",
      "filename" : "Icon-App-83.5x83.5@2x.png",
      "scale" : "2x"
    },
    {
      "size" : "1024x1024",
      "idiom" : "ios-marketing",
      "filename" : "Icon-App-1024x1024@1x.png",
      "scale" : "1x"
    }
  ],
  "info" : {
    "version" : 1,
    "author" : "xcode"
  }
}



================================================
FILE: example/ios/Runner/Assets.xcassets/LaunchImage.imageset/README.md
================================================
# Launch Screen Assets

You can customize the launch screen with your own desired assets by replacing the image files in this directory.

You can also do it by opening your Flutter project's Xcode project with `open ios/Runner.xcworkspace`, selecting `Runner/Assets.xcassets` in the Project Navigator and dropping in the desired images.


================================================
FILE: example/ios/Runner/Assets.xcassets/LaunchImage.imageset/Contents.json
================================================
{
  "images" : [
    {
      "idiom" : "universal",
      "filename" : "LaunchImage.png",
      "scale" : "1x"
    },
    {
      "idiom" : "universal",
      "filename" : "LaunchImage@2x.png",
      "scale" : "2x"
    },
    {
      "idiom" : "universal",
      "filename" : "LaunchImage@3x.png",
      "scale" : "3x"
    }
  ],
  "info" : {
    "version" : 1,
    "author" : "xcode"
  }
}



================================================
FILE: example/ios/Runner/Base.lproj/LaunchScreen.storyboard
================================================
<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<document type="com.apple.InterfaceBuilder3.CocoaTouch.Storyboard.XIB" version="3.0" toolsVersion="12121" systemVersion="16G29" targetRuntime="iOS.CocoaTouch" propertyAccessControl="none" useAutolayout="YES" launchScreen="YES" colorMatched="YES" initialViewController="01J-lp-oVM">
    <dependencies>
        <deployment identifier="iOS"/>
        <plugIn identifier="com.apple.InterfaceBuilder.IBCocoaTouchPlugin" version="12089"/>
    </dependencies>
    <scenes>
        <!--View Controller-->
        <scene sceneID="EHf-IW-A2E">
            <objects>
                <viewController id="01J-lp-oVM" sceneMemberID="viewController">
                    <layoutGuides>
                        <viewControllerLayoutGuide type="top" id="Ydg-fD-yQy"/>
                        <viewControllerLayoutGuide type="bottom" id="xbc-2k-c8Z"/>
                    </layoutGuides>
                    <view key="view" contentMode="scaleToFill" id="Ze5-6b-2t3">
                        <autoresizingMask key="autoresizingMask" widthSizable="YES" heightSizable="YES"/>
                        <subviews>
                            <imageView opaque="NO" clipsSubviews="YES" multipleTouchEnabled="YES" contentMode="center" image="LaunchImage" translatesAutoresizingMaskIntoConstraints="NO" id="YRO-k0-Ey4">
                            </imageView>
                        </subviews>
                        <color key="backgroundColor" red="1" green="1" blue="1" alpha="1" colorSpace="custom" customColorSpace="sRGB"/>
                        <constraints>
                            <constraint firstItem="YRO-k0-Ey4" firstAttribute="centerX" secondItem="Ze5-6b-2t3" secondAttribute="centerX" id="1a2-6s-vTC"/>
                            <constraint firstItem="YRO-k0-Ey4" firstAttribute="centerY" secondItem="Ze5-6b-2t3" secondAttribute="centerY" id="4X2-HB-R7a"/>
                        </constraints>
                    </view>
                </viewController>
                <placeholder placeholderIdentifier="IBFirstResponder" id="iYj-Kq-Ea1" userLabel="First Responder" sceneMemberID="firstResponder"/>
            </objects>
            <point key="canvasLocation" x="53" y="375"/>
        </scene>
    </scenes>
    <resources>
        <image name="LaunchImage" width="168" height="185"/>
    </resources>
</document>



================================================
FILE: example/ios/Runner/Base.lproj/Main.storyboard
================================================
<?xml version="1.0" encoding="UTF-8"?>
<document type="com.apple.InterfaceBuilder3.CocoaTouch.Storyboard.XIB" version="3.0" toolsVersion="21225" targetRuntime="iOS.CocoaTouch" propertyAccessControl="none" useAutolayout="YES" useTraitCollections="YES" colorMatched="YES" initialViewController="BYZ-38-t0r">
    <device id="retina6_0" orientation="portrait" appearance="light"/>
    <dependencies>
        <deployment identifier="iOS"/>
        <plugIn identifier="com.apple.InterfaceBuilder.IBCocoaTouchPlugin" version="21207"/>
        <capability name="documents saved in the Xcode 8 format" minToolsVersion="8.0"/>
    </dependencies>
    <scenes>
        <!--Flutter View Controller-->
        <scene sceneID="tne-QT-ifu">
            <objects>
                <viewController id="BYZ-38-t0r" customClass="FlutterViewController" sceneMemberID="viewController">
                    <layoutGuides>
                        <viewControllerLayoutGuide type="top" id="y3c-jy-aDJ"/>
                        <viewControllerLayoutGuide type="bottom" id="wfy-db-euE"/>
                    </layoutGuides>
                    <view key="view" contentMode="scaleToFill" id="8bC-Xf-vdC">
                        <rect key="frame" x="0.0" y="0.0" width="390" height="844"/>
                        <autoresizingMask key="autoresizingMask" widthSizable="YES" heightSizable="YES"/>
                        <color key="backgroundColor" red="1" green="1" blue="1" alpha="1" colorSpace="custom" customColorSpace="sRGB"/>
                    </view>
                </viewController>
                <placeholder placeholderIdentifier="IBFirstResponder" id="dkx-z0-nzr" sceneMemberID="firstResponder"/>
            </objects>
            <point key="canvasLocation" x="-16" y="-40"/>
        </scene>
    </scenes>
</document>



================================================
FILE: example/ios/Runner.xcodeproj/project.pbxproj
================================================
// !$*UTF8*$!
{
	archiveVersion = 1;
	classes = {
	};
	objectVersion = 54;
	objects = {

/* Begin PBXBuildFile section */
		1498D2341E8E89220040F4C2 /* GeneratedPluginRegistrant.m in Sources */ = {isa = PBXBuildFile; fileRef = 1498D2331E8E89220040F4C2 /* GeneratedPluginRegistrant.m */; };
		209237308B8AC0367063F441 /* Pods_Runner.framework in Frameworks */ = {isa = PBXBuildFile; fileRef = C767E01B8C9409276081D1C0 /* Pods_Runner.framework */; };
		3B3967161E833CAA004F5970 /* AppFrameworkInfo.plist in Resources */ = {isa = PBXBuildFile; fileRef = 3B3967151E833CAA004F5970 /* AppFrameworkInfo.plist */; };
		5C7D455B2951A544006F3F20 /* RunnerTests.m in Sources */ = {isa = PBXBuildFile; fileRef = 5C7D455A2951A544006F3F20 /* RunnerTests.m */; };
		74858FAF1ED2DC5600515810 /* AppDelegate.swift in Sources */ = {isa = PBXBuildFile; fileRef = 74858FAE1ED2DC5600515810 /* AppDelegate.swift */; };
		78A318202AECB46A00862997 /* FlutterGeneratedPluginSwiftPackage in Frameworks */ = {isa = PBXBuildFile; productRef = 78A3181F2AECB46A00862997 /* FlutterGeneratedPluginSwiftPackage */; };
		97C146FC1CF9000F007C117D /* Main.storyboard in Resources */ = {isa = PBXBuildFile; fileRef = 97C146FA1CF9000F007C117D /* Main.storyboard */; };
		97C146FE1CF9000F007C117D /* Assets.xcassets in Resources */ = {isa = PBXBuildFile; fileRef = 97C146FD1CF9000F007C117D /* Assets.xcassets */; };
		97C147011CF9000F007C117D /* LaunchScreen.storyboard in Resources */ = {isa = PBXBuildFile; fileRef = 97C146FF1CF9000F007C117D /* LaunchScreen.storyboard */; };
		F37CA266AD26762B0AF249B9 /* Pods_Runner_RunnerUITests.framework in Frameworks */ = {isa = PBXBuildFile; fileRef = F8A12429A32B0D9BDE97BD7C /* Pods_Runner_RunnerUITests.framework */; };
/* End PBXBuildFile section */

/* Begin PBXContainerItemProxy section */
		5C7D45542951A480006F3F20 /* PBXContainerItemProxy */ = {
			isa = PBXContainerItemProxy;
			containerPortal = 97C146E61CF9000F007C117D /* Project object */;
			proxyType = 1;
			remoteGlobalIDString = 97C146ED1CF9000F007C117D;
			remoteInfo = Runner;
		};
/* End PBXContainerItemProxy section */

/* Begin PBXCopyFilesBuildPhase section */
		9705A1C41CF9048500538489 /* Embed Frameworks */ = {
			isa = PBXCopyFilesBuildPhase;
			buildActionMask = 2147483647;
			dstPath = "";
			dstSubfolderSpec = 10;
			files = (
			);
			name = "Embed Frameworks";
			runOnlyForDeploymentPostprocessing = 0;
		};
/* End PBXCopyFilesBuildPhase section */

/* Begin PBXFileReference section */
		1498D2321E8E86230040F4C2 /* GeneratedPluginRegistrant.h */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.c.h; path = GeneratedPluginRegistrant.h; sourceTree = "<group>"; };
		1498D2331E8E89220040F4C2 /* GeneratedPluginRegistrant.m */ = {isa = PBXFileReference; fileEncoding = 4; lastKnownFileType = sourcecode.c.objc; path = GeneratedPluginRegistrant.m; sourceTree = "<group>"; };
		3B3967151E833CAA004F5970 /* AppFrameworkInfo.plist */ = {isa = PBXFileReference; fileEncoding = 4; lastKnownFileType = text.plist.xml; name = AppFrameworkInfo.plist; path = Flutter/AppFrameworkInfo.plist; sourceTree = "<group>"; };
		59177DE91E27166036D3D28D /* Pods-Runner.debug.xcconfig */ = {isa = PBXFileReference; includeInIndex = 1; lastKnownFileType = text.xcconfig; name = "Pods-Runner.debug.xcconfig"; path = "Target Support Files/Pods-Runner/Pods-Runner.debug.xcconfig"; sourceTree = "<group>"; };
		5C7D454E2951A480006F3F20 /* RunnerUITests.xctest */ = {isa = PBXFileReference; explicitFileType = wrapper.cfbundle; includeInIndex = 0; path = RunnerUITests.xctest; sourceTree = BUILT_PRODUCTS_DIR; };
		5C7D455A2951A544006F3F20 /* RunnerTests.m */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.c.objc; path = RunnerTests.m; sourceTree = "<group>"; };
		74858FAD1ED2DC5600515810 /* Runner-Bridging-Header.h */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.c.h; path = "Runner-Bridging-Header.h"; sourceTree = "<group>"; };
		74858FAE1ED2DC5600515810 /* AppDelegate.swift */ = {isa = PBXFileReference; fileEncoding = 4; lastKnownFileType = sourcecode.swift; path = AppDelegate.swift; sourceTree = "<group>"; };
		7AFA3C8E1D35360C0083082E /* Release.xcconfig */ = {isa = PBXFileReference; lastKnownFileType = text.xcconfig; name = Release.xcconfig; path = Flutter/Release.xcconfig; sourceTree = "<group>"; };
		9675421EDA6BAE5FEC1E968C /* Pods-Runner.profile.xcconfig */ = {isa = PBXFileReference; includeInIndex = 1; lastKnownFileType = text.xcconfig; name = "Pods-Runner.profile.xcconfig"; path = "Target Support Files/Pods-Runner/Pods-Runner.profile.xcconfig"; sourceTree = "<group>"; };
		9740EEB21CF90195004384FC /* Debug.xcconfig */ = {isa = PBXFileReference; fileEncoding = 4; lastKnownFileType = text.xcconfig; name = Debug.xcconfig; path = Flutter/Debug.xcconfig; sourceTree = "<group>"; };
		9740EEB31CF90195004384FC /* Generated.xcconfig */ = {isa = PBXFileReference; fileEncoding = 4; lastKnownFileType = text.xcconfig; name = Generated.xcconfig; path = Flutter/Generated.xcconfig; sourceTree = "<group>"; };
		97C146EE1CF9000F007C117D /* Runner.app */ = {isa = PBXFileReference; explicitFileType = wrapper.application; includeInIndex = 0; path = Runner.app; sourceTree = BUILT_PRODUCTS_DIR; };
		97C146FB1CF9000F007C117D /* Base */ = {isa = PBXFileReference; lastKnownFileType = file.storyboard; name = Base; path = Base.lproj/Main.storyboard; sourceTree = "<group>"; };
		97C146FD1CF9000F007C117D /* Assets.xcassets */ = {isa = PBXFileReference; lastKnownFileType = folder.assetcatalog; path = Assets.xcassets; sourceTree = "<group>"; };
		97C147001CF9000F007C117D /* Base */ = {isa = PBXFileReference; lastKnownFileType = file.storyboard; name = Base; path = Base.lproj/LaunchScreen.storyboard; sourceTree = "<group>"; };
		97C147021CF9000F007C117D /* Info.plist */ = {isa = PBXFileReference; lastKnownFileType = text.plist.xml; path = Info.plist; sourceTree = "<group>"; };
		A47A46130D85EFFE9CF95EFD /* Pods-Runner.release.xcconfig */ = {isa = PBXFileReference; includeInIndex = 1; lastKnownFileType = text.xcconfig; name = "Pods-Runner.release.xcconfig"; path = "Target Support Files/Pods-Runner/Pods-Runner.release.xcconfig"; sourceTree = "<group>"; };
		AC2906DB4786F63B1A8115B5 /* Pods-Runner-RunnerUITests.debug.xcconfig */ = {isa = PBXFileReference; includeInIndex = 1; lastKnownFileType = text.xcconfig; name = "Pods-Runner-RunnerUITests.debug.xcconfig"; path = "Target Support Files/Pods-Runner-RunnerUITests/Pods-Runner-RunnerUITests.debug.xcconfig"; sourceTree = "<group>"; };
		C767E01B8C9409276081D1C0 /* Pods_Runner.framework */ = {isa = PBXFileReference; explicitFileType = wrapper.framework; includeInIndex = 0; path = Pods_Runner.framework; sourceTree = BUILT_PRODUCTS_DIR; };
		E0E15AFA9A84772E42C752B5 /* Pods-Runner-RunnerUITests.release.xcconfig */ = {isa = PBXFileReference; includeInIndex = 1; lastKnownFileType = text.xcconfig; name = "Pods-Runner-RunnerUITests.release.xcconfig"; path = "Target Support Files/Pods-Runner-RunnerUITests/Pods-Runner-RunnerUITests.release.xcconfig"; sourceTree = "<group>"; };
		EDA1D42BFE61E5D3D276935E /* Pods-Runner-RunnerUITests.profile.xcconfig */ = {isa = PBXFileReference; includeInIndex = 1; lastKnownFileType = text.xcconfig; name = "Pods-Runner-RunnerUITests.profile.xcconfig"; path = "Target Support Files/Pods-Runner-RunnerUITests/Pods-Runner-RunnerUITests.profile.xcconfig"; sourceTree = "<group>"; };
		F8A12429A32B0D9BDE97BD7C /* Pods_Runner_RunnerUITests.framework */ = {isa = PBXFileReference; explicitFileType = wrapper.framework; includeInIndex = 0; path = Pods_Runner_RunnerUITests.framework; sourceTree = BUILT_PRODUCTS_DIR; };
/* End PBXFileReference section */

/* Begin PBXFrameworksBuildPhase section */
		5C7D454B2951A480006F3F20 /* Frameworks */ = {
			isa = PBXFrameworksBuildPhase;
			buildActionMask = 2147483647;
			files = (
				F37CA266AD26762B0AF249B9 /* Pods_Runner_RunnerUITests.framework in Frameworks */,
			);
			runOnlyForDeploymentPostprocessing = 0;
		};
		97C146EB1CF9000F007C117D /* Frameworks */ = {
			isa = PBXFrameworksBuildPhase;
			buildActionMask = 2147483647;
			files = (
				78A318202AECB46A00862997 /* FlutterGeneratedPluginSwiftPackage in Frameworks */,
				209237308B8AC0367063F441 /* Pods_Runner.framework in Frameworks */,
			);
			runOnlyForDeploymentPostprocessing = 0;
		};
/* End PBXFrameworksBuildPhase section */

/* Begin PBXGroup section */
		4AD115DFAA049187EAF87D3E /* Pods */ = {
			isa = PBXGroup;
			children = (
				59177DE91E27166036D3D28D /* Pods-Runner.debug.xcconfig */,
				A47A46130D85EFFE9CF95EFD /* Pods-Runner.release.xcconfig */,
				9675421EDA6BAE5FEC1E968C /* Pods-Runner.profile.xcconfig */,
				AC2906DB4786F63B1A8115B5 /* Pods-Runner-RunnerUITests.debug.xcconfig */,
				E0E15AFA9A84772E42C752B5 /* Pods-Runner-RunnerUITests.release.xcconfig */,
				EDA1D42BFE61E5D3D276935E /* Pods-Runner-RunnerUITests.profile.xcconfig */,
			);
			path = Pods;
			sourceTree = "<group>";
		};
		5C7D454F2951A480006F3F20 /* RunnerUITests */ = {
			isa = PBXGroup;
			children = (
				5C7D455A2951A544006F3F20 /* RunnerTests.m */,
			);
			path = RunnerUITests;
			sourceTree = "<group>";
		};
		9740EEB11CF90186004384FC /* Flutter */ = {
			isa = PBXGroup;
			children = (
				3B3967151E833CAA004F5970 /* AppFrameworkInfo.plist */,
				9740EEB21CF90195004384FC /* Debug.xcconfig */,
				7AFA3C8E1D35360C0083082E /* Release.xcconfig */,
				9740EEB31CF90195004384FC /* Generated.xcconfig */,
			);
			name = Flutter;
			sourceTree = "<group>";
		};
		97C146E51CF9000F007C117D = {
			isa = PBXGroup;
			children = (
				9740EEB11CF90186004384FC /* Flutter */,
				97C146F01CF9000F007C117D /* Runner */,
				5C7D454F2951A480006F3F20 /* RunnerUITests */,
				97C146EF1CF9000F007C117D /* Products */,
				4AD115DFAA049187EAF87D3E /* Pods */,
				BCA7FC006B29DF8CCCD7A3DA /* Frameworks */,
			);
			sourceTree = "<group>";
		};
		97C146EF1CF9000F007C117D /* Products */ = {
			isa = PBXGroup;
			children = (
				97C146EE1CF9000F007C117D /* Runner.app */,
				5C7D454E2951A480006F3F20 /* RunnerUITests.xctest */,
			);
			name = Products;
			sourceTree = "<group>";
		};
		97C146F01CF9000F007C117D /* Runner */ = {
			isa = PBXGroup;
			children = (
				97C146FA1CF9000F007C117D /* Main.storyboard */,
				97C146FD1CF9000F007C117D /* Assets.xcassets */,
				97C146FF1CF9000F007C117D /* LaunchScreen.storyboard */,
				97C147021CF9000F007C117D /* Info.plist */,
				1498D2321E8E86230040F4C2 /* GeneratedPluginRegistrant.h */,
				1498D2331E8E89220040F4C2 /* GeneratedPluginRegistrant.m */,
				74858FAE1ED2DC5600515810 /* AppDelegate.swift */,
				74858FAD1ED2DC5600515810 /* Runner-Bridging-Header.h */,
			);
			path = Runner;
			sourceTree = "<group>";
		};
		BCA7FC006B29DF8CCCD7A3DA /* Frameworks */ = {
			isa = PBXGroup;
			children = (
				C767E01B8C9409276081D1C0 /* Pods_Runner.framework */,
				F8A12429A32B0D9BDE97BD7C /* Pods_Runner_RunnerUITests.framework */,
			);
			name = Frameworks;
			sourceTree = "<group>";
		};
/* End PBXGroup section */

/* Begin PBXNativeTarget section */
		5C7D454D2951A480006F3F20 /* RunnerUITests */ = {
			isa = PBXNativeTarget;
			buildConfigurationList = 5C7D45562951A480006F3F20 /* Build configuration list for PBXNativeTarget "RunnerUITests" */;
			buildPhases = (
				19087E07C8B06A9E8C2D3FD2 /* [CP] Check Pods Manifest.lock */,
				5C7D454A2951A480006F3F20 /* Sources */,
				5C7D454B2951A480006F3F20 /* Frameworks */,
				5C7D454C2951A480006F3F20 /* Resources */,
				29F4848E84F8DE84C087989D /* [CP] Embed Pods Frameworks */,
				360771E8AB01B37DE9C285E8 /* [CP] Copy Pods Resources */,
			);
			buildRules = (
			);
			dependencies = (
				5C7D45552951A480006F3F20 /* PBXTargetDependency */,
			);
			name = RunnerUITests;
			productName = RunnerUITests;
			productReference = 5C7D454E2951A480006F3F20 /* RunnerUITests.xctest */;
			productType = "com.apple.product-type.bundle.ui-testing";
		};
		97C146ED1CF9000F007C117D /* Runner */ = {
			isa = PBXNativeTarget;
			buildConfigurationList = 97C147051CF9000F007C117D /* Build configuration list for PBXNativeTarget "Runner" */;
			buildPhases = (
				1ED409C74C37451465008A86 /* [CP] Check Pods Manifest.lock */,
				9740EEB61CF901F6004384FC /* Run Script */,
				97C146EA1CF9000F007C117D /* Sources */,
				97C146EB1CF9000F007C117D /* Frameworks */,
				97C146EC1CF9000F007C117D /* Resources */,
				9705A1C41CF9048500538489 /* Embed Frameworks */,
				3B06AD1E1E4923F5004D2608 /* Thin Binary */,
				70474C872543028706A14462 /* [CP] Embed Pods Frameworks */,
				C0474859B9E7EB1EC0503A0E /* [CP] Copy Pods Resources */,
			);
			buildRules = (
			);
			dependencies = (
			);
			name = Runner;
			packageProductDependencies = (
				78A3181F2AECB46A00862997 /* FlutterGeneratedPluginSwiftPackage */,
			);
			productName = Runner;
			productReference = 97C146EE1CF9000F007C117D /* Runner.app */;
			productType = "com.apple.product-type.application";
		};
/* End PBXNativeTarget section */

/* Begin PBXProject section */
		97C146E61CF9000F007C117D /* Project object */ = {
			isa = PBXProject;
			attributes = {
				LastUpgradeCheck = 1510;
				ORGANIZATIONNAME = "";
				TargetAttributes = {
					5C7D454D2951A480006F3F20 = {
						CreatedOnToolsVersion = 14.0;
						TestTargetID = 97C146ED1CF9000F007C117D;
					};
					97C146ED1CF9000F007C117D = {
						CreatedOnToolsVersion = 7.3.1;
						LastSwiftMigration = 1100;
					};
				};
			};
			buildConfigurationList = 97C146E91CF9000F007C117D /* Build configuration list for PBXProject "Runner" */;
			compatibilityVersion = "Xcode 9.3";
			developmentRegion = en;
			hasScannedForEncodings = 0;
			knownRegions = (
				en,
				Base,
			);
			mainGroup = 97C146E51CF9000F007C117D;
			packageReferences = (
				781AD8BC2B33823900A9FFBB /* XCLocalSwiftPackageReference "Flutter/ephemeral/Packages/FlutterGeneratedPluginSwiftPackage" */,
			);
			productRefGroup = 97C146EF1CF9000F007C117D /* Products */;
			projectDirPath = "";
			projectRoot = "";
			targets = (
				97C146ED1CF9000F007C117D /* Runner */,
				5C7D454D2951A480006F3F20 /* RunnerUITests */,
			);
		};
/* End PBXProject section */

/* Begin PBXResourcesBuildPhase section */
		5C7D454C2951A480006F3F20 /* Resources */ = {
			isa = PBXResourcesBuildPhase;
			buildActionMask = 2147483647;
			files = (
			);
			runOnlyForDeploymentPostprocessing = 0;
		};
		97C146EC1CF9000F007C117D /* Resources */ = {
			isa = PBXResourcesBuildPhase;
			buildActionMask = 2147483647;
			files = (
				97C147011CF9000F007C117D /* LaunchScreen.storyboard in Resources */,
				3B3967161E833CAA004F5970 /* AppFrameworkInfo.plist in Resources */,
				97C146FE1CF9000F007C117D /* Assets.xcassets in Resources */,
				97C146FC1CF9000F007C117D /* Main.storyboard in Resources */,
			);
			runOnlyForDeploymentPostprocessing = 0;
		};
/* End PBXResourcesBuildPhase section */

/* Begin PBXShellScriptBuildPhase section */
		19087E07C8B06A9E8C2D3FD2 /* [CP] Check Pods Manifest.lock */ = {
			isa = PBXShellScriptBuildPhase;
			buildActionMask = 2147483647;
			files = (
			);
			inputFileListPaths = (
			);
			inputPaths = (
				"${PODS_PODFILE_DIR_PATH}/Podfile.lock",
				"${PODS_ROOT}/Manifest.lock",
			);
			name = "[CP] Check Pods Manifest.lock";
			outputFileListPaths = (
			);
			outputPaths = (
				"$(DERIVED_FILE_DIR)/Pods-Runner-RunnerUITests-checkManifestLockResult.txt",
			);
			runOnlyForDeploymentPostprocessing = 0;
			shellPath = /bin/sh;
			shellScript = "diff \"${PODS_PODFILE_DIR_PATH}/Podfile.lock\" \"${PODS_ROOT}/Manifest.lock\" > /dev/null\nif [ $? != 0 ] ; then\n    # print error to STDERR\n    echo \"error: The sandbox is not in sync with the Podfile.lock. Run 'pod install' or update your CocoaPods installation.\" >&2\n    exit 1\nfi\n# This output is used by Xcode 'outputs' to avoid re-running this script phase.\necho \"SUCCESS\" > \"${SCRIPT_OUTPUT_FILE_0}\"\n";
			showEnvVarsInLog = 0;
		};
		1ED409C74C37451465008A86 /* [CP] Check Pods Manifest.lock */ = {
			isa = PBXShellScriptBuildPhase;
			buildActionMask = 2147483647;
			files = (
			);
			inputFileListPaths = (
			);
			inputPaths = (
				"${PODS_PODFILE_DIR_PATH}/Podfile.lock",
				"${PODS_ROOT}/Manifest.lock",
			);
			name = "[CP] Check Pods Manifest.lock";
			outputFileListPaths = (
			);
			outputPaths = (
				"$(DERIVED_FILE_DIR)/Pods-Runner-checkManifestLockResult.txt",
			);
			runOnlyForDeploymentPostprocessing = 0;
			shellPath = /bin/sh;
			shellScript = "diff \"${PODS_PODFILE_DIR_PATH}/Podfile.lock\" \"${PODS_ROOT}/Manifest.lock\" > /dev/null\nif [ $? != 0 ] ; then\n    # print error to STDERR\n    echo \"error: The sandbox is not in sync with the Podfile.lock. Run 'pod install' or update your CocoaPods installation.\" >&2\n    exit 1\nfi\n# This output is used by Xcode 'outputs' to avoid re-running this script phase.\necho \"SUCCESS\" > \"${SCRIPT_OUTPUT_FILE_0}\"\n";
			showEnvVarsInLog = 0;
		};
		29F4848E84F8DE84C087989D /* [CP] Embed Pods Frameworks */ = {
			isa = PBXShellScriptBuildPhase;
			buildActionMask = 2147483647;
			files = (
			);
			inputFileListPaths = (
				"${PODS_ROOT}/Target Support Files/Pods-Runner-RunnerUITests/Pods-Runner-RunnerUITests-frameworks-${CONFIGURATION}-input-files.xcfilelist",
			);
			name = "[CP] Embed Pods Frameworks";
			outputFileListPaths = (
				"${PODS_ROOT}/Target Support Files/Pods-Runner-RunnerUITests/Pods-Runner-RunnerUITests-frameworks-${CONFIGURATION}-output-files.xcfilelist",
			);
			runOnlyForDeploymentPostprocessing = 0;
			shellPath = /bin/sh;
			shellScript = "\"${PODS_ROOT}/Target Support Files/Pods-Runner-RunnerUITests/Pods-Runner-RunnerUITests-frameworks.sh\"\n";
			showEnvVarsInLog = 0;
		};
		360771E8AB01B37DE9C285E8 /* [CP] Copy Pods Resources */ = {
			isa = PBXShellScriptBuildPhase;
			buildActionMask = 2147483647;
			files = (
			);
			inputFileListPaths = (
				"${PODS_ROOT}/Target Support Files/Pods-Runner-RunnerUITests/Pods-Runner-RunnerUITests-resources-${CONFIGURATION}-input-files.xcfilelist",
			);
			name = "[CP] Copy Pods Resources";
			outputFileListPaths = (
				"${PODS_ROOT}/Target Support Files/Pods-Runner-RunnerUITests/Pods-Runner-RunnerUITests-resources-${CONFIGURATION}-output-files.xcfilelist",
			);
			runOnlyForDeploymentPostprocessing = 0;
			shellPath = /bin/sh;
			shellScript = "\"${PODS_ROOT}/Target Support Files/Pods-Runner-RunnerUITests/Pods-Runner-RunnerUITests-resources.sh\"\n";
			showEnvVarsInLog = 0;
		};
		3B06AD1E1E4923F5004D2608 /* Thin Binary */ = {
			isa = PBXShellScriptBuildPhase;
			alwaysOutOfDate = 1;
			buildActionMask = 2147483647;
			files = (
			);
			inputPaths = (
				"${TARGET_BUILD_DIR}/${INFOPLIST_PATH}",
			);
			name = "Thin Binary";
			outputPaths = (
			);
			runOnlyForDeploymentPostprocessing = 0;
			shellPath = /bin/sh;
			shellScript = "/bin/sh \"$FLUTTER_ROOT/packages/flutter_tools/bin/xcode_backend.sh\" embed_and_thin";
		};
		70474C872543028706A14462 /* [CP] Embed Pods Frameworks */ = {
			isa = PBXShellScriptBuildPhase;
			buildActionMask = 2147483647;
			files = (
			);
			inputFileListPaths = (
				"${PODS_ROOT}/Target Support Files/Pods-Runner/Pods-Runner-frameworks-${CONFIGURATION}-input-files.xcfilelist",
			);
			name = "[CP] Embed Pods Frameworks";
			outputFileListPaths = (
				"${PODS_ROOT}/Target Support Files/Pods-Runner/Pods-Runner-frameworks-${CONFIGURATION}-output-files.xcfilelist",
			);
			runOnlyForDeploymentPostprocessing = 0;
			shellPath = /bin/sh;
			shellScript = "\"${PODS_ROOT}/Target Support Files/Pods-Runner/Pods-Runner-frameworks.sh\"\n";
			showEnvVarsInLog = 0;
		};
		9740EEB61CF901F6004384FC /* Run Script */ = {
			isa = PBXShellScriptBuildPhase;
			alwaysOutOfDate = 1;
			buildActionMask = 2147483647;
			files = (
			);
			inputPaths = (
			);
			name = "Run Script";
			outputPaths = (
			);
			runOnlyForDeploymentPostprocessing = 0;
			shellPath = /bin/sh;
			shellScript = "/bin/sh \"$FLUTTER_ROOT/packages/flutter_tools/bin/xcode_backend.sh\" build";
		};
		C0474859B9E7EB1EC0503A0E /* [CP] Copy Pods Resources */ = {
			isa = PBXShellScriptBuildPhase;
			buildActionMask = 2147483647;
			files = (
			);
			inputFileListPaths = (
				"${PODS_ROOT}/Target Support Files/Pods-Runner/Pods-Runner-resources-${CONFIGURATION}-input-files.xcfilelist",
			);
			name = "[CP] Copy Pods Resources";
			outputFileListPaths = (
				"${PODS_ROOT}/Target Support Files/Pods-Runner/Pods-Runner-resources-${CONFIGURATION}-output-files.xcfilelist",
			);
			runOnlyForDeploymentPostprocessing = 0;
			shellPath = /bin/sh;
			shellScript = "\"${PODS_ROOT}/Target Support Files/Pods-Runner/Pods-Runner-resources.sh\"\n";
			showEnvVarsInLog = 0;
		};
/* End PBXShellScriptBuildPhase section */

/* Begin PBXSourcesBuildPhase section */
		5C7D454A2951A480006F3F20 /* Sources */ = {
			isa = PBXSourcesBuildPhase;
			buildActionMask = 2147483647;
			files = (
				5C7D455B2951A544006F3F20 /* RunnerTests.m in Sources */,
			);
			runOnlyForDeploymentPostprocessing = 0;
		};
		97C146EA1CF9000F007C117D /* Sources */ = {
			isa = PBXSourcesBuildPhase;
			buildActionMask = 2147483647;
			files = (
				74858FAF1ED2DC5600515810 /* AppDelegate.swift in Sources */,
				1498D2341E8E89220040F4C2 /* GeneratedPluginRegistrant.m in Sources */,
			);
			runOnlyForDeploymentPostprocessing = 0;
		};
/* End PBXSourcesBuildPhase section */

/* Begin PBXTargetDependency section */
		5C7D45552951A480006F3F20 /* PBXTargetDependency */ = {
			isa = PBXTargetDependency;
			target = 97C146ED1CF9000F007C117D /* Runner */;
			targetProxy = 5C7D45542951A480006F3F20 /* PBXContainerItemProxy */;
		};
/* End PBXTargetDependency section */

/* Begin PBXVariantGroup section */
		97C146FA1CF9000F007C117D /* Main.storyboard */ = {
			isa = PBXVariantGroup;
			children = (
				97C146FB1CF9000F007C117D /* Base */,
			);
			name = Main.storyboard;
			sourceTree = "<group>";
		};
		97C146FF1CF9000F007C117D /* LaunchScreen.storyboard */ = {
			isa = PBXVariantGroup;
			children = (
				97C147001CF9000F007C117D /* Base */,
			);
			name = LaunchScreen.storyboard;
			sourceTree = "<group>";
		};
/* End PBXVariantGroup section */

/* Begin XCBuildConfiguration section */
		249021D3217E4FDB00AE95B9 /* Profile */ = {
			isa = XCBuildConfiguration;
			buildSettings = {
				ALWAYS_SEARCH_USER_PATHS = NO;
				CLANG_ANALYZER_NONNULL = YES;
				CLANG_CXX_LANGUAGE_STANDARD = "gnu++0x";
				CLANG_CXX_LIBRARY = "libc++";
				CLANG_ENABLE_MODULES = YES;
				CLANG_ENABLE_OBJC_ARC = YES;
				CLANG_WARN_BLOCK_CAPTURE_AUTORELEASING = YES;
				CLANG_WARN_BOOL_CONVERSION = YES;
				CLANG_WARN_COMMA = YES;
				CLANG_WARN_CONSTANT_CONVERSION = YES;
				CLANG_WARN_DEPRECATED_OBJC_IMPLEMENTATIONS = YES;
				CLANG_WARN_DIRECT_OBJC_ISA_USAGE = YES_ERROR;
				CLANG_WARN_EMPTY_BODY = YES;
				CLANG_WARN_ENUM_CONVERSION = YES;
				CLANG_WARN_INFINITE_RECURSION = YES;
				CLANG_WARN_INT_CONVERSION = YES;
				CLANG_WARN_NON_LITERAL_NULL_CONVERSION = YES;
				CLANG_WARN_OBJC_IMPLICIT_RETAIN_SELF = YES;
				CLANG_WARN_OBJC_LITERAL_CONVERSION = YES;
				CLANG_WARN_OBJC_ROOT_CLASS = YES_ERROR;
				CLANG_WARN_RANGE_LOOP_ANALYSIS = YES;
				CLANG_WARN_STRICT_PROTOTYPES = YES;
				CLANG_WARN_SUSPICIOUS_MOVE = YES;
				CLANG_WARN_UNREACHABLE_CODE = YES;
				CLANG_WARN__DUPLICATE_METHOD_MATCH = YES;
				"CODE_SIGN_IDENTITY[sdk=iphoneos*]" = "iPhone Developer";
				COPY_PHASE_STRIP = NO;
				DEBUG_INFORMATION_FORMAT = "dwarf-with-dsym";
				ENABLE_NS_ASSERTIONS = NO;
				ENABLE_STRICT_OBJC_MSGSEND = YES;
				GCC_C_LANGUAGE_STANDARD = gnu99;
				GCC_NO_COMMON_BLOCKS = YES;
				GCC_WARN_64_TO_32_BIT_CONVERSION = YES;
				GCC_WARN_ABOUT_RETURN_TYPE = YES_ERROR;
				GCC_WARN_UNDECLARED_SELECTOR = YES;
				GCC_WARN_UNINITIALIZED_AUTOS = YES_AGGRESSIVE;
				GCC_WARN_UNUSED_FUNCTION = YES;
				GCC_WARN_UNUSED_VARIABLE = YES;
				IPHONEOS_DEPLOYMENT_TARGET = 12.0;
				MTL_ENABLE_DEBUG_INFO = NO;
				SDKROOT = iphoneos;
				SUPPORTED_PLATFORMS = iphoneos;
				TARGETED_DEVICE_FAMILY = "1,2";
				VALIDATE_PRODUCT = YES;
			};
			name = Profile;
		};
		249021D4217E4FDB00AE95B9 /* Profile */ = {
			isa = XCBuildConfiguration;
			baseConfigurationReference = 7AFA3C8E1D35360C0083082E /* Release.xcconfig */;
			buildSettings = {
				ASSETCATALOG_COMPILER_APPICON_NAME = AppIcon;
				CLANG_ENABLE_MODULES = YES;
				CURRENT_PROJECT_VERSION = "$(FLUTTER_BUILD_NUMBER)";
				DEFINES_MODULE = YES;
				DEVELOPMENT_TEAM = Q8N2T428YG;
				ENABLE_BITCODE = NO;
				INFOPLIST_FILE = Runner/Info.plist;
				LD_RUNPATH_SEARCH_PATHS = (
					"$(inherited)",
					"@executable_path/Frameworks",
				);
				PRODUCT_BUNDLE_IDENTIFIER = com.example.cameraApp;
				PRODUCT_NAME = "$(TARGET_NAME)";
				SWIFT_OBJC_BRIDGING_HEADER = "Runner/Runner-Bridging-Header.h";
				SWIFT_VERSION = 5.0;
				VERSIONING_SYSTEM = "apple-generic";
			};
			name = Profile;
		};
		5C7D45572951A480006F3F20 /* Debug */ = {
			isa = XCBuildConfiguration;
			baseConfigurationReference = AC2906DB4786F63B1A8115B5 /* Pods-Runner-RunnerUITests.debug.xcconfig */;
			buildSettings = {
				ALWAYS_EMBED_SWIFT_STANDARD_LIBRARIES = YES;
				CLANG_ANALYZER_NUMBER_OBJECT_CONVERSION = YES_AGGRESSIVE;
				CLANG_CXX_LANGUAGE_STANDARD = "gnu++20";
				CLANG_ENABLE_OBJC_WEAK = YES;
				CLANG_WARN_DOCUMENTATION_COMMENTS = YES;
				CLANG_WARN_QUOTED_INCLUDE_IN_FRAMEWORK_HEADER = YES;
				CLANG_WARN_UNGUARDED_AVAILABILITY = YES_AGGRESSIVE;
				CODE_SIGN_STYLE = Automatic;
				CURRENT_PROJECT_VERSION = 1;
				DEVELOPMENT_TEAM = Q8N2T428YG;
				GCC_C_LANGUAGE_STANDARD = gnu11;
				GENERATE_INFOPLIST_FILE = YES;
				IPHONEOS_DEPLOYMENT_TARGET = 16.0;
				MARKETING_VERSION = 1.0;
				MTL_ENABLE_DEBUG_INFO = INCLUDE_SOURCE;
				MTL_FAST_MATH = YES;
				PRODUCT_BUNDLE_IDENTIFIER = com.example.cameraApp.RunnerUITests;
				PRODUCT_NAME = "$(TARGET_NAME)";
				SUPPORTED_PLATFORMS = "iphoneos iphonesimulator";
				SUPPORTS_MACCATALYST = NO;
				SUPPORTS_MAC_DESIGNED_FOR_IPHONE_IPAD = YES;
				SWIFT_EMIT_LOC_STRINGS = NO;
				TARGETED_DEVICE_FAMILY = "1,2";
				TEST_TARGET_NAME = Runner;
			};
			name = Debug;
		};
		5C7D45582951A480006F3F20 /* Release */ = {
			isa = XCBuildConfiguration;
			baseConfigurationReference = E0E15AFA9A84772E42C752B5 /* Pods-Runner-RunnerUITests.release.xcconfig */;
			buildSettings = {
				ALWAYS_EMBED_SWIFT_STANDARD_LIBRARIES = YES;
				CLANG_ANALYZER_NUMBER_OBJECT_CONVERSION = YES_AGGRESSIVE;
				CLANG_CXX_LANGUAGE_STANDARD = "gnu++20";
				CLANG_ENABLE_OBJC_WEAK = YES;
				CLANG_WARN_DOCUMENTATION_COMMENTS = YES;
				CLANG_WARN_QUOTED_INCLUDE_IN_FRAMEWORK_HEADER = YES;
				CLANG_WARN_UNGUARDED_AVAILABILITY = YES_AGGRESSIVE;
				CODE_SIGN_STYLE = Automatic;
				CURRENT_PROJECT_VERSION = 1;
				DEVELOPMENT_TEAM = Q8N2T428YG;
				GCC_C_LANGUAGE_STANDARD = gnu11;
				GENERATE_INFOPLIST_FILE = YES;
				IPHONEOS_DEPLOYMENT_TARGET = 16.0;
				MARKETING_VERSION = 1.0;
				MTL_FAST_MATH = YES;
				PRODUCT_BUNDLE_IDENTIFIER = com.example.cameraApp.RunnerUITests;
				PRODUCT_NAME = "$(TARGET_NAME)";
				SUPPORTED_PLATFORMS = "iphoneos iphonesimulator";
				SUPPORTS_MACCATALYST = NO;
				SUPPORTS_MAC_DESIGNED_FOR_IPHONE_IPAD = YES;
				SWIFT_EMIT_LOC_STRINGS = NO;
				TARGETED_DEVICE_FAMILY = "1,2";
				TEST_TARGET_NAME = Runner;
			};
			name = Release;
		};
		5C7D45592951A480006F3F20 /* Profile */ = {
			isa = XCBuildConfiguration;
			baseConfigurationReference = EDA1D42BFE61E5D3D276935E /* Pods-Runner-RunnerUITests.profile.xcconfig */;
			buildSettings = {
				ALWAYS_EMBED_SWIFT_STANDARD_LIBRARIES = YES;
				CLANG_ANALYZER_NUMBER_OBJECT_CONVERSION = YES_AGGRESSIVE;
				CLANG_CXX_LANGUAGE_STANDARD = "gnu++20";
				CLANG_ENABLE_OBJC_WEAK = YES;
				CLANG_WARN_DOCUMENTATION_COMMENTS = YES;
				CLANG_WARN_QUOTED_INCLUDE_IN_FRAMEWORK_HEADER = YES;
				CLANG_WARN_UNGUARDED_AVAILABILITY = YES_AGGRESSIVE;
				CODE_SIGN_STYLE = Automatic;
				CURRENT_PROJECT_VERSION = 1;
				DEVELOPMENT_TEAM = Q8N2T428YG;
				GCC_C_LANGUAGE_STANDARD = gnu11;
				GENERATE_INFOPLIST_FILE = YES;
				IPHONEOS_DEPLOYMENT_TARGET = 16.0;
				MARKETING_VERSION = 1.0;
				MTL_FAST_MATH = YES;
				PRODUCT_BUNDLE_IDENTIFIER = com.example.cameraApp.RunnerUITests;
				PRODUCT_NAME = "$(TARGET_NAME)";
				SUPPORTED_PLATFORMS = "iphoneos iphonesimulator";
				SUPPORTS_MACCATALYST = NO;
				SUPPORTS_MAC_DESIGNED_FOR_IPHONE_IPAD = YES;
				SWIFT_EMIT_LOC_STRINGS = NO;
				TARGETED_DEVICE_FAMILY = "1,2";
				TEST_TARGET_NAME = Runner;
			};
			name = Profile;
		};
		97C147031CF9000F007C117D /* Debug */ = {
			isa = XCBuildConfiguration;
			buildSettings = {
				ALWAYS_SEARCH_USER_PATHS = NO;
				CLANG_ANALYZER_NONNULL = YES;
				CLANG_CXX_LANGUAGE_STANDARD = "gnu++0x";
				CLANG_CXX_LIBRARY = "libc++";
				CLANG_ENABLE_MODULES = YES;
				CLANG_ENABLE_OBJC_ARC = YES;
				CLANG_WARN_BLOCK_CAPTURE_AUTORELEASING = YES;
				CLANG_WARN_BOOL_CONVERSION = YES;
				CLANG_WARN_COMMA = YES;
				CLANG_WARN_CONSTANT_CONVERSION = YES;
				CLANG_WARN_DEPRECATED_OBJC_IMPLEMENTATIONS = YES;
				CLANG_WARN_DIRECT_OBJC_ISA_USAGE = YES_ERROR;
				CLANG_WARN_EMPTY_BODY = YES;
				CLANG_WARN_ENUM_CONVERSION = YES;
				CLANG_WARN_INFINITE_RECURSION = YES;
				CLANG_WARN_INT_CONVERSION = YES;
				CLANG_WARN_NON_LITERAL_NULL_CONVERSION = YES;
				CLANG_WARN_OBJC_IMPLICIT_RETAIN_SELF = YES;
				CLANG_WARN_OBJC_LITERAL_CONVERSION = YES;
				CLANG_WARN_OBJC_ROOT_CLASS = YES_ERROR;
				CLANG_WARN_RANGE_LOOP_ANALYSIS = YES;
				CLANG_WARN_STRICT_PROTOTYPES = YES;
				CLANG_WARN_SUSPICIOUS_MOVE = YES;
				CLANG_WARN_UNREACHABLE_CODE = YES;
				CLANG_WARN__DUPLICATE_METHOD_MATCH = YES;
				"CODE_SIGN_IDENTITY[sdk=iphoneos*]" = "iPhone Developer";
				COPY_PHASE_STRIP = NO;
				DEBUG_INFORMATION_FORMAT = dwarf;
				ENABLE_STRICT_OBJC_MSGSEND = YES;
				ENABLE_TESTABILITY = YES;
				GCC_C_LANGUAGE_STANDARD = gnu99;
				GCC_DYNAMIC_NO_PIC = NO;
				GCC_NO_COMMON_BLOCKS = YES;
				GCC_OPTIMIZATION_LEVEL = 0;
				GCC_PREPROCESSOR_DEFINITIONS = (
					"DEBUG=1",
					"$(inherited)",
				);
				GCC_WARN_64_TO_32_BIT_CONVERSION = YES;
				GCC_WARN_ABOUT_RETURN_TYPE = YES_ERROR;
				GCC_WARN_UNDECLARED_SELECTOR = YES;
				GCC_WARN_UNINITIALIZED_AUTOS = YES_AGGRESSIVE;
				GCC_WARN_UNUSED_FUNCTION = YES;
				GCC_WARN_UNUSED_VARIABLE = YES;
				IPHONEOS_DEPLOYMENT_TARGET = 12.0;
				MTL_ENABLE_DEBUG_INFO = YES;
				ONLY_ACTIVE_ARCH = YES;
				SDKROOT = iphoneos;
				TARGETED_DEVICE_FAMILY = "1,2";
			};
			name = Debug;
		};
		97C147041CF9000F007C117D /* Release */ = {
			isa = XCBuildConfiguration;
			buildSettings = {
				ALWAYS_SEARCH_USER_PATHS = NO;
				CLANG_ANALYZER_NONNULL = YES;
				CLANG_CXX_LANGUAGE_STANDARD = "gnu++0x";
				CLANG_CXX_LIBRARY = "libc++";
				CLANG_ENABLE_MODULES = YES;
				CLANG_ENABLE_OBJC_ARC = YES;
				CLANG_WARN_BLOCK_CAPTURE_AUTORELEASING = YES;
				CLANG_WARN_BOOL_CONVERSION = YES;
				CLANG_WARN_COMMA = YES;
				CLANG_WARN_CONSTANT_CONVERSION = YES;
				CLANG_WARN_DEPRECATED_OBJC_IMPLEMENTATIONS = YES;
				CLANG_WARN_DIRECT_OBJC_ISA_USAGE = YES_ERROR;
				CLANG_WARN_EMPTY_BODY = YES;
				CLANG_WARN_ENUM_CONVERSION = YES;
				CLANG_WARN_INFINITE_RECURSION = YES;
				CLANG_WARN_INT_CONVERSION = YES;
				CLANG_WARN_NON_LITERAL_NULL_CONVERSION = YES;
				CLANG_WARN_OBJC_IMPLICIT_RETAIN_SELF = YES;
				CLANG_WARN_OBJC_LITERAL_CONVERSION = YES;
				CLANG_WARN_OBJC_ROOT_CLASS = YES_ERROR;
				CLANG_WARN_RANGE_LOOP_ANALYSIS = YES;
				CLANG_WARN_STRICT_PROTOTYPES = YES;
				CLANG_WARN_SUSPICIOUS_MOVE = YES;
				CLANG_WARN_UNREACHABLE_CODE = YES;
				CLANG_WARN__DUPLICATE_METHOD_MATCH = YES;
				"CODE_SIGN_IDENTITY[sdk=iphoneos*]" = "iPhone Developer";
				COPY_PHASE_STRIP = NO;
				DEBUG_INFORMATION_FORMAT = "dwarf-with-dsym";
				ENABLE_NS_ASSERTIONS = NO;
				ENABLE_STRICT_OBJC_MSGSEND = YES;
				GCC_C_LANGUAGE_STANDARD = gnu99;
				GCC_NO_COMMON_BLOCKS = YES;
				GCC_WARN_64_TO_32_BIT_CONVERSION = YES;
				GCC_WARN_ABOUT_RETURN_TYPE = YES_ERROR;
				GCC_WARN_UNDECLARED_SELECTOR = YES;
				GCC_WARN_UNINITIALIZED_AUTOS = YES_AGGRESSIVE;
				GCC_WARN_UNUSED_FUNCTION = YES;
				GCC_WARN_UNUSED_VARIABLE = YES;
				IPHONEOS_DEPLOYMENT_TARGET = 12.0;
				MTL_ENABLE_DEBUG_INFO = NO;
				SDKROOT = iphoneos;
				SUPPORTED_PLATFORMS = iphoneos;
				SWIFT_COMPILATION_MODE = wholemodule;
				SWIFT_OPTIMIZATION_LEVEL = "-O";
				TARGETED_DEVICE_FAMILY = "1,2";
				VALIDATE_PRODUCT = YES;
			};
			name = Release;
		};
		97C147061CF9000F007C117D /* Debug */ = {
			isa = XCBuildConfiguration;
			baseConfigurationReference = 9740EEB21CF90195004384FC /* Debug.xcconfig */;
			buildSettings = {
				ASSETCATALOG_COMPILER_APPICON_NAME = AppIcon;
				CLANG_ENABLE_MODULES = YES;
				CURRENT_PROJECT_VERSION = "$(FLUTTER_BUILD_NUMBER)";
				DEFINES_MODULE = YES;
				DEVELOPMENT_TEAM = Q8N2T428YG;
				ENABLE_BITCODE = NO;
				INFOPLIST_FILE = Runner/Info.plist;
				LD_RUNPATH_SEARCH_PATHS = (
					"$(inherited)",
					"@executable_path/Frameworks",
				);
				PRODUCT_BUNDLE_IDENTIFIER = com.example.cameraApp;
				PRODUCT_NAME = "$(TARGET_NAME)";
				SWIFT_OBJC_BRIDGING_HEADER = "Runner/Runner-Bridging-Header.h";
				SWIFT_OPTIMIZATION_LEVEL = "-Onone";
				SWIFT_VERSION = 5.0;
				VERSIONING_SYSTEM = "apple-generic";
			};
			name = Debug;
		};
		97C147071CF9000F007C117D /* Release */ = {
			isa = XCBuildConfiguration;
			baseConfigurationReference = 7AFA3C8E1D35360C0083082E /* Release.xcconfig */;
			buildSettings = {
				ASSETCATALOG_COMPILER_APPICON_NAME = AppIcon;
				CLANG_ENABLE_MODULES = YES;
				CURRENT_PROJECT_VERSION = "$(FLUTTER_BUILD_NUMBER)";
				DEFINES_MODULE = YES;
				DEVELOPMENT_TEAM = Q8N2T428YG;
				ENABLE_BITCODE = NO;
				INFOPLIST_FILE = Runner/Info.plist;
				LD_RUNPATH_SEARCH_PATHS = (
					"$(inherited)",
					"@executable_path/Frameworks",
				);
				PRODUCT_BUNDLE_IDENTIFIER = com.example.cameraApp;
				PRODUCT_NAME = "$(TARGET_NAME)";
				SWIFT_OBJC_BRIDGING_HEADER = "Runner/Runner-Bridging-Header.h";
				SWIFT_VERSION = 5.0;
				VERSIONING_SYSTEM = "apple-generic";
			};
			name = Release;
		};
/* End XCBuildConfiguration section */

/* Begin XCConfigurationList section */
		5C7D45562951A480006F3F20 /* Build configuration list for PBXNativeTarget "RunnerUITests" */ = {
			isa = XCConfigurationList;
			buildConfigurations = (
				5C7D45572951A480006F3F20 /* Debug */,
				5C7D45582951A480006F3F20 /* Release */,
				5C7D45592951A480006F3F20 /* Profile */,
			);
			defaultConfigurationIsVisible = 0;
			defaultConfigurationName = Release;
		};
		97C146E91CF9000F007C117D /* Build configuration list for PBXProject "Runner" */ = {
			isa = XCConfigurationList;
			buildConfigurations = (
				97C147031CF9000F007C117D /* Debug */,
				97C147041CF9000F007C117D /* Release */,
				249021D3217E4FDB00AE95B9 /* Profile */,
			);
			defaultConfigurationIsVisible = 0;
			defaultConfigurationName = Release;
		};
		97C147051CF9000F007C117D /* Build configuration list for PBXNativeTarget "Runner" */ = {
			isa = XCConfigurationList;
			buildConfigurations = (
				97C147061CF9000F007C117D /* Debug */,
				97C147071CF9000F007C117D /* Release */,
				249021D4217E4FDB00AE95B9 /* Profile */,
			);
			defaultConfigurationIsVisible = 0;
			defaultConfigurationName = Release;
		};
/* End XCConfigurationList section */

/* Begin XCLocalSwiftPackageReference section */
		781AD8BC2B33823900A9FFBB /* XCLocalSwiftPackageReference "Flutter/ephemeral/Packages/FlutterGeneratedPluginSwiftPackage" */ = {
			isa = XCLocalSwiftPackageReference;
			relativePath = Flutter/ephemeral/Packages/FlutterGeneratedPluginSwiftPackage;
		};
/* End XCLocalSwiftPackageReference section */

/* Begin XCSwiftPackageProductDependency section */
		78A3181F2AECB46A00862997 /* FlutterGeneratedPluginSwiftPackage */ = {
			isa = XCSwiftPackageProductDependency;
			productName = FlutterGeneratedPluginSwiftPackage;
		};
/* End XCSwiftPackageProductDependency section */
	};
	rootObject = 97C146E61CF9000F007C117D /* Project object */;
}



================================================
FILE: example/ios/Runner.xcodeproj/project.xcworkspace/contents.xcworkspacedata
================================================
<?xml version="1.0" encoding="UTF-8"?>
<Workspace
   version = "1.0">
   <FileRef
      location = "self:">
   </FileRef>
</Workspace>



================================================
FILE: example/ios/Runner.xcodeproj/project.xcworkspace/xcshareddata/IDEWorkspaceChecks.plist
================================================
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
	<key>IDEDidComputeMac32BitWarning</key>
	<true/>
</dict>
</plist>



================================================
FILE: example/ios/Runner.xcodeproj/project.xcworkspace/xcshareddata/WorkspaceSettings.xcsettings
================================================
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
	<key>PreviewsEnabled</key>
	<false/>
</dict>
</plist>



================================================
FILE: example/ios/Runner.xcodeproj/xcshareddata/xcschemes/Runner.xcscheme
================================================
<?xml version="1.0" encoding="UTF-8"?>
<Scheme
   LastUpgradeVersion = "1510"
   version = "1.3">
   <BuildAction
      parallelizeBuildables = "YES"
      buildImplicitDependencies = "YES">
      <PreActions>
         <ExecutionAction
            ActionType = "Xcode.IDEStandardExecutionActionsCore.ExecutionActionType.ShellScriptAction">
            <ActionContent
               title = "Run Prepare Flutter Framework Script"
               scriptText = "/bin/sh &quot;$FLUTTER_ROOT/packages/flutter_tools/bin/xcode_backend.sh&quot; prepare&#10;">
               <EnvironmentBuildable>
                  <BuildableReference
                     BuildableIdentifier = "primary"
                     BlueprintIdentifier = "97C146ED1CF9000F007C117D"
                     BuildableName = "Runner.app"
                     BlueprintName = "Runner"
                     ReferencedContainer = "container:Runner.xcodeproj">
                  </BuildableReference>
               </EnvironmentBuildable>
            </ActionContent>
         </ExecutionAction>
      </PreActions>
      <BuildActionEntries>
         <BuildActionEntry
            buildForTesting = "YES"
            buildForRunning = "YES"
            buildForProfiling = "YES"
            buildForArchiving = "YES"
            buildForAnalyzing = "YES">
            <BuildableReference
               BuildableIdentifier = "primary"
               BlueprintIdentifier = "97C146ED1CF9000F007C117D"
               BuildableName = "Runner.app"
               BlueprintName = "Runner"
               ReferencedContainer = "container:Runner.xcodeproj">
            </BuildableReference>
         </BuildActionEntry>
      </BuildActionEntries>
   </BuildAction>
   <TestAction
      buildConfiguration = "Debug"
      selectedDebuggerIdentifier = "Xcode.DebuggerFoundation.Debugger.LLDB"
      selectedLauncherIdentifier = "Xcode.DebuggerFoundation.Launcher.LLDB"
      shouldUseLaunchSchemeArgsEnv = "YES">
      <MacroExpansion>
         <BuildableReference
            BuildableIdentifier = "primary"
            BlueprintIdentifier = "97C146ED1CF9000F007C117D"
            BuildableName = "Runner.app"
            BlueprintName = "Runner"
            ReferencedContainer = "container:Runner.xcodeproj">
         </BuildableReference>
      </MacroExpansion>
      <Testables>
         <TestableReference
            skipped = "NO"
            parallelizable = "YES">
            <BuildableReference
               BuildableIdentifier = "primary"
               BlueprintIdentifier = "5C7D454D2951A480006F3F20"
               BuildableName = "RunnerUITests.xctest"
               BlueprintName = "RunnerUITests"
               ReferencedContainer = "container:Runner.xcodeproj">
            </BuildableReference>
         </TestableReference>
      </Testables>
   </TestAction>
   <LaunchAction
      buildConfiguration = "Debug"
      selectedDebuggerIdentifier = "Xcode.DebuggerFoundation.Debugger.LLDB"
      selectedLauncherIdentifier = "Xcode.DebuggerFoundation.Launcher.LLDB"
      launchStyle = "0"
      useCustomWorkingDirectory = "NO"
      ignoresPersistentStateOnLaunch = "NO"
      debugDocumentVersioning = "YES"
      debugServiceExtension = "internal"
      enableGPUValidationMode = "1"
      allowLocationSimulation = "YES">
      <BuildableProductRunnable
         runnableDebuggingMode = "0">
         <BuildableReference
            BuildableIdentifier = "primary"
            BlueprintIdentifier = "97C146ED1CF9000F007C117D"
            BuildableName = "Runner.app"
            BlueprintName = "Runner"
            ReferencedContainer = "container:Runner.xcodeproj">
         </BuildableReference>
      </BuildableProductRunnable>
   </LaunchAction>
   <ProfileAction
      buildConfiguration = "Profile"
      shouldUseLaunchSchemeArgsEnv = "YES"
      savedToolIdentifier = ""
      useCustomWorkingDirectory = "NO"
      debugDocumentVersioning = "YES">
      <BuildableProductRunnable
         runnableDebuggingMode = "0">
         <BuildableReference
            BuildableIdentifier = "primary"
            BlueprintIdentifier = "97C146ED1CF9000F007C117D"
            BuildableName = "Runner.app"
            BlueprintName = "Runner"
            ReferencedContainer = "container:Runner.xcodeproj">
         </BuildableReference>
      </BuildableProductRunnable>
   </ProfileAction>
   <AnalyzeAction
      buildConfiguration = "Debug">
   </AnalyzeAction>
   <ArchiveAction
      buildConfiguration = "Release"
      revealArchiveInOrganizer = "YES">
   </ArchiveAction>
</Scheme>



================================================
FILE: example/ios/Runner.xcodeproj/xcshareddata/xcschemes/Runner.xcscheme.backup
================================================
<?xml version="1.0" encoding="UTF-8"?>
<Scheme
   LastUpgradeVersion = "1510"
   version = "1.3">
   <BuildAction
      parallelizeBuildables = "YES"
      buildImplicitDependencies = "YES">
      <BuildActionEntries>
         <BuildActionEntry
            buildForTesting = "YES"
            buildForRunning = "YES"
            buildForProfiling = "YES"
            buildForArchiving = "YES"
            buildForAnalyzing = "YES">
            <BuildableReference
               BuildableIdentifier = "primary"
               BlueprintIdentifier = "97C146ED1CF9000F007C117D"
               BuildableName = "Runner.app"
               BlueprintName = "Runner"
               ReferencedContainer = "container:Runner.xcodeproj">
            </BuildableReference>
         </BuildActionEntry>
      </BuildActionEntries>
   </BuildAction>
   <TestAction
      buildConfiguration = "Debug"
      selectedDebuggerIdentifier = "Xcode.DebuggerFoundation.Debugger.LLDB"
      selectedLauncherIdentifier = "Xcode.DebuggerFoundation.Launcher.LLDB"
      shouldUseLaunchSchemeArgsEnv = "YES">
      <MacroExpansion>
         <BuildableReference
            BuildableIdentifier = "primary"
            BlueprintIdentifier = "97C146ED1CF9000F007C117D"
            BuildableName = "Runner.app"
            BlueprintName = "Runner"
            ReferencedContainer = "container:Runner.xcodeproj">
         </BuildableReference>
      </MacroExpansion>
      <Testables>
         <TestableReference
            skipped = "NO"
            parallelizable = "YES">
            <BuildableReference
               BuildableIdentifier = "primary"
               BlueprintIdentifier = "5C7D454D2951A480006F3F20"
               BuildableName = "RunnerUITests.xctest"
               BlueprintName = "RunnerUITests"
               ReferencedContainer = "container:Runner.xcodeproj">
            </BuildableReference>
         </TestableReference>
      </Testables>
   </TestAction>
   <LaunchAction
      buildConfiguration = "Debug"
      selectedDebuggerIdentifier = "Xcode.DebuggerFoundation.Debugger.LLDB"
      selectedLauncherIdentifier = "Xcode.DebuggerFoundation.Launcher.LLDB"
      launchStyle = "0"
      useCustomWorkingDirectory = "NO"
      ignoresPersistentStateOnLaunch = "NO"
      debugDocumentVersioning = "YES"
      debugServiceExtension = "internal"
      allowLocationSimulation = "YES">
      <BuildableProductRunnable
         runnableDebuggingMode = "0">
         <BuildableReference
            BuildableIdentifier = "primary"
            BlueprintIdentifier = "97C146ED1CF9000F007C117D"
            BuildableName = "Runner.app"
            BlueprintName = "Runner"
            ReferencedContainer = "container:Runner.xcodeproj">
         </BuildableReference>
      </BuildableProductRunnable>
   </LaunchAction>
   <ProfileAction
      buildConfiguration = "Profile"
      shouldUseLaunchSchemeArgsEnv = "YES"
      savedToolIdentifier = ""
      useCustomWorkingDirectory = "NO"
      debugDocumentVersioning = "YES">
      <BuildableProductRunnable
         runnableDebuggingMode = "0">
         <BuildableReference
            BuildableIdentifier = "primary"
            BlueprintIdentifier = "97C146ED1CF9000F007C117D"
            BuildableName = "Runner.app"
            BlueprintName = "Runner"
            ReferencedContainer = "container:Runner.xcodeproj">
         </BuildableReference>
      </BuildableProductRunnable>
   </ProfileAction>
   <AnalyzeAction
      buildConfiguration = "Debug">
   </AnalyzeAction>
   <ArchiveAction
      buildConfiguration = "Release"
      revealArchiveInOrganizer = "YES">
   </ArchiveAction>
</Scheme>



================================================
FILE: example/ios/Runner.xcworkspace/contents.xcworkspacedata
================================================
<?xml version="1.0" encoding="UTF-8"?>
<Workspace
   version = "1.0">
   <FileRef
      location = "group:Runner.xcodeproj">
   </FileRef>
   <FileRef
      location = "group:Pods/Pods.xcodeproj">
   </FileRef>
</Workspace>



================================================
FILE: example/ios/Runner.xcworkspace/xcshareddata/IDEWorkspaceChecks.plist
================================================
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
	<key>IDEDidComputeMac32BitWarning</key>
	<true/>
</dict>
</plist>



================================================
FILE: example/ios/Runner.xcworkspace/xcshareddata/WorkspaceSettings.xcsettings
================================================
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
	<key>PreviewsEnabled</key>
	<false/>
</dict>
</plist>



================================================
FILE: example/ios/RunnerTests/RunnerTests.swift
================================================
import Flutter
import UIKit
import XCTest

// If your plugin has been explicitly set to "type: .dynamic" in the Package.swift,
// you will need to add your plugin as a dependency of RunnerTests within Xcode.

@testable import camerawesome

// This demonstrates a simple unit test of the Swift portion of this plugin's implementation.
//
// See https://developer.apple.com/documentation/xctest for more information about using XCTest.

class RunnerTests: XCTestCase {

  func testGetPlatformVersion() {
    let plugin = CamerawesomePlugin()

    let call = FlutterMethodCall(methodName: "getPlatformVersion", arguments: [])

    let resultExpectation = expectation(description: "result block must be called.")
    plugin.handle(call) { result in
      XCTAssertEqual(result as! String, "iOS " + UIDevice.current.systemVersion)
      resultExpectation.fulfill()
    }
    waitForExpectations(timeout: 1)
  }

}



================================================
FILE: example/ios/RunnerUITests/RunnerTests.m
================================================
@import XCTest;
@import patrol;
@import integration_test;

PATROL_INTEGRATION_TEST_IOS_RUNNER(RunnerTests)



================================================
FILE: example/lib/ai_analysis_barcode.dart
================================================
import 'dart:async';

import 'package:camera_app/utils/mlkit_utils.dart';
import 'package:camerawesome/camerawesome_plugin.dart';
import 'package:flutter/material.dart';
import 'package:google_mlkit_barcode_scanning/google_mlkit_barcode_scanning.dart';
import 'package:rxdart/rxdart.dart';

void main() {
  runApp(const MyApp());
}

class MyApp extends StatelessWidget {
  const MyApp({super.key});

  @override
  Widget build(BuildContext context) {
    return MaterialApp(
      title: 'camerAwesome App',
      theme: ThemeData(
        primarySwatch: Colors.blue,
      ),
      home: const MyHomePage(),
    );
  }
}

class MyHomePage extends StatefulWidget {
  const MyHomePage({super.key});

  @override
  State<MyHomePage> createState() => _MyHomePageState();
}

class _MyHomePageState extends State<MyHomePage> {
  final _barcodeScanner = BarcodeScanner(formats: [BarcodeFormat.all]);

  final _buffer = <String>[];
  final _barcodesController = BehaviorSubject<List<String>>();
  late final Stream<List<String>> _barcodesStream = _barcodesController.stream;
  final _scrollController = ScrollController();

  @override
  void dispose() {
    _barcodesController.close();
    super.dispose();
  }

  @override
  Widget build(BuildContext context) {
    return Scaffold(
      body: CameraAwesomeBuilder.previewOnly(
        onImageForAnalysis: (img) => _processImageBarcode(img),
        imageAnalysisConfig: AnalysisConfig(
          androidOptions: const AndroidAnalysisOptions.nv21(
            width: 1024,
          ),
          maxFramesPerSecond: 5,
          autoStart: false,
        ),
        builder: (cameraModeState, preview) {
          return _BarcodeDisplayWidget(
            barcodesStream: _barcodesStream,
            scrollController: _scrollController,
            analysisController: cameraModeState.analysisController!,
          );
        },
      ),
    );
  }

  Future _processImageBarcode(AnalysisImage img) async {
    final inputImage = img.toInputImage();

    try {
      var recognizedBarCodes = await _barcodeScanner.processImage(inputImage);
      for (Barcode barcode in recognizedBarCodes) {
        debugPrint("Barcode: [${barcode.format}]: ${barcode.rawValue}");
        _addBarcode("[${barcode.format.name}]: ${barcode.rawValue}");
      }
    } catch (error) {
      debugPrint("...sending image resulted error $error");
    }
  }

  void _addBarcode(String value) {
    try {
      if (_buffer.length > 300) {
        _buffer.removeRange(_buffer.length - 300, _buffer.length);
      }
      if (_buffer.isEmpty || value != _buffer[0]) {
        _buffer.insert(0, value);
        _barcodesController.add(_buffer);
        _scrollController.animateTo(
          0,
          duration: const Duration(milliseconds: 400),
          curve: Curves.fastLinearToSlowEaseIn,
        );
      }
    } catch (err) {
      debugPrint("...logging error $err");
    }
  }
}

class _BarcodeDisplayWidget extends StatefulWidget {
  final Stream<List<String>> barcodesStream;
  final ScrollController scrollController;

  final AnalysisController analysisController;

  const _BarcodeDisplayWidget({
    // ignore: unused_element, unused_element_parameter
    super.key,
    required this.barcodesStream,
    required this.scrollController,
    required this.analysisController,
  });

  @override
  State<_BarcodeDisplayWidget> createState() => _BarcodeDisplayWidgetState();
}

class _BarcodeDisplayWidgetState extends State<_BarcodeDisplayWidget> {
  @override
  Widget build(BuildContext context) {
    return Align(
      alignment: Alignment.bottomCenter,
      child: Container(
        decoration: BoxDecoration(
          color: Colors.tealAccent.withValues(alpha: 0.7),
        ),
        child: Column(mainAxisSize: MainAxisSize.min, children: [
          Material(
            color: Colors.transparent,
            child: CheckboxListTile(
              value: widget.analysisController.enabled,
              onChanged: (newValue) async {
                if (widget.analysisController.enabled == true) {
                  await widget.analysisController.stop();
                } else {
                  await widget.analysisController.start();
                }
                setState(() {});
              },
              title: const Text(
                "Enable barcode scan",
                style: TextStyle(fontWeight: FontWeight.bold),
              ),
            ),
          ),
          Container(
            height: 120,
            padding: const EdgeInsets.symmetric(horizontal: 16),
            child: StreamBuilder<List<String>>(
              stream: widget.barcodesStream,
              builder: (context, value) => !value.hasData
                  ? const SizedBox.expand()
                  : ListView.separated(
                      padding: const EdgeInsets.only(top: 8),
                      controller: widget.scrollController,
                      itemCount: value.data!.length,
                      separatorBuilder: (context, index) =>
                          const SizedBox(height: 4),
                      itemBuilder: (context, index) => Text(value.data![index]),
                    ),
            ),
          ),
        ]),
      ),
    );
  }
}



================================================
FILE: example/lib/ai_analysis_faces.dart
================================================
import 'dart:async';
import 'dart:math';

import 'package:camera_app/utils/mlkit_utils.dart';
import 'package:camerawesome/camerawesome_plugin.dart';
import 'package:flutter/material.dart';
import 'package:google_mlkit_face_detection/google_mlkit_face_detection.dart';
import 'package:rxdart/rxdart.dart';

/// This is an example using machine learning with the camera image
/// This is still in progress and some changes are about to come
/// - a provided canvas to draw over the camera
/// - scale and position points on the canvas easily (without calculating rotation, scale...)
/// ---------------------------
/// This use Google ML Kit plugin to process images on firebase
/// for more informations check
/// https://github.com/bharat-biradar/Google-Ml-Kit-plugin
void main() {
  runApp(const CameraAwesomeApp());
}

class CameraAwesomeApp extends StatelessWidget {
  const CameraAwesomeApp({super.key});

  @override
  Widget build(BuildContext context) {
    return const MaterialApp(
      title: 'camerAwesome App',
      home: CameraPage(),
    );
  }
}

class CameraPage extends StatefulWidget {
  const CameraPage({super.key});

  @override
  State<CameraPage> createState() => _CameraPageState();
}

class _CameraPageState extends State<CameraPage> {
  final _faceDetectionController = BehaviorSubject<FaceDetectionModel>();
  AnalysisPreview? _preview;

  final options = FaceDetectorOptions(
    enableContours: true,
    enableClassification: true,
  );
  late final faceDetector = FaceDetector(options: options);

  @override
  void deactivate() {
    faceDetector.close();
    super.deactivate();
  }

  @override
  void dispose() {
    _faceDetectionController.close();
    super.dispose();
  }

  @override
  Widget build(BuildContext context) {
    return Scaffold(
      body: CameraAwesomeBuilder.previewOnly(
        previewFit: CameraPreviewFit.contain,
        sensorConfig: SensorConfig.single(
          sensor: Sensor.position(SensorPosition.front),
          aspectRatio: CameraAspectRatios.ratio_1_1,
        ),
        onImageForAnalysis: (img) => _analyzeImage(img),
        imageAnalysisConfig: AnalysisConfig(
          androidOptions: const AndroidAnalysisOptions.nv21(
            width: 250,
          ),
          maxFramesPerSecond: 5,
        ),
        builder: (state, preview) {
          _preview = preview;
          return _MyPreviewDecoratorWidget(
            cameraState: state,
            faceDetectionStream: _faceDetectionController,
            preview: _preview!,
          );
        },
      ),
    );
  }

  Future _analyzeImage(AnalysisImage img) async {
    final inputImage = img.toInputImage();

    try {
      _faceDetectionController.add(
        FaceDetectionModel(
          faces: await faceDetector.processImage(inputImage),
          absoluteImageSize: inputImage.metadata!.size,
          rotation: 0,
          imageRotation: img.inputImageRotation,
          img: img,
        ),
      );
      // debugPrint("...sending image resulted with : ${faces?.length} faces");
    } catch (error) {
      debugPrint("...sending image resulted error $error");
    }
  }
}

class _MyPreviewDecoratorWidget extends StatelessWidget {
  final CameraState cameraState;
  final Stream<FaceDetectionModel> faceDetectionStream;
  final AnalysisPreview preview;

  const _MyPreviewDecoratorWidget({
    required this.cameraState,
    required this.faceDetectionStream,
    required this.preview,
  });

  @override
  Widget build(BuildContext context) {
    return IgnorePointer(
      child: StreamBuilder(
        stream: cameraState.sensorConfig$,
        builder: (_, snapshot) {
          if (!snapshot.hasData) {
            return const SizedBox();
          } else {
            return StreamBuilder<FaceDetectionModel>(
              stream: faceDetectionStream,
              builder: (_, faceModelSnapshot) {
                if (!faceModelSnapshot.hasData) return const SizedBox();
                // this is the transformation needed to convert the image to the preview
                // Android mirrors the preview but the analysis image is not
                final canvasTransformation = faceModelSnapshot.data!.img
                    ?.getCanvasTransformation(preview);
                return CustomPaint(
                  painter: FaceDetectorPainter(
                    model: faceModelSnapshot.requireData,
                    canvasTransformation: canvasTransformation,
                    preview: preview,
                  ),
                );
              },
            );
          }
        },
      ),
    );
  }
}

class FaceDetectorPainter extends CustomPainter {
  final FaceDetectionModel model;
  final CanvasTransformation? canvasTransformation;
  final AnalysisPreview? preview;

  FaceDetectorPainter({
    required this.model,
    this.canvasTransformation,
    this.preview,
  });

  @override
  void paint(Canvas canvas, Size size) {
    if (preview == null || model.img == null) {
      return;
    }
    // We apply the canvas transformation to the canvas so that the barcode
    // rect is drawn in the correct orientation. (Android only)
    if (canvasTransformation != null) {
      canvas.save();
      canvas.applyTransformation(canvasTransformation!, size);
    }
    for (final Face face in model.faces) {
      Map<FaceContourType, Path> paths = {
        for (var fct in FaceContourType.values) fct: Path()
      };
      face.contours.forEach((contourType, faceContour) {
        if (faceContour != null) {
          paths[contourType]!.addPolygon(
              faceContour.points
                  .map(
                    (element) => preview!.convertFromImage(
                      Offset(element.x.toDouble(), element.y.toDouble()),
                      model.img!,
                    ),
                  )
                  .toList(),
              true);
          for (var element in faceContour.points) {
            var position = preview!.convertFromImage(
              Offset(element.x.toDouble(), element.y.toDouble()),
              model.img!,
            );
            canvas.drawCircle(
              position,
              4,
              Paint()..color = Colors.blue,
            );
          }
        }
      });
      paths.removeWhere((key, value) => value.getBounds().isEmpty);
      for (var p in paths.entries) {
        canvas.drawPath(
            p.value,
            Paint()
              ..color = Colors.orange
              ..strokeWidth = 2
              ..style = PaintingStyle.stroke);
      }
    }
    // if you want to draw without canvas transformation, use this:
    if (canvasTransformation != null) {
      canvas.restore();
    }
  }

  @override
  bool shouldRepaint(FaceDetectorPainter oldDelegate) {
    return oldDelegate.model != model;
  }
}

extension InputImageRotationConversion on InputImageRotation {
  double toRadians() {
    final degrees = toDegrees();
    return degrees * 2 * pi / 360;
  }

  int toDegrees() {
    switch (this) {
      case InputImageRotation.rotation0deg:
        return 0;
      case InputImageRotation.rotation90deg:
        return 90;
      case InputImageRotation.rotation180deg:
        return 180;
      case InputImageRotation.rotation270deg:
        return 270;
    }
  }
}

class FaceDetectionModel {
  final List<Face> faces;
  final Size absoluteImageSize;
  final int rotation;
  final InputImageRotation imageRotation;
  final AnalysisImage? img;

  FaceDetectionModel({
    required this.faces,
    required this.absoluteImageSize,
    required this.rotation,
    required this.imageRotation,
    this.img,
  });

  Size get croppedSize => img!.croppedSize;

  @override
  bool operator ==(Object other) =>
      identical(this, other) ||
      other is FaceDetectionModel &&
          runtimeType == other.runtimeType &&
          faces == other.faces &&
          absoluteImageSize == other.absoluteImageSize &&
          rotation == other.rotation &&
          imageRotation == other.imageRotation &&
          croppedSize == other.croppedSize;

  @override
  int get hashCode =>
      faces.hashCode ^
      absoluteImageSize.hashCode ^
      rotation.hashCode ^
      imageRotation.hashCode ^
      croppedSize.hashCode;
}



================================================
FILE: example/lib/ai_analysis_text.dart
================================================
import 'dart:async';

import 'package:camera_app/utils/mlkit_utils.dart';
import 'package:camerawesome/camerawesome_plugin.dart';
import 'package:flutter/material.dart';
import 'package:google_mlkit_text_recognition/google_mlkit_text_recognition.dart';

void main() {
  runApp(const MyApp());
}

class MyApp extends StatelessWidget {
  const MyApp({super.key});

  @override
  Widget build(BuildContext context) {
    return MaterialApp(
      title: 'camerAwesome App',
      theme: ThemeData(
        primarySwatch: Colors.blue,
      ),
      home: const MyHomePage(),
    );
  }
}

class MyHomePage extends StatefulWidget {
  const MyHomePage({super.key});

  @override
  State<MyHomePage> createState() => _MyHomePageState();
}

class _MyHomePageState extends State<MyHomePage> {
  final _textRecognizer = TextRecognizer(script: TextRecognitionScript.latin);

  @override
  void dispose() {
    _textRecognizer.close();
    super.dispose();
  }

  @override
  Widget build(BuildContext context) {
    return Scaffold(
      body: CameraAwesomeBuilder.previewOnly(
        onImageForAnalysis: (img) => _processImageBarcode(img),
        imageAnalysisConfig: AnalysisConfig(
          androidOptions: const AndroidAnalysisOptions.nv21(
            width: 1024,
          ),
          maxFramesPerSecond: 5,
          autoStart: false,
        ),
        builder: (cameraModeState, preview) {
          return Container();
        },
      ),
    );
  }

  Future _processImageBarcode(AnalysisImage img) async {
    final inputImage = img.toInputImage();
    final RecognizedText recognizedText = await _textRecognizer //
        .processImage(inputImage);
    // String text = recognizedText.text;
    for (TextBlock block in recognizedText.blocks) {
      // final Rect rect = block.boundingBox;
      // final List<Point<int>> cornerPoints = block.cornerPoints;
      // final String text = block.text;
      // final List<String> languages = block.recognizedLanguages;
      for (TextLine line in block.lines) {
        debugPrint("[${line.text}]");
        for (TextElement element in line.elements) {
          debugPrint("   ${element.text}");
        }
      }
    }
  }
}



================================================
FILE: example/lib/analysis_image_filter.dart
================================================
import 'dart:async';
import 'dart:io';
import 'dart:math';
import 'dart:typed_data';

import 'package:camerawesome/camerawesome_plugin.dart';
import 'package:flutter/material.dart';
import 'package:image/image.dart' as imglib;

void main() {
  runApp(const CameraAwesomeApp());
}

class CameraAwesomeApp extends StatelessWidget {
  const CameraAwesomeApp({super.key});

  @override
  Widget build(BuildContext context) {
    return const MaterialApp(
      title: 'CamerAwesome App - Filter example',
      home: CameraPage(),
    );
  }
}

class CameraPage extends StatefulWidget {
  const CameraPage({super.key});

  @override
  State<CameraPage> createState() => _CameraPageState();
}

class _CameraPageState extends State<CameraPage> {
  final _imageStreamController = StreamController<AnalysisImage>();

  @override
  void dispose() {
    _imageStreamController.close();
    super.dispose();
  }

  @override
  Widget build(BuildContext context) {
    return Scaffold(
      body: CameraAwesomeBuilder.analysisOnly(
        sensorConfig: SensorConfig.single(
          sensor: Sensor.position(SensorPosition.front),
          aspectRatio: CameraAspectRatios.ratio_1_1,
        ),
        onImageForAnalysis: (img) async => _imageStreamController.add(img),
        imageAnalysisConfig: AnalysisConfig(
          androidOptions: const AndroidAnalysisOptions.yuv420(
            width: 150,
          ),
          maxFramesPerSecond: 20,
        ),
        builder: (state, preview) {
          return CameraPreviewDisplayer(
            analysisImageStream: _imageStreamController.stream,
          );
        },
      ),
    );
  }
}

class CameraPreviewDisplayer extends StatefulWidget {
  final Stream<AnalysisImage> analysisImageStream;

  const CameraPreviewDisplayer({
    super.key,
    required this.analysisImageStream,
  });

  @override
  State<CameraPreviewDisplayer> createState() => _CameraPreviewDisplayerState();
}

class _CameraPreviewDisplayerState extends State<CameraPreviewDisplayer> {
  Uint8List? _cachedJpeg;

  @override
  Widget build(BuildContext context) {
    return Center(
      child: StreamBuilder<AnalysisImage>(
        stream: widget.analysisImageStream,
        builder: (_, snapshot) {
          if (!snapshot.hasData) {
            return const SizedBox.shrink();
          }

          final img = snapshot.requireData;
          return img.when(jpeg: (image) {
            _cachedJpeg = _applyFilterOnBytes(image.bytes);

            return ImageAnalysisPreview(
              currentJpeg: _cachedJpeg!,
              width: image.width.toDouble(),
              height: image.height.toDouble(),
            );
          }, yuv420: (Yuv420Image image) {
            return FutureBuilder<JpegImage>(
                future: image.toJpeg(),
                builder: (_, snapshot) {
                  if (snapshot.data == null && _cachedJpeg == null) {
                    return const Center(
                      child: CircularProgressIndicator(),
                    );
                  } else if (snapshot.data != null) {
                    _cachedJpeg = _applyFilterOnBytes(
                      snapshot.data!.bytes,
                    );
                  }
                  return ImageAnalysisPreview(
                    currentJpeg: _cachedJpeg!,
                    width: image.width.toDouble(),
                    height: image.height.toDouble(),
                  );
                });
          }, nv21: (Nv21Image image) {
            return FutureBuilder<JpegImage>(
                future: image.toJpeg(),
                builder: (_, snapshot) {
                  if (snapshot.data == null && _cachedJpeg == null) {
                    return const Center(
                      child: CircularProgressIndicator(),
                    );
                  } else if (snapshot.data != null) {
                    _cachedJpeg = _applyFilterOnBytes(
                      snapshot.data!.bytes,
                    );
                  }
                  return ImageAnalysisPreview(
                    currentJpeg: _cachedJpeg!,
                    width: image.width.toDouble(),
                    height: image.height.toDouble(),
                  );
                });
          }, bgra8888: (Bgra8888Image image) {
            // Conversion from dart directly
            _cachedJpeg = _applyFilterOnImage(
              imglib.Image.fromBytes(
                width: image.width,
                height: image.height,
                bytes: image.planes[0].bytes.buffer,
                order: imglib.ChannelOrder.bgra,
              ),
            );

            return ImageAnalysisPreview(
              currentJpeg: _cachedJpeg!,
              width: image.width.toDouble(),
              height: image.height.toDouble(),
            );
            // We handle all formats so we're sure there won't be a null value
          })!;
        },
      ),
    );
  }

  Uint8List _applyFilterOnBytes(Uint8List bytes) {
    return _applyFilterOnImage(imglib.decodeJpg(bytes)!);
  }

  Uint8List _applyFilterOnImage(imglib.Image image) {
    return imglib.encodeJpg(
      imglib.billboard(image),
      quality: 70,
    );
  }
}

class ImageAnalysisPreview extends StatelessWidget {
  final double width;
  final double height;
  final Uint8List currentJpeg;

  const ImageAnalysisPreview({
    super.key,
    required this.currentJpeg,
    required this.width,
    required this.height,
  });

  @override
  Widget build(BuildContext context) {
    return Container(
      color: Colors.black,
      child: Transform.scale(
        scaleX: Platform.isAndroid ? -1 : null,
        child: Transform.rotate(
          angle: 3 / 2 * pi,
          child: SizedBox.expand(
            child: Image.memory(
              currentJpeg,
              gaplessPlayback: true,
              fit: BoxFit.cover,
            ),
          ),
        ),
      ),
    );
  }
}



================================================
FILE: example/lib/analysis_image_filter_picker.dart
================================================
import 'dart:async';
import 'dart:io';
import 'dart:math';
import 'dart:typed_data';

import 'package:camerawesome/camerawesome_plugin.dart';
import 'package:flutter/material.dart';
import 'package:image/image.dart' as imglib;

void main() {
  runApp(const CameraAwesomeApp());
}

class CameraAwesomeApp extends StatelessWidget {
  const CameraAwesomeApp({super.key});

  @override
  Widget build(BuildContext context) {
    return const MaterialApp(
      title: 'CamerAwesome App - Filter picker example',
      home: CameraPage(),
    );
  }
}

class CameraPage extends StatefulWidget {
  const CameraPage({super.key});

  @override
  State<CameraPage> createState() => _CameraPageState();
}

class _CameraPageState extends State<CameraPage> {
  final _imageStreamController = StreamController<AnalysisImage>();

  @override
  void dispose() {
    _imageStreamController.close();
    super.dispose();
  }

  @override
  Widget build(BuildContext context) {
    return Scaffold(
      body: CameraAwesomeBuilder.analysisOnly(
        sensorConfig: SensorConfig.single(
          sensor: Sensor.position(SensorPosition.front),
          aspectRatio: CameraAspectRatios.ratio_1_1,
        ),
        onImageForAnalysis: (img) async => _imageStreamController.add(img),
        imageAnalysisConfig: AnalysisConfig(
          androidOptions: const AndroidAnalysisOptions.yuv420(
            width: 150,
          ),
          cupertinoOptions: const CupertinoAnalysisOptions.bgra8888(),
          maxFramesPerSecond: 30,
        ),
        builder: (state, preview) {
          return _MyPreviewDecoratorWidget(
            analysisImageStream: _imageStreamController.stream,
          );
        },
      ),
    );
  }
}

enum ImageFilter {
  adjustColor,
  billboard,
  bleachBypass,
  bulgeDistortion,
  bumpToNormal,
  chromaticAberration,
  colorHalftone,
  colorOffset,
  contrast,
  convolution,
  ditherImage,
  dotScreen,
  edgeGlow,
  emboss,
  gamma,
  gaussianBlur,
  grayScale,
  hdrToLdr,
  hexagonPixelate,
  invert,
  luminanceThreshold,
  monochrome,
  noise,
  normalize,
  pixelate,
  quantize,
  remapColors,
  scaleRgba,
  sepia,
  sketch,
  smooth,
  sobel,
  stretchDistorsion,
  vignettte;

  imglib.Image applyFilter(imglib.Image image) {
    switch (this) {
      case ImageFilter.adjustColor:
        return imglib.adjustColor(image);
      case ImageFilter.billboard:
        return imglib.billboard(image);
      case ImageFilter.bleachBypass:
        return imglib.bleachBypass(image);
      case ImageFilter.bulgeDistortion:
        return imglib.bulgeDistortion(image);
      case ImageFilter.bumpToNormal:
        return imglib.bumpToNormal(image);
      case ImageFilter.chromaticAberration:
        return imglib.chromaticAberration(image);
      case ImageFilter.colorHalftone:
        return imglib.colorHalftone(image);
      case ImageFilter.colorOffset:
        return imglib.colorOffset(image);
      case ImageFilter.contrast:
        return imglib.contrast(image, contrast: 150);
      case ImageFilter.convolution:
        return imglib.convolution(
          image,
          filter: [
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
            1,
          ],
        );
      case ImageFilter.ditherImage:
        return imglib.ditherImage(image);
      case ImageFilter.dotScreen:
        return imglib.dotScreen(image);
      case ImageFilter.edgeGlow:
        return imglib.edgeGlow(image);
      case ImageFilter.emboss:
        return imglib.emboss(image);
      case ImageFilter.gamma:
        return imglib.gamma(image, gamma: 7);
      case ImageFilter.gaussianBlur:
        return imglib.gaussianBlur(image, radius: 4);
      case ImageFilter.grayScale:
        return imglib.grayscale(image);
      case ImageFilter.hdrToLdr:
        return imglib.hdrToLdr(image);
      case ImageFilter.hexagonPixelate:
        return imglib.hexagonPixelate(image);
      case ImageFilter.invert:
        return imglib.invert(image);
      case ImageFilter.luminanceThreshold:
        return imglib.luminanceThreshold(image);
      case ImageFilter.monochrome:
        return imglib.monochrome(image);
      case ImageFilter.noise:
        return imglib.noise(image, 0.7);
      case ImageFilter.normalize:
        return imglib.normalize(image, min: 120, max: 220);
      case ImageFilter.pixelate:
        return imglib.pixelate(image, size: 4);
      case ImageFilter.quantize:
        return imglib.quantize(image);
      case ImageFilter.remapColors:
        return imglib.remapColors(
          image,
          blue: imglib.Channel.red,
          red: imglib.Channel.green,
          green: imglib.Channel.blue,
        );
      case ImageFilter.scaleRgba:
        return imglib.scaleRgba(image, scale: imglib.ColorRgb8(200, 50, 50));
      case ImageFilter.sepia:
        return imglib.sepia(image);
      case ImageFilter.sketch:
        return imglib.sketch(image);
      case ImageFilter.smooth:
        return imglib.smooth(image, weight: 4);
      case ImageFilter.sobel:
        return imglib.sobel(image);
      case ImageFilter.stretchDistorsion:
        return imglib.stretchDistortion(image);
      case ImageFilter.vignettte:
        return imglib.vignette(image);
    }
  }
}

class _MyPreviewDecoratorWidget extends StatefulWidget {
  final Stream<AnalysisImage> analysisImageStream;

  const _MyPreviewDecoratorWidget({
    required this.analysisImageStream,
  });

  @override
  State<_MyPreviewDecoratorWidget> createState() =>
      _MyPreviewDecoratorWidgetState();
}

class _MyPreviewDecoratorWidgetState extends State<_MyPreviewDecoratorWidget> {
  Uint8List? _currentJpeg;
  ImageFilter _filter = ImageFilter.billboard;

  @override
  Widget build(BuildContext context) {
    return Column(
      children: [
        Expanded(
          child: StreamBuilder<AnalysisImage>(
            stream: widget.analysisImageStream,
            builder: (_, snapshot) {
              if (!snapshot.hasData) {
                return const SizedBox.shrink();
              }

              final img = snapshot.requireData;
              return img.when(jpeg: (image) {
                    _currentJpeg = _applyFilterOnBytes(image.bytes);

                    return ImageAnalysisPreview(
                      currentJpeg: _currentJpeg!,
                      width: image.width.toDouble(),
                      height: image.height.toDouble(),
                    );
                  }, yuv420: (image) {
                    return FutureBuilder<JpegImage>(
                        future: image.toJpeg(),
                        builder: (_, snapshot) {
                          if (snapshot.data == null && _currentJpeg == null) {
                            return const Center(
                                child: CircularProgressIndicator());
                          } else if (snapshot.data != null) {
                            _currentJpeg =
                                _applyFilterOnBytes(snapshot.data!.bytes);
                          }
                          return ImageAnalysisPreview(
                            currentJpeg: _currentJpeg!,
                            width: image.width.toDouble(),
                            height: image.height.toDouble(),
                          );
                        });
                  }, nv21: (image) {
                    return FutureBuilder<JpegImage>(
                        future: image.toJpeg(),
                        builder: (_, snapshot) {
                          if (snapshot.data == null && _currentJpeg == null) {
                            return const Center(
                                child: CircularProgressIndicator());
                          } else if (snapshot.data != null) {
                            _currentJpeg =
                                _applyFilterOnBytes(snapshot.data!.bytes);
                          }
                          return ImageAnalysisPreview(
                            currentJpeg: _currentJpeg!,
                            width: image.width.toDouble(),
                            height: image.height.toDouble(),
                          );
                        });
                  }, bgra8888: (image) {
                    // _currentJpeg = _applyFilterOnImage(
                    //   imglib.Image.fromBytes(
                    //     width: image.width,
                    //     height: image.height,
                    //     bytes: image.planes[0].bytes.buffer,
                    //     order: imglib.ChannelOrder.bgra,
                    //   ),
                    // );

                    return FutureBuilder<JpegImage>(
                        future: image.toJpeg(),
                        builder: (_, snapshot) {
                          if (snapshot.data == null && _currentJpeg == null) {
                            return const Center(
                                child: CircularProgressIndicator());
                          } else if (snapshot.data != null) {
                            _currentJpeg =
                                _applyFilterOnBytes(snapshot.data!.bytes);
                          }
                          return ImageAnalysisPreview(
                            currentJpeg: _currentJpeg!,
                            width: image.width.toDouble(),
                            height: image.height.toDouble(),
                          );
                        });

                    // return ImageAnalysisPreview(
                    //   currentJpeg: _currentJpeg!,
                    //   width: image.width.toDouble(),
                    //   height: image.height.toDouble(),
                    // );
                  }) ??
                  Container(
                    color: Colors.red,
                    child: const Center(
                      child: Text("Format unsupported or conversion failed"),
                    ),
                  );
            },
          ),
        ),
        SizedBox(
          height: 200,
          child: GridView.builder(
            gridDelegate: const SliverGridDelegateWithFixedCrossAxisCount(
              crossAxisCount: 2,
              childAspectRatio: 40 / 9,
              crossAxisSpacing: 2,
              mainAxisSpacing: 2,
            ),
            itemCount: ImageFilter.values.length,
            itemBuilder: (_, index) {
              return Material(
                color: _filter == ImageFilter.values[index]
                    ? Colors.blue
                    : Colors.white,
                child: InkWell(
                  onTap: _filter == ImageFilter.values[index]
                      ? null
                      : () {
                          setState(() {
                            _filter = ImageFilter.values[index];
                          });
                        },
                  child: Center(
                    child: Text(ImageFilter.values[index].name),
                  ),
                ),
              );
            },
          ),
        ),
      ],
    );
  }

  Uint8List _applyFilterOnBytes(Uint8List bytes) {
    return _applyFilterOnImage(imglib.decodeJpg(bytes)!);
  }

  Uint8List _applyFilterOnImage(imglib.Image image) {
    return imglib.encodeJpg(
      _filter.applyFilter(image),
      quality: 70,
    );
  }
}

class ImageAnalysisPreview extends StatelessWidget {
  final double width;
  final double height;
  final Uint8List currentJpeg;

  const ImageAnalysisPreview({
    super.key,
    required this.currentJpeg,
    required this.width,
    required this.height,
  });

  @override
  Widget build(BuildContext context) {
    return Container(
      color: Colors.black,
      child: Transform.scale(
        scaleX: -1,
        child: Transform.rotate(
          angle: Platform.isAndroid ? 3 / 2 * pi : 0,
          child: SizedBox.expand(
            child: Image.memory(
              currentJpeg,
              gaplessPlayback: true,
              fit: BoxFit.cover,
            ),
          ),
        ),
      ),
    );
  }
}



================================================
FILE: example/lib/camera_analysis_capabilities.dart
================================================
import 'package:camerawesome/camerawesome_plugin.dart';
import 'package:flutter/material.dart';

void main() {
  runApp(const CameraAwesomeApp());
}

class CameraAwesomeApp extends StatelessWidget {
  const CameraAwesomeApp({super.key});

  @override
  Widget build(BuildContext context) {
    return const MaterialApp(
      title: 'Camera Analysis Capabilities',
      home: CameraPage(),
    );
  }
}

class CameraPage extends StatelessWidget {
  const CameraPage({super.key});

  @override
  Widget build(BuildContext context) {
    final sensor = Sensor.position(SensorPosition.back);
    return Scaffold(
      body: Container(
        color: Colors.white,
        child: CameraAwesomeBuilder.awesome(
          // Setting both video recording and image analysis is an error on Android if the camera is not of LEVEL 3
          // See explanations: https://developer.android.com/training/camerax/architecture#combine-use-cases
          saveConfig: SaveConfig.photoAndVideo(
            initialCaptureMode: CaptureMode.video,
          ),
          onImageForAnalysis: (image) async {
            debugPrint('Image for analysis received: ${image.size}');
          },
          imageAnalysisConfig: AnalysisConfig(
            androidOptions: const AndroidAnalysisOptions.jpeg(
              width: 250,
            ),
            maxFramesPerSecond: 3,
          ),
          sensorConfig: SensorConfig.single(
            sensor: sensor,
          ),
          previewDecoratorBuilder: (state, _) {
            return Center(
              child: FutureBuilder<bool>(
                  future: CameraCharacteristics
                      .isVideoRecordingAndImageAnalysisSupported(
                          sensor.position!),
                  builder: (_, snapshot) {
                    debugPrint("___---___--- received result ${snapshot.data}");
                    if (snapshot.data == null) {
                      return const CircularProgressIndicator();
                    }
                    return Padding(
                      padding: const EdgeInsets.all(20),
                      child: Text(
                        'Video recording AND image analysis at the same time ${snapshot.data! ? 'IS' : 'IS NOT'} supported on ${sensor.position?.name} sensor',
                        style: const TextStyle(
                          color: Colors.white,
                          fontSize: 20,
                          backgroundColor: Colors.black54,
                        ),
                        textAlign: TextAlign.center,
                      ),
                    );
                  }),
            );
          },
        ),
      ),
    );
  }
}



================================================
FILE: example/lib/custom_awesome_ui.dart
================================================
import 'package:camerawesome/camerawesome_plugin.dart';
import 'package:flutter/material.dart';

void main() {
  runApp(const CameraAwesomeApp());
}

class CameraAwesomeApp extends StatelessWidget {
  const CameraAwesomeApp({super.key});

  @override
  Widget build(BuildContext context) {
    return const MaterialApp(
      title: 'Custom CamerAwesome UI',
      home: CameraPage(),
    );
  }
}

class CameraPage extends StatelessWidget {
  const CameraPage({super.key});

  @override
  Widget build(BuildContext context) {
    return Scaffold(
      body: CameraAwesomeBuilder.awesome(
        saveConfig: SaveConfig.photo(),
        sensorConfig: SensorConfig.single(
          sensor: Sensor.position(SensorPosition.back),
          aspectRatio: CameraAspectRatios.ratio_1_1,
        ),
        previewFit: CameraPreviewFit.contain,
        previewPadding: const EdgeInsets.only(left: 150, top: 100),
        previewAlignment: Alignment.topRight,
        // Buttons of CamerAwesome UI will use this theme
        theme: AwesomeTheme(
          bottomActionsBackgroundColor: Colors.cyan.withValues(alpha: 0.5),
          buttonTheme: AwesomeButtonTheme(
            backgroundColor: Colors.cyan.withValues(alpha: 0.5),
            iconSize: 20,
            foregroundColor: Colors.white,
            padding: const EdgeInsets.all(16),
            // Tap visual feedback (ripple, bounce...)
            buttonBuilder: (child, onTap) {
              return ClipOval(
                child: Material(
                  color: Colors.transparent,
                  shape: const CircleBorder(),
                  child: InkWell(
                    splashColor: Colors.cyan,
                    highlightColor: Colors.cyan.withValues(alpha: 0.5),
                    onTap: onTap,
                    child: child,
                  ),
                ),
              );
            },
          ),
        ),
        topActionsBuilder: (state) => AwesomeTopActions(
          padding: EdgeInsets.zero,
          state: state,
          children: [
            Expanded(
              child: AwesomeFilterWidget(
                state: state,
                filterListPosition: FilterListPosition.aboveButton,
                filterListPadding: const EdgeInsets.only(top: 8),
              ),
            ),
          ],
        ),
        middleContentBuilder: (state) {
          return Column(
            children: [
              const Spacer(),
              Builder(builder: (context) {
                return Container(
                  color: AwesomeThemeProvider.of(context)
                      .theme
                      .bottomActionsBackgroundColor,
                  child: const Align(
                    alignment: Alignment.bottomCenter,
                    child: Padding(
                      padding: EdgeInsets.only(bottom: 10, top: 10),
                      child: Text(
                        "Take your best shot!",
                        style: TextStyle(
                          color: Colors.white,
                          fontWeight: FontWeight.bold,
                          fontStyle: FontStyle.italic,
                        ),
                      ),
                    ),
                  ),
                );
              }),
            ],
          );
        },
        bottomActionsBuilder: (state) => AwesomeBottomActions(
          state: state,
          left: AwesomeFlashButton(
            state: state,
          ),
          right: AwesomeCameraSwitchButton(
            state: state,
            scale: 1.0,
            onSwitchTap: (state) {
              state.switchCameraSensor(
                aspectRatio: state.sensorConfig.aspectRatio,
              );
            },
          ),
        ),
      ),
    );
  }
}



================================================
FILE: example/lib/custom_theme.dart
================================================
import 'package:camera_app/utils/file_utils.dart';
import 'package:camerawesome/camerawesome_plugin.dart';
import 'package:flutter/material.dart';

void main() {
  runApp(const CameraAwesomeApp());
}

class CameraAwesomeApp extends StatelessWidget {
  const CameraAwesomeApp({super.key});

  @override
  Widget build(BuildContext context) {
    return const MaterialApp(
      title: 'Themed CamerAwesome',
      home: CameraPage(),
    );
  }
}

class CameraPage extends StatelessWidget {
  const CameraPage({super.key});

  @override
  Widget build(BuildContext context) {
    return Scaffold(
      body: CameraAwesomeBuilder.awesome(
        saveConfig: SaveConfig.photoAndVideo(
          initialCaptureMode: CaptureMode.photo,
        ),
        defaultFilter: AwesomeFilter.AddictiveRed,
        sensorConfig: SensorConfig.single(
          aspectRatio: CameraAspectRatios.ratio_1_1,
        ),
        previewFit: CameraPreviewFit.fitWidth,
        // Buttons of CamerAwesome UI will use this theme
        theme: AwesomeTheme(
          bottomActionsBackgroundColor: Colors.deepPurple.withValues(alpha: 0.5),
          buttonTheme: AwesomeButtonTheme(
            backgroundColor: Colors.deepPurple.withValues(alpha: 0.5),
            iconSize: 32,
            padding: const EdgeInsets.all(18),
            foregroundColor: Colors.lightBlue,
            // Tap visual feedback (ripple, bounce...)
            buttonBuilder: (child, onTap) {
              return ClipOval(
                child: Material(
                  color: Colors.transparent,
                  shape: const CircleBorder(),
                  child: InkWell(
                    splashColor: Colors.deepPurple,
                    highlightColor: Colors.deepPurpleAccent.withValues(alpha: 0.5),
                    onTap: onTap,
                    child: child,
                  ),
                ),
              );
            },
          ),
        ),
        onMediaTap: (mediaCapture) {
          mediaCapture.captureRequest
              .when(single: (single) => single.file?.open());
        },
      ),
    );
  }
}



================================================
FILE: example/lib/custom_ui_example_1.dart
================================================
import 'package:camerawesome/camerawesome_plugin.dart';
import 'package:flutter/material.dart';

class CustomUiExample1 extends StatelessWidget {
  const CustomUiExample1({super.key});

  @override
  Widget build(BuildContext context) {
    return Scaffold(
      body: CameraAwesomeBuilder.custom(
        builder: (cameraState, preview) {
          return cameraState.when(
            onPreparingCamera: (state) =>
                const Center(child: CircularProgressIndicator()),
            onPhotoMode: (state) => TakePhotoUI(state),
            onVideoMode: (state) => RecordVideoUI(state, recording: false),
            onVideoRecordingMode: (state) =>
                RecordVideoUI(state, recording: true),
          );
        },
        saveConfig: SaveConfig.photoAndVideo(),
      ),
    );
  }
}

class TakePhotoUI extends StatelessWidget {
  final PhotoCameraState state;

  const TakePhotoUI(this.state, {super.key});

  @override
  Widget build(BuildContext context) {
    return Container();
  }
}

class RecordVideoUI extends StatelessWidget {
  final CameraState state;
  final bool recording;

  const RecordVideoUI(this.state, {super.key, required this.recording});

  @override
  Widget build(BuildContext context) {
    return Container();
  }
}



================================================
FILE: example/lib/custom_ui_example_2.dart
================================================
import 'dart:math';

import 'package:camerawesome/camerawesome_plugin.dart';
import 'package:camerawesome/pigeon.dart';
import 'package:flutter/material.dart';

/// Tap to take a photo example, with almost no UI
class CustomUiExample2 extends StatelessWidget {
  const CustomUiExample2({super.key});

  @override
  Widget build(BuildContext context) {
    return Scaffold(
      body: CameraAwesomeBuilder.custom(
        builder: (cameraState, preview) {
          return Stack(
            children: [
              const Positioned.fill(
                child: Align(
                  alignment: Alignment.bottomCenter,
                  child: Padding(
                    padding: EdgeInsets.only(bottom: 100),
                    child: Text(
                      "Tap to take a photo",
                      style: TextStyle(color: Colors.white),
                    ),
                  ),
                ),
              ),
              Positioned.fill(
                child: Align(
                  alignment: Alignment.bottomRight,
                  child: SizedBox(
                    width: 100,
                    child: StreamBuilder<MediaCapture?>(
                      stream: cameraState.captureState$,
                      builder: (_, snapshot) {
                        if (snapshot.data == null) {
                          return const SizedBox.shrink();
                        }
                        return AwesomeMediaPreview(
                          mediaCapture: snapshot.data!,
                          onMediaTap: (MediaCapture mediaCapture) {
                            // ignore: avoid_print
                            print("Tap on $mediaCapture");
                          },
                        );
                      },
                    ),
                  ),
                ),
              ),
            ],
          );
        },
        saveConfig: SaveConfig.photo(),
        onPreviewTapBuilder: (state) => OnPreviewTap(
          onTap: (Offset position, PreviewSize flutterPreviewSize,
              PreviewSize pixelPreviewSize) {
            state.when(onPhotoMode: (picState) => picState.takePhoto());
          },
          onTapPainter: (tapPosition) => TweenAnimationBuilder(
            key: ValueKey(tapPosition),
            tween: Tween<double>(begin: 1.0, end: 0.0),
            duration: const Duration(milliseconds: 500),
            builder: (context, anim, child) {
              return Transform.rotate(
                angle: anim * 2 * pi,
                child: Transform.scale(
                  scale: 4 * anim,
                  child: child,
                ),
              );
            },
            child: const Icon(
              Icons.camera,
              color: Colors.white,
            ),
          ),
        ),
      ),
    );
  }
}



================================================
FILE: example/lib/custom_ui_example_3.dart
================================================
import 'package:camerawesome/camerawesome_plugin.dart';
import 'package:flutter/material.dart';
import 'package:camera_app/utils/file_utils.dart';
import 'widgets/custom_media_preview.dart';

class CustomUiExample3 extends StatelessWidget {
  const CustomUiExample3({super.key});

  @override
  Widget build(BuildContext context) {
    return Scaffold(
      body: CameraAwesomeBuilder.custom(
        builder: (cameraState, preview) {
          return cameraState.when(
            onPreparingCamera: (state) =>
                const Center(child: CircularProgressIndicator()),
            onPhotoMode: (state) => TakePhotoUI(state),
            onVideoMode: (state) => RecordVideoUI(state, recording: false),
            onVideoRecordingMode: (state) =>
                RecordVideoUI(state, recording: true),
          );
        },
        saveConfig: SaveConfig.video(),
      ),
    );
  }
}

class TakePhotoUI extends StatelessWidget {
  final PhotoCameraState state;

  const TakePhotoUI(this.state, {super.key});

  @override
  Widget build(BuildContext context) {
    return Container();
  }
}

class RecordVideoUI extends StatelessWidget {
  final CameraState state;
  final bool recording;

  const RecordVideoUI(this.state, {super.key, required this.recording});

  @override
  Widget build(BuildContext context) {
    return Column(
      children: [
        const Spacer(),
        Padding(
          padding: const EdgeInsets.only(bottom: 20, left: 20, right: 20),
          child: Row(
            children: [
              AwesomeCaptureButton(state: state),
              const Spacer(),
              StreamBuilder(
                stream: state.captureState$,
                builder: (_, snapshot) {
                  return SizedBox(
                    width: 100,
                    height: 100,
                    child: CustomMediaPreview(
                      mediaCapture: snapshot.data,
                      onMediaTap: (mediaCapture) {
                        mediaCapture.captureRequest
                            .when(single: (single) => single.file?.open());
                      },
                    ),
                  );
                },
              ),
            ],
          ),
        ),
      ],
    );
  }
}



================================================
FILE: example/lib/drivable_camera.dart
================================================
import 'package:camerawesome/camerawesome_plugin.dart';
import 'package:flutter/material.dart';

class DrivableCamera extends StatelessWidget {
  final SaveConfig saveConfig;
  final List<Sensor> sensors;

  const DrivableCamera({
    super.key,
    required this.saveConfig,
    required this.sensors,
  });

  @override
  Widget build(BuildContext context) {
    return MaterialApp(
      home: Scaffold(
        body: CameraAwesomeBuilder.awesome(
          saveConfig: saveConfig,
          onMediaTap: (media) {},
          sensorConfig: sensors.length == 1
              ? SensorConfig.single(sensor: sensors.first)
              : SensorConfig.multiple(sensors: sensors),
        ),
      ),
    );
  }
}



================================================
FILE: example/lib/main.dart
================================================
import 'dart:io';

// import 'package:better_open_file/better_open_file.dart';
import 'package:camerawesome/camerawesome_plugin.dart';
import 'package:camerawesome/pigeon.dart';
import 'package:flutter/material.dart';
import 'package:path_provider/path_provider.dart';
import 'utils/file_utils.dart';

void main() {
  runApp(const CameraAwesomeApp());
}

class CameraAwesomeApp extends StatelessWidget {
  const CameraAwesomeApp({super.key});

  @override
  Widget build(BuildContext context) {
    return const MaterialApp(
      title: 'camerAwesome',
      home: CameraPage(),
    );
  }
}

class CameraPage extends StatelessWidget {
  const CameraPage({super.key});

  @override
  Widget build(BuildContext context) {
    return Scaffold(
      body: Container(
        color: Colors.white,
        child: CameraAwesomeBuilder.awesome(
          onMediaCaptureEvent: (event) {
            switch ((event.status, event.isPicture, event.isVideo)) {
              case (MediaCaptureStatus.capturing, true, false):
                debugPrint('Capturing picture...');
              case (MediaCaptureStatus.success, true, false):
                event.captureRequest.when(
                  single: (single) {
                    debugPrint('Picture saved: ${single.file?.path}');
                  },
                  multiple: (multiple) {
                    multiple.fileBySensor.forEach((key, value) {
                      debugPrint('multiple image taken: $key ${value?.path}');
                    });
                  },
                );
              case (MediaCaptureStatus.failure, true, false):
                debugPrint('Failed to capture picture: ${event.exception}');
              case (MediaCaptureStatus.capturing, false, true):
                debugPrint('Capturing video...');
              case (MediaCaptureStatus.success, false, true):
                event.captureRequest.when(
                  single: (single) {
                    debugPrint('Video saved: ${single.file?.path}');
                  },
                  multiple: (multiple) {
                    multiple.fileBySensor.forEach((key, value) {
                      debugPrint('multiple video taken: $key ${value?.path}');
                    });
                  },
                );
              case (MediaCaptureStatus.failure, false, true):
                debugPrint('Failed to capture video: ${event.exception}');
              default:
                debugPrint('Unknown event: $event');
            }
          },
          saveConfig: SaveConfig.photoAndVideo(
            initialCaptureMode: CaptureMode.photo,
            photoPathBuilder: (sensors) async {
              final Directory extDir = await getTemporaryDirectory();
              final testDir = await Directory(
                '${extDir.path}/camerawesome',
              ).create(recursive: true);
              if (sensors.length == 1) {
                final String filePath =
                    '${testDir.path}/${DateTime.now().millisecondsSinceEpoch}.jpg';
                return SingleCaptureRequest(filePath, sensors.first);
              }
              // Separate pictures taken with front and back camera
              return MultipleCaptureRequest(
                {
                  for (final sensor in sensors)
                    sensor:
                        '${testDir.path}/${sensor.position == SensorPosition.front ? 'front_' : "back_"}${DateTime.now().millisecondsSinceEpoch}.jpg',
                },
              );
            },
            videoOptions: VideoOptions(
              enableAudio: true,
              ios: CupertinoVideoOptions(
                fps: 10,
              ),
              android: AndroidVideoOptions(
                bitrate: 6000000,
                fallbackStrategy: QualityFallbackStrategy.lower,
              ),
            ),
            exifPreferences: ExifPreferences(saveGPSLocation: true),
          ),
          sensorConfig: SensorConfig.single(
            sensor: Sensor.position(SensorPosition.back),
            flashMode: FlashMode.auto,
            aspectRatio: CameraAspectRatios.ratio_4_3,
            zoom: 0.0,
          ),
          enablePhysicalButton: true,
          // filter: AwesomeFilter.AddictiveRed,
          previewAlignment: Alignment.center,
          previewFit: CameraPreviewFit.contain,
          onMediaTap: (mediaCapture) {
            mediaCapture.captureRequest.when(
              single: (single) {
                debugPrint('single: ${single.file?.path}');
                single.file?.open();
              },
              multiple: (multiple) {
                multiple.fileBySensor.forEach((key, value) {
                  debugPrint('multiple file taken: $key ${value?.path}');
                  value?.open();
                });
              },
            );
          },
          availableFilters: awesomePresetFiltersList,
        ),
      ),
    );
  }
}



================================================
FILE: example/lib/multi_camera.dart
================================================
import 'dart:io';
import 'dart:math';

import 'package:camera_app/utils/file_utils.dart';
import 'package:camerawesome/camerawesome_plugin.dart';
import 'package:flutter/material.dart';
import 'package:video_player/video_player.dart';

void main() {
  runApp(const CameraAwesomeApp());
}

class CameraAwesomeApp extends StatelessWidget {
  const CameraAwesomeApp({super.key});

  @override
  Widget build(BuildContext context) {
    return MaterialApp(
      title: 'camerAwesome',
      // home: CameraPage(),
      onGenerateRoute: (settings) {
        if (settings.name == '/') {
          return MaterialPageRoute(
            builder: (context) => const CameraPage(),
          );
        } else if (settings.name == '/gallery') {
          final multipleCaptureRequest =
              settings.arguments as MultipleCaptureRequest;
          return MaterialPageRoute(
            builder: (context) => GalleryPage(
              multipleCaptureRequest: multipleCaptureRequest,
            ),
          );
        }
        return null;
      },
    );
  }
}

class CameraPage extends StatefulWidget {
  const CameraPage({super.key});

  @override
  State<CameraPage> createState() => _CameraPageState();
}

class _CameraPageState extends State<CameraPage> {
  SensorDeviceData? sensorDeviceData;
  bool? isMultiCamSupported;
  PipShape shape = PipShape.circle;

  @override
  void initState() {
    super.initState();

    CamerawesomePlugin.getSensors().then((value) {
      setState(() {
        sensorDeviceData = value;
      });
    });

    CamerawesomePlugin.isMultiCamSupported().then((value) {
      setState(() {
        debugPrint("📸 isMultiCamSupported: $value");
        isMultiCamSupported = value;
      });
    });
  }

  @override
  Widget build(BuildContext context) {
    final screenSize = MediaQuery.of(context).size;
    return Scaffold(
      body: Container(
        color: Colors.white,
        child: sensorDeviceData != null && isMultiCamSupported != null
            ? CameraAwesomeBuilder.awesome(
                saveConfig: SaveConfig.photoAndVideo(
                    // initialCaptureMode: CaptureMode.video,
                    ),
                sensorConfig: isMultiCamSupported == true
                    ? SensorConfig.multiple(
                        sensors: (Platform.isIOS)
                            ? [
                                Sensor.type(SensorType.telephoto),
                                Sensor.position(SensorPosition.front),
                              ]
                            : [
                                Sensor.position(SensorPosition.back),
                                Sensor.position(SensorPosition.front),
                              ],
                        flashMode: FlashMode.auto,
                        aspectRatio: CameraAspectRatios.ratio_16_9,
                      )
                    : SensorConfig.single(
                        sensor: Sensor.position(SensorPosition.back),
                        flashMode: FlashMode.auto,
                        aspectRatio: CameraAspectRatios.ratio_16_9,
                      ),
                // TODO: create factory for multi cam & single
                // sensors: sensorDeviceData!.availableSensors
                //     .map((e) => Sensor.id(e.uid))
                //     .toList(),
                previewFit: CameraPreviewFit.fitWidth,
                onMediaTap: (mediaCapture) {
                  mediaCapture.captureRequest.when(
                    single: (single) => single.file?.open(),
                    multiple: (multiple) => Navigator.of(context).pushNamed(
                      '/gallery',
                      arguments: multiple,
                    ),
                  );
                },
                pictureInPictureConfigBuilder: (index, sensor) {
                  const width = 300.0;
                  return PictureInPictureConfig(
                    isDraggable: true,
                    startingPosition: Offset(
                      -50,
                      screenSize.height - 420,
                    ),
                    onTap: () {
                      debugPrint('on preview tap');
                    },
                    sensor: sensor,
                    pictureInPictureBuilder: (preview, aspectRatio) {
                      return SizedBox(
                        width: width,
                        height: width,
                        child: ClipPath(
                          clipper: _MyCustomPipClipper(
                            width: width,
                            height: width * aspectRatio,
                            shape: shape,
                          ),
                          child: SizedBox(
                            width: width,
                            child: preview,
                          ),
                        ),
                      );
                    },
                  );
                },
                previewDecoratorBuilder: (state, _) {
                  return Column(
                    mainAxisSize: MainAxisSize.min,
                    mainAxisAlignment: MainAxisAlignment.center,
                    crossAxisAlignment: CrossAxisAlignment.start,
                    children: [
                      Container(
                        color: Colors.white70,
                        margin: const EdgeInsets.only(left: 8),
                        child: const Text("Change picture in picture's shape:"),
                      ),
                      GridView.builder(
                        gridDelegate:
                            const SliverGridDelegateWithFixedCrossAxisCount(
                          crossAxisCount: 3,
                          childAspectRatio: 16 / 9,
                        ),
                        shrinkWrap: true,
                        padding: EdgeInsets.zero,
                        itemCount: PipShape.values.length,
                        itemBuilder: (context, index) {
                          final shape = PipShape.values[index];
                          return GestureDetector(
                            onTap: () {
                              setState(() {
                                this.shape = shape;
                              });
                            },
                            child: Container(
                              color: Colors.red.withValues(alpha: 0.5),
                              margin: const EdgeInsets.all(8.0),
                              child: Center(
                                child: Text(
                                  shape.name,
                                  textAlign: TextAlign.center,
                                ),
                              ),
                            ),
                          );
                        },
                      ),
                    ],
                  );
                },
              )
            : const SizedBox.shrink(),
      ),
    );
  }
}

enum PipShape {
  square,
  circle,
  roundedSquare,
  triangle,
  hexagon;

  Path getPath(Offset center, double width, double height) {
    switch (this) {
      case PipShape.square:
        return Path()
          ..addRect(Rect.fromCenter(
            center: center,
            width: min(width, height),
            height: min(width, height),
          ));
      case PipShape.circle:
        return Path()
          ..addOval(Rect.fromCenter(
            center: center,
            width: min(width, height),
            height: min(width, height),
          ));
      case PipShape.triangle:
        return Path()
          ..moveTo(center.dx, center.dy - min(width, height) / 2)
          ..lineTo(center.dx + min(width, height) / 2,
              center.dy + min(width, height) / 2)
          ..lineTo(center.dx - min(width, height) / 2,
              center.dy + min(width, height) / 2)
          ..close();
      case PipShape.roundedSquare:
        return Path()
          ..addRRect(RRect.fromRectAndRadius(
            Rect.fromCenter(
              center: center,
              width: min(width, height),
              height: min(width, height),
            ),
            const Radius.circular(20.0),
          ));
      case PipShape.hexagon:
        return Path()
          ..moveTo(center.dx, center.dy - min(width, height) / 2)
          ..lineTo(center.dx + min(width, height) / 2,
              center.dy - min(width, height) / 4)
          ..lineTo(center.dx + min(width, height) / 2,
              center.dy + min(width, height) / 4)
          ..lineTo(center.dx, center.dy + min(width, height) / 2)
          ..lineTo(center.dx - min(width, height) / 2,
              center.dy + min(width, height) / 4)
          ..lineTo(center.dx - min(width, height) / 2,
              center.dy - min(width, height) / 4)
          ..close();
    }
  }
}

class _MyCustomPipClipper extends CustomClipper<Path> {
  final double width;
  final double height;
  final PipShape shape;

  const _MyCustomPipClipper({
    required this.width,
    required this.height,
    required this.shape,
  });

  @override
  Path getClip(Size size) {
    return shape.getPath(
      size.center(Offset.zero),
      width,
      height,
    );
  }

  @override
  bool shouldReclip(covariant _MyCustomPipClipper oldClipper) {
    return width != oldClipper.width ||
        height != oldClipper.height ||
        shape != oldClipper.shape;
  }
}

class GalleryPage extends StatefulWidget {
  final MultipleCaptureRequest multipleCaptureRequest;

  const GalleryPage({super.key, required this.multipleCaptureRequest});

  @override
  State<GalleryPage> createState() => _GalleryPageState();
}

class _GalleryPageState extends State<GalleryPage> {
  @override
  Widget build(BuildContext context) {
    return Scaffold(
      appBar: AppBar(
        title: const Text('Gallery'),
      ),
      body: GridView.builder(
        gridDelegate: const SliverGridDelegateWithFixedCrossAxisCount(
          crossAxisCount: 3,
        ),
        itemCount: widget.multipleCaptureRequest.fileBySensor.length,
        itemBuilder: (context, index) {
          final sensor =
              widget.multipleCaptureRequest.fileBySensor.keys.toList()[index];
          final file = widget.multipleCaptureRequest.fileBySensor[sensor];
          return GestureDetector(
            onTap: () => file.open(),
            child: file!.path.endsWith("jpg")
                ? Image.file(
                    File(file.path),
                    fit: BoxFit.cover,
                  )
                : VideoPreview(file: File(file.path)),
          );
        },
      ),
    );
  }
}

class VideoPreview extends StatefulWidget {
  final File file;

  const VideoPreview({super.key, required this.file});

  @override
  State<StatefulWidget> createState() {
    return _VideoPreviewState();
  }
}

class _VideoPreviewState extends State<VideoPreview> {
  late VideoPlayerController _controller;

  @override
  void initState() {
    super.initState();
    _controller = VideoPlayerController.file(widget.file)
      ..setLooping(true)
      ..initialize().then((_) {
        setState(() {});
        _controller.play();
      });
  }

  @override
  Widget build(BuildContext context) {
    return Center(
      child: _controller.value.isInitialized
          ? AspectRatio(
              aspectRatio: _controller.value.aspectRatio,
              child: VideoPlayer(_controller),
            )
          : const SizedBox.shrink(),
    );
  }
}



================================================
FILE: example/lib/preview_overlay_example.dart
================================================
import 'dart:async';

import 'package:camera_app/utils/file_utils.dart';
import 'package:camera_app/utils/mlkit_utils.dart';
import 'package:camera_app/widgets/barcode_preview_overlay.dart';
import 'package:camerawesome/camerawesome_plugin.dart';
import 'package:flutter/material.dart';
import 'package:google_mlkit_barcode_scanning/google_mlkit_barcode_scanning.dart';

void main() {
  runApp(const CameraAwesomeApp());
}

class CameraAwesomeApp extends StatelessWidget {
  const CameraAwesomeApp({super.key});

  @override
  Widget build(BuildContext context) {
    return const MaterialApp(
      title: 'Preview Overlay',
      home: CameraPage(),
    );
  }
}

class CameraPage extends StatefulWidget {
  const CameraPage({super.key});

  @override
  State<CameraPage> createState() => _CameraPageState();
}

class _CameraPageState extends State<CameraPage> {
  final _barcodeScanner = BarcodeScanner(formats: [BarcodeFormat.all]);
  List<Barcode> _barcodes = [];
  AnalysisImage? _image;

  @override
  Widget build(BuildContext context) {
    return Scaffold(
      extendBodyBehindAppBar: true,
      body: CameraAwesomeBuilder.awesome(
        saveConfig: SaveConfig.photoAndVideo(
          initialCaptureMode: CaptureMode.photo,
        ),
        sensorConfig: SensorConfig.single(
          flashMode: FlashMode.auto,
          aspectRatio: CameraAspectRatios.ratio_16_9,
        ),
        previewFit: CameraPreviewFit.fitWidth,
        onMediaTap: (mediaCapture) {
          mediaCapture.captureRequest
              .when(single: (single) => single.file?.open());
        },
        previewDecoratorBuilder: (state, preview) {
          return BarcodePreviewOverlay(
            state: state,
            barcodes: _barcodes,
            analysisImage: _image,
            preview: preview,
          );
        },
        topActionsBuilder: (state) {
          return AwesomeTopActions(
            state: state,
            children: [
              AwesomeFlashButton(state: state),
              if (state is PhotoCameraState)
                AwesomeAspectRatioButton(state: state),
            ],
          );
        },
        middleContentBuilder: (state) {
          return const SizedBox.shrink();
        },
        bottomActionsBuilder: (state) {
          return const Padding(
            padding: EdgeInsets.only(bottom: 20),
            child: Text(
              "Scan your barcodes",
              style: TextStyle(
                color: Colors.white,
                fontSize: 30,
              ),
            ),
          );
        },
        onImageForAnalysis: (img) => _processImageBarcode(img),
        imageAnalysisConfig: AnalysisConfig(
          androidOptions: const AndroidAnalysisOptions.nv21(
            width: 256,
          ),
          maxFramesPerSecond: 3,
        ),
      ),
    );
  }

  Future _processImageBarcode(AnalysisImage img) async {
    try {
      var recognizedBarCodes =
          await _barcodeScanner.processImage(img.toInputImage());
      setState(() {
        _barcodes = recognizedBarCodes;
        _image = img;
      });
    } catch (error) {
      debugPrint("...sending image resulted error $error");
    }
  }
}



================================================
FILE: example/lib/run_drivable_camera.dart
================================================
import 'package:camera_app/drivable_camera.dart';
import 'package:camerawesome/camerawesome_plugin.dart';
import 'package:flutter/material.dart';

void main() {
  runApp(const CameraAwesomeApp());
}

class CameraAwesomeApp extends StatelessWidget {
  const CameraAwesomeApp({super.key});

  @override
  Widget build(BuildContext context) {
    return MaterialApp(
      title: 'camerAwesome',
      home: DrivableCamera(
        saveConfig: SaveConfig.photo(),
        sensors: [
          Sensor.position(SensorPosition.back),
        ],
      ),
    );
  }
}



================================================
FILE: example/lib/subroute_camera.dart
================================================
import 'package:camera_app/utils/file_utils.dart';
import 'package:camerawesome/camerawesome_plugin.dart';
import 'package:flutter/material.dart';

// this example is based on the camerawesome issue
// check if memory increase when showing and hiding the camera multiple times
// https://github.com/Apparence-io/CamerAwesome/issues/242

void main() {
  runApp(const CameraAwesomeApp());
}

class CameraAwesomeApp extends StatelessWidget {
  const CameraAwesomeApp({super.key});

  @override
  Widget build(BuildContext context) {
    return MaterialApp(
      title: 'camerAwesome',
      initialRoute: 'emptyPage',
      onGenerateRoute: (settings) {
        switch (settings.name) {
          case 'cameraPage':
            return MaterialPageRoute(builder: (_) => const CameraPage());
          default:
            return MaterialPageRoute(builder: (_) => const EmptyPage());
        }
      },
    );
  }
}

class CameraPage extends StatelessWidget {
  const CameraPage({super.key});

  @override
  Widget build(BuildContext context) {
    return Scaffold(
      body: Column(
        children: [
          Expanded(
            child: CameraAwesomeBuilder.awesome(
              saveConfig: SaveConfig.photoAndVideo(
                initialCaptureMode: CaptureMode.photo,
              ),
              defaultFilter: AwesomeFilter.AddictiveRed,
              sensorConfig: SensorConfig.single(
                flashMode: FlashMode.auto,
                aspectRatio: CameraAspectRatios.ratio_16_9,
              ),
              previewFit: CameraPreviewFit.fitWidth,
              onMediaTap: (mediaCapture) {
                mediaCapture.captureRequest.when(
                  single: (single) => single.file?.open(),
                );
              },
            ),
          ),
          ElevatedButton(
            child: const Text("Go to empty page"),
            onPressed: () {
              Navigator.pushReplacementNamed(context, 'emptyPage');
            },
          ),
        ],
      ),
    );
  }
}

class EmptyPage extends StatelessWidget {
  const EmptyPage({super.key});

  @override
  Widget build(BuildContext context) {
    return Scaffold(
      appBar: AppBar(
        title: const Text('Empty Page'),
        automaticallyImplyLeading: true,
      ),
      body: Center(
        child: ElevatedButton(
          child: const Text("Go to camera page"),
          onPressed: () {
            Navigator.pushReplacementNamed(context, 'cameraPage');
          },
        ),
      ),
    );
  }
}



================================================
FILE: example/lib/utils/file_utils.dart
================================================
import 'dart:io';

import 'package:camerawesome/camerawesome_plugin.dart';
import 'package:path_provider/path_provider.dart';
import 'package:cross_file/cross_file.dart';
import 'package:open_filex/open_filex.dart';

Future<String> path(CaptureMode captureMode) async {
  final Directory extDir = await getTemporaryDirectory();
  final testDir =
      await Directory('${extDir.path}/test').create(recursive: true);
  final String fileExtension = captureMode == CaptureMode.photo ? 'jpg' : 'mp4';
  final String filePath =
      '${testDir.path}/${DateTime.now().millisecondsSinceEpoch}.$fileExtension';
  return filePath;
}

extension XfileOpen on XFile {
  Future<void> open() async {
    final spath = this.path;
    await OpenFilex.open(spath);
  }
}



================================================
FILE: example/lib/utils/mlkit_utils.dart
================================================
import 'package:camerawesome/camerawesome_plugin.dart';
import 'package:google_mlkit_face_detection/google_mlkit_face_detection.dart';

extension MLKitUtils on AnalysisImage {
  InputImage toInputImage() {
    return when(
      nv21: (image) {
        return InputImage.fromBytes(
          bytes: image.bytes,
          metadata: InputImageMetadata(
            rotation: inputImageRotation,
            format: InputImageFormat.nv21,
            size: image.size,
            bytesPerRow: image.planes.first.bytesPerRow,
          ),
        );
      },
      bgra8888: (image) {
        final inputImageData = InputImageMetadata(
          size: size,
          rotation: inputImageRotation,
          format: inputImageFormat,
          bytesPerRow: image.planes.first.bytesPerRow,
        );

        return InputImage.fromBytes(
          bytes: image.bytes,
          metadata: inputImageData,
        );
      },
    )!;
  }

  InputImageRotation get inputImageRotation =>
      InputImageRotation.values.byName(rotation.name);

  InputImageFormat get inputImageFormat {
    switch (format) {
      case InputAnalysisImageFormat.bgra8888:
        return InputImageFormat.bgra8888;
      case InputAnalysisImageFormat.nv21:
        return InputImageFormat.nv21;
      default:
        return InputImageFormat.yuv420;
    }
  }
}



================================================
FILE: example/lib/widgets/barcode_preview_overlay.dart
================================================
import 'package:camerawesome/camerawesome_plugin.dart';
import 'package:flutter/material.dart';
import 'package:google_mlkit_barcode_scanning/google_mlkit_barcode_scanning.dart';

class BarcodePreviewOverlay extends StatefulWidget {
  final CameraState state;
  final List<Barcode> barcodes;
  final AnalysisImage? analysisImage;
  final bool isBackCamera;
  final AnalysisPreview preview;

  const BarcodePreviewOverlay({
    super.key,
    required this.state,
    required this.barcodes,
    required this.analysisImage,
    required this.preview,
    this.isBackCamera = true,
  });

  @override
  State<BarcodePreviewOverlay> createState() => _BarcodePreviewOverlayState();
}

class _BarcodePreviewOverlayState extends State<BarcodePreviewOverlay> {
  late Size _screenSize;
  late Rect _scanArea;

  // The barcode that is currently in the scan area (one at a time)
  String? _barcodeRead;

  Rect? _barcodeRect;

  // Whether the barcode is in the scan area
  bool? _barcodeInArea;

  // The image that was used to detect the barcode
  AnalysisImage? img;

  // The transformation that was used to display the image correctly (Android only)
  CanvasTransformation? canvasTransformation;

  @override
  void initState() {
    _refreshScanArea();
    super.initState();
  }

  @override
  void didUpdateWidget(covariant BarcodePreviewOverlay oldWidget) {
    if (widget.barcodes != oldWidget.barcodes ||
        widget.analysisImage != oldWidget.analysisImage &&
            widget.analysisImage != null) {
      _refreshScanArea();
      _detectBarcodeInArea(widget.analysisImage!, widget.barcodes);
    }
    super.didUpdateWidget(oldWidget);
  }

  _refreshScanArea() {
    // previewSize is the preview as seen by the camera but it might
    // not fulfill the current aspectRatio.
    // previewRect on the other hand is the preview as seen by the user,
    // including the clipping that may be needed to respect the current
    // aspectRatio.
    _scanArea = Rect.fromCenter(
      center: widget.preview.rect.center,
      // In this example, we want the barcode scan area to be a fraction
      // of the preview that is seen by the user, so we use previewRect
      width: widget.preview.rect.width * 0.7,
      height: widget.preview.rect.height * 0.3,
    );
  }

  @override
  Widget build(BuildContext context) {
    _screenSize = MediaQuery.of(context).size;

    return IgnorePointer(
      ignoring: true,
      child: Stack(children: [
        Positioned.fill(
          child: CustomPaint(
            painter: BarcodeFocusAreaPainter(
              scanArea: _scanArea.size,
              barcodeRect: _barcodeRect,
              canvasTransformation: canvasTransformation,
            ),
          ),
        ),

        // Place text indications around the scan area
        Positioned(
          top:
              widget.preview.previewSize.height / 2 + _scanArea.size.height / 2,
          left: 0,
          right: 0,
          child: Column(children: [
            Text(
              _barcodeRead ?? "",
              textAlign: TextAlign.center,
              style: const TextStyle(
                color: Colors.white,
                fontSize: 20,
              ),
            ),
            if (_barcodeInArea != null)
              Container(
                margin: const EdgeInsets.only(top: 8),
                color: _barcodeInArea! ? Colors.green : Colors.red,
                child: Text(
                  _barcodeInArea! ? "Barcode in area" : "Barcode not in area",
                  textAlign: TextAlign.center,
                  style: const TextStyle(
                    color: Colors.white,
                    fontSize: 20,
                  ),
                ),
              ),
          ]),
        ),
      ]),
    );
  }

  /// Detects if one of the [barcodes] is in the [_scanArea] and updates UI
  /// accordingly.
  Future _detectBarcodeInArea(AnalysisImage img, List<Barcode> barcodes) async {
    try {
      String? barcodeRead;
      _barcodeInArea = null;

      // The canvas transformation is needed to display the barcode rect correctly on android
      canvasTransformation = img.getCanvasTransformation(widget.preview);

      for (Barcode barcode in barcodes) {
        if (barcode.cornerPoints.isEmpty) {
          continue;
        }

        barcodeRead = "[${barcode.format.name}]: ${barcode.rawValue}";
        // For simplicity we consider the barcode to be a Rect. Due to
        // perspective, it might not be in reality. You could build a Path
        // from the 4 corner points instead.
        final topLeftOffset = barcode.cornerPoints[0];
        final bottomRightOffset = barcode.cornerPoints[2];
        var topLeftOff = widget.preview.convertFromImage(
          topLeftOffset.toOffset(),
          img,
        );
        var bottomRightOff = widget.preview.convertFromImage(
          bottomRightOffset.toOffset(),
          img,
        );

        _barcodeRect = Rect.fromLTRB(
          topLeftOff.dx,
          topLeftOff.dy,
          bottomRightOff.dx,
          bottomRightOff.dy,
        );

        // Approximately detect if the barcode is in the scan area by checking
        // if the center of the barcode is in the scan area.
        if (_scanArea.contains(
          _barcodeRect!.center.translate(
            (_screenSize.width - widget.preview.previewSize.width) / 2,
            (_screenSize.height - widget.preview.previewSize.height) / 2,
          ),
        )) {
          // Note: for a better detection, you should calculate the area of the
          // intersection between the barcode and the scan area and compare it
          // with the area of the barcode. If the intersection is greater than
          // a certain percentage, then the barcode is in the scan area.
          _barcodeInArea = true;
          // Only handle one good barcode in this example
          break;
        } else {
          _barcodeInArea = false;
        }

        if (_barcodeInArea != null && mounted) {
          setState(() {
            _barcodeRead = barcodeRead;
          });
        }
      }
    } catch (error, stacktrace) {
      debugPrint("...sending image resulted error $error $stacktrace");
    }
  }
}

class BarcodeFocusAreaPainter extends CustomPainter {
  final Size scanArea;
  final Rect? barcodeRect;
  final CanvasTransformation? canvasTransformation;

  BarcodeFocusAreaPainter({
    required this.scanArea,
    required this.barcodeRect,
    this.canvasTransformation,
  });

  @override
  void paint(Canvas canvas, Size size) {
    final clippedRect = getClippedRect(size);
    // Draw a semi-transparent overlay outside of the scan area
    canvas.drawPath(
      clippedRect,
      Paint()..color = Colors.black38,
    );
    canvas.drawLine(
      Offset(size.width / 2 - scanArea.width / 2, size.height / 2),
      Offset(size.width / 2 + scanArea.width / 2, size.height / 2),
      Paint()
        ..color = Colors.red
        ..strokeWidth = 2,
    );
    // Add border around the scan area
    canvas.drawPath(
      getInnerRect(size),
      Paint()
        ..style = PaintingStyle.stroke
        ..color = Colors.white70
        ..strokeWidth = 3,
    );

    // We apply the canvas transformation to the canvas so that the barcode
    // rect is drawn in the correct orientation. (Android only)
    if (canvasTransformation != null) {
      canvas.save();
      canvas.applyTransformation(canvasTransformation!, size);
    }

    // Draw the barcode rect for debugging purpose
    if (barcodeRect != null) {
      // apply canvas transformation
      canvas.drawRect(
        barcodeRect!,
        Paint()
          ..style = PaintingStyle.stroke
          ..color = Colors.blue
          ..strokeWidth = 2,
      );
    }

    // if you want to draw without canvas transformation, use this:
    if (canvasTransformation != null) {
      canvas.restore();
    }
  }

  Path getInnerRect(Size size) {
    return Path()
      ..addRRect(
        RRect.fromRectAndRadius(
          Rect.fromLTWH(
            (size.width - scanArea.width) / 2,
            (size.height - scanArea.height) / 2,
            scanArea.width,
            scanArea.height,
          ),
          const Radius.circular(32),
        ),
      );
  }

  Path getClippedRect(Size size) {
    final fullRect = Path()
      ..addRect(Rect.fromLTWH(0, 0, size.width, size.height));
    final innerRect = getInnerRect(size);
    // Substract innerRect from fullRect
    return Path.combine(
      PathOperation.difference,
      fullRect,
      innerRect,
    );
  }

  @override
  bool shouldRepaint(covariant BarcodeFocusAreaPainter oldDelegate) {
    return scanArea != oldDelegate.scanArea &&
        canvasTransformation != oldDelegate.canvasTransformation &&
        barcodeRect != oldDelegate.barcodeRect;
  }
}

extension RenderObjectExtensions on RenderObject {
  Offset localToGlobal(Offset localPosition) {
    final transform = getTransformTo(null);
    return MatrixUtils.transformPoint(transform, localPosition);
  }
}



================================================
FILE: example/lib/widgets/custom_media_preview.dart
================================================
import 'dart:io';

import 'package:camera_app/widgets/mini_video_player.dart';
import 'package:camerawesome/camerawesome_plugin.dart';
import 'package:flutter/cupertino.dart';
import 'package:flutter/foundation.dart';
import 'package:flutter/material.dart';

class CustomMediaPreview extends StatelessWidget {
  final MediaCapture? mediaCapture;
  final OnMediaTap onMediaTap;

  const CustomMediaPreview({
    super.key,
    required this.mediaCapture,
    required this.onMediaTap,
  });

  @override
  Widget build(BuildContext context) {
    return AwesomeOrientedWidget(
      child: AspectRatio(
        aspectRatio: 1,
        child: AwesomeBouncingWidget(
          onTap: mediaCapture != null && onMediaTap != null
              ? () => onMediaTap!(mediaCapture!)
              : null,
          child: ClipOval(
            child: Container(
              decoration: BoxDecoration(
                color: Colors.white10,
                shape: BoxShape.circle,
                border: Border.all(
                  color: Colors.white38,
                  width: 2,
                ),
              ),
              child: Container(
                color: Colors.transparent,
                child: _buildMedia(mediaCapture),
              ),
            ),
          ),
        ),
      ),
    );
  }

  Widget _buildMedia(MediaCapture? mediaCapture) {
    switch (mediaCapture?.status) {
      case MediaCaptureStatus.capturing:
        return Center(
          child: Padding(
            padding: const EdgeInsets.all(8),
            child: Platform.isIOS
                ? const CupertinoActivityIndicator(color: Colors.white)
                : const CircularProgressIndicator(color: Colors.white),
          ),
        );
      case MediaCaptureStatus.success:
        if (mediaCapture!.isPicture) {
          if (kIsWeb) {
            // TODO Check if that works
            return FutureBuilder<Uint8List>(
                future: mediaCapture.captureRequest.when(
                  single: (single) => single.file!.readAsBytes(),
                  multiple: (multiple) =>
                      multiple.fileBySensor.values.first!.readAsBytes(),
                ),
                builder: (_, snapshot) {
                  if (snapshot.hasData) {
                    return Image.memory(
                      snapshot.requireData,
                      fit: BoxFit.cover,
                      width: 300,
                    );
                  } else {
                    return Platform.isIOS
                        ? const CupertinoActivityIndicator(
                            color: Colors.white,
                          )
                        : const Padding(
                            padding: EdgeInsets.all(8.0),
                            child: CircularProgressIndicator(
                              color: Colors.white,
                              strokeWidth: 2.0,
                            ),
                          );
                  }
                });
          } else {
            return Image(
              fit: BoxFit.cover,
              image: ResizeImage(
                FileImage(
                  File(
                    mediaCapture.captureRequest.when(
                      single: (single) => single.file!.path,
                      multiple: (multiple) =>
                          multiple.fileBySensor.values.first!.path,
                    ),
                  ),
                ),
                width: 300,
              ),
            );
          }
        } else {
          return Ink(
            child: MiniVideoPlayer(
              filePath: mediaCapture.captureRequest.when(
                single: (single) => single.file!.path,
                multiple: (multiple) =>
                    multiple.fileBySensor.values.first!.path,
              ),
            ),
          );
        }
      case MediaCaptureStatus.failure:
        return const Icon(Icons.error);
      case null:
        return const SizedBox(
          width: 32,
          height: 32,
        );
    }
  }
}



================================================
FILE: example/lib/widgets/mini_video_player.dart
================================================
import 'dart:io';

import 'package:flutter/material.dart';
import 'package:video_player/video_player.dart';

class MiniVideoPlayer extends StatefulWidget {
  final String filePath;

  const MiniVideoPlayer({super.key, required this.filePath});

  @override
  State<StatefulWidget> createState() {
    return _MiniVideoPlayer();
  }
}

class _MiniVideoPlayer extends State<MiniVideoPlayer> {
  VideoPlayerController? _controller;

  @override
  void initState() {
    _controller = VideoPlayerController.file(File(widget.filePath))
      ..initialize().then((value) => setState(() {
            _controller?.setLooping(true);
            _controller?.play();
          }));
    super.initState();
  }

  @override
  void dispose() {
    _controller?.dispose();
    super.dispose();
  }

  @override
  Widget build(BuildContext context) {
    if (_controller == null || _controller?.value.isInitialized != true) {
      return const Center(child: CircularProgressIndicator());
    }
    return VideoPlayer(_controller!);
  }
}



================================================
FILE: example/scripts/run_firebase_test_lab.sh
================================================
SCRIPT_DIR=$( cd -- "$( dirname -- "${BASH_SOURCE[0]}" )" &> /dev/null && pwd )
pushd "${SCRIPT_DIR}/../android"
# flutter build generates files in android/ for building the app
flutter build apk
./gradlew app:assembleDebugAndroidTest
./gradlew app:assembleDebug -Ptarget=`pwd`/../integration_test/bundled_test.dart

popd


gcloud auth activate-service-account --key-file="${SCRIPT_DIR}/../../camerawesome-6e777-13db0fddbbe5.json"
gcloud --quiet config set project camerawesome-6e777

gcloud firebase test android run --type instrumentation \
  --app build/app/outputs/apk/debug/app-debug.apk \
  --test build/app/outputs/apk/androidTest/debug/app-debug-androidTest.apk \
  --device model=cheetah,version=33,locale=en,orientation=portrait \
  --timeout 15m
#  --results-bucket=<RESULTS_BUCKET> \
#  --results-dir=<RESULTS_DIRECTORY>




================================================
FILE: example/scripts/run_firebase_test_lab_multicam.sh
================================================
SCRIPT_DIR=$( cd -- "$( dirname -- "${BASH_SOURCE[0]}" )" &> /dev/null && pwd )
pushd "${SCRIPT_DIR}/../android"
# flutter build generates files in android/ for building the app
flutter build apk
./gradlew app:assembleDebugAndroidTest
./gradlew app:assembleDebug -Ptarget=`pwd`/../integration_test/concurrent_camera_test.dart

popd


gcloud auth activate-service-account --key-file="${SCRIPT_DIR}/../../camerawesome-6e777-13db0fddbbe5.json"
gcloud --quiet config set project camerawesome-6e777

gcloud firebase test android run --type instrumentation \
  --app build/app/outputs/apk/debug/app-debug.apk \
  --test build/app/outputs/apk/androidTest/debug/app-debug-androidTest.apk \
  --device model=cheetah,version=33,locale=en,orientation=portrait \
  --timeout 15m
#  --results-bucket=<RESULTS_BUCKET> \
#  --results-dir=<RESULTS_DIRECTORY>




================================================
FILE: example/scripts/run_native_android_multicam_tests.sh
================================================
SCRIPT_DIR=$( cd -- "$( dirname -- "${BASH_SOURCE[0]}" )" &> /dev/null && pwd )
pushd "${SCRIPT_DIR}/../android"
./gradlew :app:connectedDebugAndroidTest -Ptarget=$(pwd)/../integration_test/concurrent_camera_test.dart
popd


================================================
FILE: example/scripts/run_native_android_tests.sh
================================================
SCRIPT_DIR=$( cd -- "$( dirname -- "${BASH_SOURCE[0]}" )" &> /dev/null && pwd )
pushd "${SCRIPT_DIR}/../android"
./gradlew :app:connectedDebugAndroidTest -Ptarget=$(pwd)/../integration_test/bundled_test.dart
popd


================================================
FILE: example/scripts/run_patrol.sh
================================================
# Activate patrol cli
#dart pub global activate patrol_cli

# Run all tests
#patrol drive --target integration_test/bundled_test.dart
patrol test --target integration_test/bundled_test.dart

# Or only run specific tests
#patrol drive \
#  --target integration_test/ui_test.dart \
#  --target integration_test/photo_test.dart \
#  --target integration_test/video_test.dart



================================================
FILE: ios/camerawesome.podspec
================================================
#
# To learn more about a Podspec see http://guides.cocoapods.org/syntax/podspec.html.
# Run `pod lib lint camerawesome.podspec' to validate before publishing.
#
Pod::Spec.new do |s|
  s.name             = 'camerawesome'
  s.version          = '0.0.1'
  s.summary          = 'An open source camera plugin by the community for the community'
  s.description      = <<-DESC
An open source camera plugin by the community for the community
                       DESC
  s.homepage         = 'http://apparencekit.dev'
  s.license          = { :file => '../LICENSE' }
  s.author           = { 'Apparence.io' => 'hello@apparence.io' }
  s.source           = { :path => '.' }
  s.source_files = 'camerawesome/Sources/camerawesome/**/*'
  s.dependency 'Flutter'
  s.platform = :ios, '8.0'
  s.ios.deployment_target = '12.0'

  # Flutter.framework does not contain a i386 slice. Only x86_64 simulators are supported.
  s.pod_target_xcconfig = { 'DEFINES_MODULE' => 'YES', 'EXCLUDED_ARCHS[sdk=iphonesimulator*]' => 'i386' }
  #   s.pod_target_xcconfig = { 'DEFINES_MODULE' => 'YES', 'VALID_ARCHS[sdk=iphonesimulator*]' => 'x86_64' }
  s.swift_version = '5.0'
end



================================================
FILE: ios/.gitignore
================================================
.idea/
.vagrant/
.sconsign.dblite
.svn/

.DS_Store
*.swp
profile

DerivedData/
build/
.build/
.swiftpm/
GeneratedPluginRegistrant.h
GeneratedPluginRegistrant.m

.generated/

*.pbxuser
*.mode1v3
*.mode2v3
*.perspectivev3

!default.pbxuser
!default.mode1v3
!default.mode2v3
!default.perspectivev3

xcuserdata

*.moved-aside

*.pyc
*sync/
Icon?
.tags*

/Flutter/Generated.xcconfig
/Flutter/flutter_export_environment.sh


================================================
FILE: ios/Assets/.gitkeep
================================================



================================================
FILE: ios/camerawesome/Package.swift
================================================
// swift-tools-version: 6.0
// The swift-tools-version declares the minimum version of Swift required to build this package.

import PackageDescription

let package = Package(
    name: "camerawesome",
    platforms: [
        .iOS(.v12)
    ],
    products: [
        .library(name: "camerawesome", targets: ["camerawesome"])
    ],
    dependencies: [

    ],
    targets: [
        .target(
            name: "camerawesome",
            dependencies: [],
            resources: [],
            publicHeadersPath: "",
            cSettings: [
                .headerSearchPath("include")
            ]
        )
    ]
)




================================================
FILE: ios/camerawesome/Sources/camerawesome/CamerawesomePlugin.m
================================================
#import "CamerawesomePlugin.h"
#import "Pigeon.h"
#import "Permissions.h"
#import "SensorsController.h"
#import "SingleCameraPreview.h"
#import "MultiCameraController.h"
#import "AspectRatioUtils.h"
#import "CaptureModeUtils.h"
#import "FlashModeUtils.h"
#import "AnalysisController.h"

FlutterEventSink orientationEventSink;
FlutterEventSink videoRecordingEventSink;
FlutterEventSink imageStreamEventSink;
FlutterEventSink physicalButtonEventSink;

@interface CamerawesomePlugin () <CameraInterface, AnalysisImageUtils>
@property(readonly, nonatomic) NSObject<FlutterTextureRegistry> *textureRegistry;
@property NSMutableArray<NSNumber *> *texturesIds;
@property SingleCameraPreview *camera;
@property MultiCameraPreview *multiCamera;
- (instancetype)init:(NSObject<FlutterPluginRegistrar>*)registrar;
@end

// TODO: create a protocol to uniformize multi camera & single camera
// TODO: for multi camera, specify sensor position
// TODO: save all controllers here

@implementation CamerawesomePlugin {
  dispatch_queue_t _dispatchQueue;
  dispatch_queue_t _dispatchQueueAnalysis;
}

- (instancetype)init:(NSObject<FlutterPluginRegistrar>*)registrar {
  self = [super init];
  
  _textureRegistry = registrar.textures;
  
  if (_dispatchQueue == nil) {
    _dispatchQueue = dispatch_queue_create("camerawesome.dispatchqueue", NULL);
  }
  
  if (_dispatchQueueAnalysis == nil) {
    _dispatchQueueAnalysis = dispatch_queue_create("camerawesome.dispatchqueue.analysis", NULL);
  }
  
  return self;
}

+ (void)registerWithRegistrar:(NSObject<FlutterPluginRegistrar>*)registrar {
  CamerawesomePlugin *instance = [[CamerawesomePlugin alloc] init:registrar];
  FlutterEventChannel *orientationChannel = [FlutterEventChannel eventChannelWithName:@"camerawesome/orientation"
                                                                      binaryMessenger:[registrar messenger]];
  FlutterEventChannel *imageStreamChannel = [FlutterEventChannel eventChannelWithName:@"camerawesome/images"
                                                                      binaryMessenger:[registrar messenger]];
  FlutterEventChannel *physicalButtonChannel = [FlutterEventChannel eventChannelWithName:@"camerawesome/physical_button"
                                                                         binaryMessenger:[registrar messenger]];
  [orientationChannel setStreamHandler:instance];
  [imageStreamChannel setStreamHandler:instance];
  [physicalButtonChannel setStreamHandler:instance];
  
  CameraInterfaceSetup(registrar.messenger, instance);
  AnalysisImageUtilsSetup(registrar.messenger, instance);
}

#pragma mark - Camera engine methods

- (void)setupCameraSensors:(nonnull NSArray<PigeonSensor *> *)sensors aspectRatio:(nonnull NSString *)aspectRatio zoom:(nonnull NSNumber *)zoom mirrorFrontCamera:(nonnull NSNumber *)mirrorFrontCamera enablePhysicalButton:(nonnull NSNumber *)enablePhysicalButton flashMode:(nonnull NSString *)flashMode captureMode:(nonnull NSString *)captureMode enableImageStream:(nonnull NSNumber *)enableImageStream exifPreferences:(nonnull ExifPreferences *)exifPreferences videoOptions:(nullable VideoOptions *)videoOptions completion:(nonnull void (^)(NSNumber * _Nullable, FlutterError * _Nullable))completion {
  
  CaptureModes captureModeType = [CaptureModeUtils captureModeFromCaptureModeType:captureMode];
  if (![CameraPermissionsController checkAndRequestPermission]) {
    completion(nil, [FlutterError errorWithCode:@"MISSING_PERMISSION" message:@"you got to accept all permissions" details:nil]);
    return;
  }
  
  if (sensors == nil || [sensors count] <= 0) {
    completion(nil, [FlutterError errorWithCode:@"SENSOR_ERROR" message:@"empty sensors provided, please provide at least 1 sensor" details:nil]);
    return;
  }
  
  // If camera preview exist, dispose it
  if (self.camera != nil) {
    [self.camera dispose];
    self.camera = nil;
  }
  if (self.multiCamera != nil) {
    [self.multiCamera dispose];
    self.multiCamera = nil;
  }
  
  _texturesIds = [NSMutableArray new];
  
  AspectRatio aspectRatioMode = [AspectRatioUtils convertAspectRatio:aspectRatio];
  
  bool multiSensors = [sensors count] > 1;
  if (multiSensors) {
    if (![MultiCameraController isMultiCamSupported]) {
      completion(nil, [FlutterError errorWithCode:@"MULTI_CAM_NOT_SUPPORTED" message:@"multi camera feature is not supported" details:nil]);
      return;
    }
    
    self.multiCamera = [[MultiCameraPreview alloc] initWithSensors:sensors
                                                 mirrorFrontCamera:[mirrorFrontCamera boolValue]
                                              enablePhysicalButton:[enablePhysicalButton boolValue]
                                                   aspectRatioMode:aspectRatioMode
                                                       captureMode:captureModeType
                                                     dispatchQueue:dispatch_queue_create("camerawesome.multi_preview.dispatchqueue", NULL)];
    
    for (int i = 0; i < [sensors count]; i++) {
      int64_t textureId = [self->_textureRegistry registerTexture:self.multiCamera.textures[i]];
      [_texturesIds addObject:[NSNumber numberWithLongLong:textureId]];
    }
    
    __weak typeof(self) weakSelf = self;
    self.multiCamera.onPreviewFrameAvailable = ^(NSNumber * _Nullable i) {
      if (i == nil) {
        return;
      }
      
      NSNumber *textureNumber = weakSelf.texturesIds[[i intValue]];
      [weakSelf.textureRegistry textureFrameAvailable:[textureNumber longLongValue]];
    };
  } else {
    PigeonSensor *firstSensor = sensors.firstObject;
    self.camera = [[SingleCameraPreview alloc] initWithCameraSensor:firstSensor.position
                                                       videoOptions:videoOptions != nil ? videoOptions.ios : nil
                                                   recordingQuality:videoOptions != nil ? videoOptions.quality : VideoRecordingQualityHighest
                                                       streamImages:[enableImageStream boolValue]
                                                  mirrorFrontCamera:[mirrorFrontCamera boolValue]
                                               enablePhysicalButton:[enablePhysicalButton boolValue]
                                                    aspectRatioMode:aspectRatioMode
                                                        captureMode:captureModeType
                                                         completion:completion
                                                      dispatchQueue:dispatch_queue_create("camerawesome.single_preview.dispatchqueue", NULL)];
    
    int64_t textureId = [self->_textureRegistry registerTexture:self.camera.previewTexture];
    
    __weak typeof(self) weakSelf = self;
    self.camera.onPreviewFrameAvailable = ^{
      [weakSelf.textureRegistry textureFrameAvailable:textureId];
    };
    
    [self->_textureRegistry textureFrameAvailable:textureId];
    
    [self.texturesIds addObject:[NSNumber numberWithLongLong:textureId]];
  }
  
  completion(@(YES), nil);
}

- (nullable NSNumber *)startWithError:(FlutterError * _Nullable __autoreleasing * _Nonnull)error {
  if (self.camera == nil && self.multiCamera == nil) {
    *error = [FlutterError errorWithCode:@"CAMERA_MUST_BE_INIT" message:@"init must be call before start" details:nil];
    return @(NO);
  }
  
  dispatch_async(_dispatchQueue, ^{
    if (self.multiCamera != nil) {
      [self->_multiCamera start];
    } else {
      [self->_camera start];
    }
  });
  
  return @(YES);
}

- (nullable NSNumber *)stopWithError:(FlutterError * _Nullable __autoreleasing * _Nonnull)error {
  if (self.camera == nil && self.multiCamera == nil) {
    *error = [FlutterError errorWithCode:@"CAMERA_MUST_BE_INIT" message:@"init must be call before start" details:nil];
    return @(NO);
  }
  
  for (NSNumber *textureId in self->_texturesIds) {
    [self->_textureRegistry unregisterTexture:[textureId longLongValue]];
    dispatch_async(_dispatchQueue, ^{
      if (self.multiCamera != nil) {
        [self->_multiCamera stop];
      } else {
        [self->_camera stop];
      }
    });
  }
  
  return @(YES);
}

- (void)refreshWithError:(FlutterError * _Nullable __autoreleasing * _Nonnull)error {
  if (self.camera == nil && self.multiCamera == nil) {
    *error = [FlutterError errorWithCode:@"CAMERA_MUST_BE_INIT" message:@"init must be call before start" details:nil];
    return;
  }
  
  if (self.multiCamera != nil) {
    [self.multiCamera refresh];
  } else {
    [self.camera refresh];
  }
}

- (nullable NSNumber *)getPreviewTextureIdCameraPosition:(nonnull NSNumber *)cameraPosition error:(FlutterError * _Nullable __autoreleasing * _Nonnull)error {
  int cameraIndex = [cameraPosition intValue];
  
  if (_texturesIds != nil && [_texturesIds count] >= cameraIndex) {
    return [_texturesIds objectAtIndex:cameraIndex];
  }
  
  return nil;
}

#pragma mark - Event sink methods

- (FlutterError *)onListenWithArguments:(NSString *)arguments eventSink:(FlutterEventSink)eventSink {
  if ([arguments  isEqual: @"orientationChannel"]) {
    orientationEventSink = eventSink;
    
    if (self.camera != nil) {
      [self.camera setOrientationEventSink:orientationEventSink];
    }
    
  } else if ([arguments  isEqual: @"imagesChannel"]) {
    imageStreamEventSink = eventSink;
    
    if (self.camera != nil) {
      [self.camera setImageStreamEvent:imageStreamEventSink];
    }
  } else if ([arguments  isEqual: @"physicalButtonChannel"]) {
    physicalButtonEventSink = eventSink;
    
    if (self.camera != nil) {
      [self.camera setPhysicalButtonEventSink:physicalButtonEventSink];
    }
  }
  
  return nil;
}

- (FlutterError *)onCancelWithArguments:(NSString *)arguments {
  if ([arguments  isEqual: @"orientationChannel"]) {
    orientationEventSink = nil;
    
    if (self.camera != nil && self.camera.motionController != nil) {
      [self.camera setOrientationEventSink:orientationEventSink];
    }
  } else if ([arguments  isEqual: @"imagesChannel"]) {
    imageStreamEventSink = nil;
    
    if (self.camera != nil) {
      [self.camera setImageStreamEvent:imageStreamEventSink];
    }
  } else if ([arguments  isEqual: @"physicalButtonChannel"]) {
    physicalButtonEventSink = nil;
    
    if (self.camera != nil) {
      [self.camera setPhysicalButtonEventSink:physicalButtonEventSink];
    }
  }
  return nil;
}

#pragma mark - Permissions methods

- (void)requestPermissionsSaveGpsLocation:(nonnull NSNumber *)saveGpsLocation completion:(nonnull void (^)(NSArray<NSString *> * _Nullable, FlutterError * _Nullable))completion {
  NSMutableArray *permissions = [NSMutableArray new];
  
  const BOOL cameraGranted = [CameraPermissionsController checkAndRequestPermission];
  if (cameraGranted) {
    [permissions addObject:@"camera"];
  }
  
  bool needToSaveGPSLocation = [saveGpsLocation boolValue];
  if (needToSaveGPSLocation) {
    // TODO: move this to permissions object
    [self.camera.locationController requestWhenInUseAuthorizationOnGranted:^{
      [permissions addObject:@"location"];
      
      completion(permissions, nil);
    } declined:^{
      completion(permissions, nil);
    }];
  }
}

- (nullable NSArray<NSString *> *)checkPermissionsPermissions:(nonnull NSArray<NSString *> *)permissions error:(FlutterError * _Nullable __autoreleasing * _Nonnull)error {
  bool isMicrophonePermissionRequired = [permissions containsObject:@"microphone"];
  bool isCameraPermissionRequired = [permissions containsObject:@"camera"];
  
  bool cameraPermission = isCameraPermissionRequired ? [CameraPermissionsController checkPermission] : NO;
  bool microphonePermission = isMicrophonePermissionRequired ? [MicrophonePermissionsController checkPermission] : NO;
  
  NSMutableArray *grantedPermissions = [NSMutableArray new];
  if (cameraPermission) {
    [grantedPermissions addObject:@"camera"];
  }
  
  if (microphonePermission) {
    [grantedPermissions addObject:@"record_audio"];
  }
  
  return grantedPermissions;
}

- (nullable NSArray<NSString *> *)requestPermissionsWithError:(FlutterError * _Nullable __autoreleasing * _Nonnull)error {
  return @[];
}

#pragma mark - Focus methods

- (void)focusOnPointPreviewSize:(nonnull PreviewSize *)previewSize x:(nonnull NSNumber *)x y:(nonnull NSNumber *)y androidFocusSettings:(nullable AndroidFocusSettings *)androidFocusSettings error:(FlutterError *_Nullable __autoreleasing *_Nonnull)error {
  if (previewSize.width <= 0 || previewSize.height <= 0) {
    *error = [FlutterError errorWithCode:@"INVALID_PREVIEW" message:@"preview size width and height must be set" details:nil];
    return;
  }
  
  if (self.camera == nil && self.multiCamera == nil) {
    *error = [FlutterError errorWithCode:@"CAMERA_MUST_BE_INIT" message:@"init must be call before start" details:nil];
    return;
  }
  
  if (self.multiCamera != nil) {
    [self.multiCamera focusOnPoint:CGPointMake([x floatValue], [y floatValue]) preview:CGSizeMake([previewSize.width floatValue], [previewSize.height floatValue]) error:error];
  } else {
    [self.camera focusOnPoint:CGPointMake([x floatValue], [y floatValue]) preview:CGSizeMake([previewSize.width floatValue], [previewSize.height floatValue]) error:error];
  }
}

- (void)handleAutoFocusWithError:(FlutterError * _Nullable __autoreleasing * _Nonnull)error {
  // TODO: to remove ?
}

#pragma mark - Video recording methods

- (void)pauseVideoRecordingWithError:(FlutterError * _Nullable __autoreleasing * _Nonnull)error {
  if (self.camera == nil && self.multiCamera == nil) {
    *error = [FlutterError errorWithCode:@"CAMERA_MUST_BE_INIT" message:@"init must be call before start" details:nil];
    return;
  }
  
  if (self.camera == nil) {
    *error = [FlutterError errorWithCode:@"MULTI_CAMERA_UNSUPPORTED" message:@"this feature is currently not supported with multi camera feature" details:nil];
    return;
  }
  
  [self.camera pauseVideoRecording];
}

- (void)recordVideoSensors:(nonnull NSArray<PigeonSensor *> *)sensors paths:(nonnull NSArray<NSString *> *)paths completion:(nonnull void (^)(FlutterError * _Nullable))completion {
  if (self.camera == nil && self.multiCamera == nil) {
    completion([FlutterError errorWithCode:@"CAMERA_MUST_BE_INIT" message:@"init must be call before start" details:nil]);
    return;
  }
  
  if (self.camera == nil) {
    completion([FlutterError errorWithCode:@"MULTI_CAMERA_UNSUPPORTED" message:@"this feature is currently not supported with multi camera feature" details:nil]);
    return;
  }
  
  if (sensors == nil || [sensors count] <= 0 || paths == nil || [paths count] <= 0) {
    completion([FlutterError errorWithCode:@"PATH_NOT_SET" message:@"at least one path must be set" details:nil]);
    return;
  }
  
  if ([sensors count] != [paths count]) {
    completion([FlutterError errorWithCode:@"PATH_INVALID" message:@"sensors & paths list seems to be different" details:nil]);
    return;
  }
  
  [self.camera recordVideoAtPath:[paths firstObject] completion:completion];
}

- (void)resumeVideoRecordingWithError:(FlutterError * _Nullable __autoreleasing * _Nonnull)error {
  if (self.camera == nil && self.multiCamera == nil) {
    *error = [FlutterError errorWithCode:@"CAMERA_MUST_BE_INIT" message:@"init must be call before start" details:nil];
    return;
  }
  
  if (self.camera == nil) {
    *error = [FlutterError errorWithCode:@"MULTI_CAMERA_UNSUPPORTED" message:@"this feature is currently not supported with multi camera feature" details:nil];
    return;
  }
  
  [self.camera resumeVideoRecording];
}

- (void)setRecordingAudioModeEnableAudio:(NSNumber *)enableAudio completion:(void(^)(NSNumber *_Nullable, FlutterError *_Nullable))completion {
  if (self.camera == nil && self.multiCamera == nil) {
    completion(nil, [FlutterError errorWithCode:@"CAMERA_MUST_BE_INIT" message:@"init must be call before start" details:nil]);
    return;
  }
  
  if (self.camera == nil) {
    completion(nil, [FlutterError errorWithCode:@"MULTI_CAMERA_UNSUPPORTED" message:@"this feature is currently not supported with multi camera feature" details:nil]);
    return;
  }
  
  [self.camera setRecordingAudioMode:[enableAudio boolValue] completion:completion];
}

- (void)stopRecordingVideoWithCompletion:(nonnull void (^)(NSNumber * _Nullable, FlutterError * _Nullable))completion {
  if (self.camera == nil && self.multiCamera == nil) {
    completion(nil, [FlutterError errorWithCode:@"CAMERA_MUST_BE_INIT" message:@"init must be call before start" details:nil]);
    return;
  }
  
  if (self.camera == nil) {
    completion(nil, [FlutterError errorWithCode:@"MULTI_CAMERA_UNSUPPORTED" message:@"this feature is currently not supported with multi camera feature" details:nil]);
    return;
  }
  
  dispatch_async(_dispatchQueue, ^{
    [self->_camera stopRecordingVideo:completion];
  });
}

#pragma mark - General methods

- (void)takePhotoSensors:(nonnull NSArray<PigeonSensor *> *)sensors paths:(nonnull NSArray<NSString *> *)paths completion:(nonnull void (^)(NSNumber * _Nullable, FlutterError * _Nullable))completion {
  if (self.camera == nil && self.multiCamera == nil) {
    completion(nil, [FlutterError errorWithCode:@"CAMERA_MUST_BE_INIT" message:@"init must be call before start" details:nil]);
    return;
  }
  
  if (sensors == nil || [sensors count] <= 0 || paths == nil || [paths count] <= 0) {
    completion(0, [FlutterError errorWithCode:@"PATH_NOT_SET" message:@"at least one path must be set" details:nil]);
    return;
  }
  
  if ([sensors count] != [paths count]) {
    completion(0, [FlutterError errorWithCode:@"PATH_INVALID" message:@"sensors & paths list seems to be different" details:nil]);
    return;
  }
  
  dispatch_async(_dispatchQueue, ^{
    if (self.multiCamera != nil) {
      [self->_multiCamera takePhotoSensors:sensors paths:paths completion:completion];
    } else {
      [self->_camera takePictureAtPath:[paths firstObject] completion:completion];
    }
  });
}

- (void)setMirrorFrontCameraMirror:(nonnull NSNumber *)mirror error:(FlutterError * _Nullable __autoreleasing * _Nonnull)error {
  if (self.camera == nil && self.multiCamera == nil) {
    *error = [FlutterError errorWithCode:@"CAMERA_MUST_BE_INIT" message:@"init must be call before start" details:nil];
    return;
  }
  
  BOOL mirrorFrontCamera = [mirror boolValue];
  if (self.multiCamera != nil) {
    [self.multiCamera setMirrorFrontCamera:mirrorFrontCamera error:error];
  } else {
    [self.camera setMirrorFrontCamera:mirrorFrontCamera error:error];
  }
}

- (void)setCaptureModeMode:(nonnull NSString *)mode error:(FlutterError * _Nullable __autoreleasing * _Nonnull)error {
  if (self.camera == nil && self.multiCamera == nil) {
    *error = [FlutterError errorWithCode:@"CAMERA_MUST_BE_INIT" message:@"init must be call before start" details:nil];
    return;
  }
  
  CaptureModes captureMode = [CaptureModeUtils captureModeFromCaptureModeType:mode];
  if (self.multiCamera != nil) {
    if (captureMode == Video) {
      *error = [FlutterError errorWithCode:@"MULTI_CAMERA_UNSUPPORTED" message:@"impossible to set video mode when multi camera" details:nil];
      return;
    }
    
    [self.camera setCaptureMode:captureMode error:error];
  } else {
    [self.camera setCaptureMode:captureMode error:error];
  }
}

- (void)setCorrectionBrightness:(nonnull NSNumber *)brightness error:(FlutterError * _Nullable __autoreleasing * _Nonnull)error {
  if (self.camera == nil && self.multiCamera == nil) {
    *error = [FlutterError errorWithCode:@"CAMERA_MUST_BE_INIT" message:@"init must be call before start" details:nil];
    return;
  }
  if (self.multiCamera != nil) {
    [self.multiCamera setBrightness:brightness error:error];
  } else {
    [self.camera setBrightness:brightness error:error];
  }
}

- (void)setExifPreferencesExifPreferences:(ExifPreferences *)exifPreferences completion:(void(^)(NSNumber *_Nullable, FlutterError *_Nullable))completion {
  if (self.camera == nil && self.multiCamera == nil) {
    completion(nil, [FlutterError errorWithCode:@"CAMERA_MUST_BE_INIT" message:@"init must be call before start" details:nil]);
    return;
  }
  
  if (self.multiCamera != nil) {
    [self.multiCamera setExifPreferencesGPSLocation: exifPreferences.saveGPSLocation completion:completion];
  } else {
    [self.camera setExifPreferencesGPSLocation: exifPreferences.saveGPSLocation completion:completion];
  }
}

- (void)setFlashModeMode:(nonnull NSString *)mode error:(FlutterError * _Nullable __autoreleasing * _Nonnull)error {
  if (mode == nil || mode.length <= 0) {
    *error = [FlutterError errorWithCode:@"FLASH_MODE_ERROR" message:@"a flash mode NONE, AUTO, ALWAYS must be provided" details:nil];
    return;
  }
  
  if (self.camera == nil && self.multiCamera == nil) {
    *error = [FlutterError errorWithCode:@"CAMERA_MUST_BE_INIT" message:@"init must be call before start" details:nil];
    return;
  }
  
  CameraFlashMode flash = [FlashModeUtils flashFromString:mode];
  if (self.multiCamera != nil) {
    [self.multiCamera setFlashMode:flash error:error];
  } else {
    [self.camera setFlashMode:flash error:error];
  }
}

- (void)setPhotoSizeSize:(nonnull PreviewSize *)size error:(FlutterError * _Nullable __autoreleasing * _Nonnull)error {
  if (size.width <= 0 || size.height <= 0) {
    *error = [FlutterError errorWithCode:@"NO_SIZE_SET" message:@"width and height must be set" details:nil];
    return;
  }
  
  if (self.camera == nil && self.multiCamera == nil) {
    *error = [FlutterError errorWithCode:@"CAMERA_MUST_BE_INIT" message:@"init must be call before start" details:nil];
    return;
  }
  
  if (self.camera == nil) {
    *error = [FlutterError errorWithCode:@"MULTI_CAMERA_UNSUPPORTED" message:@"this feature is currently not supported with multi camera feature" details:nil];
    return;
  }
  
  [self.camera setCameraPreset:CGSizeMake([size.width floatValue], [size.height floatValue])];
}

- (void)setAspectRatioAspectRatio:(nonnull NSString *)aspectRatio error:(FlutterError * _Nullable __autoreleasing * _Nonnull)error {
  if (aspectRatio == nil || aspectRatio.length <= 0) {
    *error = [FlutterError errorWithCode:@"RATIO_NOT_SET" message:@"a ratio must be set" details:nil];
    return;
  }
  
  if (self.camera == nil && self.multiCamera == nil) {
    *error = [FlutterError errorWithCode:@"CAMERA_MUST_BE_INIT" message:@"init must be call before start" details:nil];
    return;
  }
  
  AspectRatio aspectRatioMode = [AspectRatioUtils convertAspectRatio:aspectRatio];
  if (self.multiCamera != nil) {
    [self.multiCamera setAspectRatio:aspectRatioMode];
  } else {
    [self.camera setAspectRatio:aspectRatioMode];
  }
}

#pragma mark - Preview methods

- (nullable NSArray<PreviewSize *> *)availableSizesWithError:(FlutterError * _Nullable __autoreleasing * _Nonnull)error {
  if (self.camera == nil && self.multiCamera == nil) {
    *error = [FlutterError errorWithCode:@"CAMERA_MUST_BE_INIT" message:@"init must be call before start" details:nil];
    return @[];
  }
  
  if (self.multiCamera != nil) {
    return [CameraQualities captureFormatsForDevice:self.multiCamera.devices.firstObject.device];
  } else {
    return [CameraQualities captureFormatsForDevice:self.camera.captureDevice];
  }
}

- (void)setPreviewSizeSize:(nonnull PreviewSize *)size error:(FlutterError * _Nullable __autoreleasing * _Nonnull)error {
  if (size.width <= 0 || size.height <= 0) {
    *error = [FlutterError errorWithCode:@"NO_SIZE_SET" message:@"width and height must be set" details:nil];
    return;
  }
  
  if (self.camera == nil && self.multiCamera == nil) {
    *error = [FlutterError errorWithCode:@"CAMERA_MUST_BE_INIT" message:@"init must be call before start" details:nil];
    return;
  }
  
  if (self.multiCamera != nil) {
    [self.multiCamera setPreviewSize:CGSizeMake([size.width floatValue], [size.height floatValue]) error:error];
  } else {
    [self.camera setPreviewSize:CGSizeMake([size.width floatValue], [size.height floatValue]) error:error];
  }
}

- (nullable PreviewSize *)getEffectivPreviewSizeIndex:(nonnull NSNumber *)index error:(FlutterError * _Nullable __autoreleasing * _Nonnull)error {
  if (self.camera == nil && self.multiCamera == nil) {
    *error = [FlutterError errorWithCode:@"CAMERA_MUST_BE_INIT" message:@"init must be call before start" details:nil];
  }
  
  CGSize previewSize;
  if (self.multiCamera != nil) {
    previewSize = [self.multiCamera getEffectivPreviewSize];
  } else {
    previewSize = [self.camera getEffectivPreviewSize];
  }
  
  // height & width are inverted, this is intentionnal, because camera is always on portrait mode
  return [PreviewSize makeWithWidth:@(previewSize.height) height:@(previewSize.width)];
}

#pragma mark - Zoom methods

- (nullable NSNumber *)getMaxZoomWithError:(FlutterError * _Nullable __autoreleasing * _Nonnull)error {
  if (self.camera == nil && self.multiCamera == nil) {
    *error = [FlutterError errorWithCode:@"CAMERA_MUST_BE_INIT" message:@"init must be call before start" details:nil];
  }
  
  if (self.multiCamera != nil) {
    return @([self.multiCamera getMaxZoom]);
  } else {
    return @([self.camera getMaxZoom]);
  }
}

- (nullable NSNumber *)getMinZoomWithError:(FlutterError * _Nullable __autoreleasing * _Nonnull)error {
  return @(0);
}

- (void)setZoomZoom:(nonnull NSNumber *)zoom error:(FlutterError * _Nullable __autoreleasing * _Nonnull)error {
  if (self.camera == nil && self.multiCamera == nil) {
    *error = [FlutterError errorWithCode:@"CAMERA_MUST_BE_INIT" message:@"init must be call before start" details:nil];
    return;
  }
  
  if (self.multiCamera != nil) {
    [self.multiCamera setZoom:[zoom floatValue] error:error];
  } else {
    [self.camera setZoom:[zoom floatValue] error:error];
  }
}

#pragma mark - Image stream methods

- (void)receivedImageFromStreamWithError:(FlutterError *_Nullable *_Nonnull)error {
  if (self.camera == nil && self.multiCamera == nil) {
    *error = [FlutterError errorWithCode:@"CAMERA_MUST_BE_INIT" message:@"init must be call before start" details:nil];
    return;
  }
  
  if (self.camera == nil) {
    *error = [FlutterError errorWithCode:@"MULTI_CAMERA_UNSUPPORTED" message:@"this feature is currently not supported with multi camera feature" details:nil];
    return;
  }
  
  [self.camera receivedImageFromStream];
}

- (void)setupImageAnalysisStreamFormat:(nonnull NSString *)format width:(nonnull NSNumber *)width maxFramesPerSecond:(nullable NSNumber *)maxFramesPerSecond autoStart:(nonnull NSNumber *)autoStart error:(FlutterError * _Nullable __autoreleasing * _Nonnull)error {
  if (self.camera == nil && self.multiCamera == nil) {
    *error = [FlutterError errorWithCode:@"CAMERA_MUST_BE_INIT" message:@"init must be call before start" details:nil];
    return;
  }
  
  if (self.camera == nil) {
    *error = [FlutterError errorWithCode:@"MULTI_CAMERA_UNSUPPORTED" message:@"this feature is currently not supported with multi camera feature" details:nil];
    return;
  }
  
  [self.camera.imageStreamController setStreamImages:autoStart];
  
  // Force a frame rate to improve performance
  [self.camera.imageStreamController setMaxFramesPerSecond:[maxFramesPerSecond floatValue]];
}

- (void)startAnalysisWithError:(FlutterError * _Nullable __autoreleasing * _Nonnull)error {
  if (self.camera == nil && self.multiCamera == nil) {
    *error = [FlutterError errorWithCode:@"CAMERA_MUST_BE_INIT" message:@"init must be call before start" details:nil];
    return;
  }
  
  if (self.camera == nil) {
    *error = [FlutterError errorWithCode:@"MULTI_CAMERA_UNSUPPORTED" message:@"this feature is currently not supported with multi camera feature" details:nil];
    return;
  }
  
  [self.camera.imageStreamController setStreamImages:true];
}

- (void)stopAnalysisWithError:(FlutterError * _Nullable __autoreleasing * _Nonnull)error {
  if (self.camera == nil && self.multiCamera == nil) {
    *error = [FlutterError errorWithCode:@"CAMERA_MUST_BE_INIT" message:@"init must be call before start" details:nil];
    return;
  }
  
  if (self.camera == nil) {
    *error = [FlutterError errorWithCode:@"MULTI_CAMERA_UNSUPPORTED" message:@"this feature is currently not supported with multi camera feature" details:nil];
    return;
  }
  
  [self.camera.imageStreamController setStreamImages:false];
}

- (void)isVideoRecordingAndImageAnalysisSupportedSensor:(PigeonSensorPosition)sensor completion:(void (^)(NSNumber *_Nullable, FlutterError *_Nullable))completion {
  completion(@(YES), nil);
}

#pragma mark - Sensors methods

- (nullable NSArray<PigeonSensorTypeDevice *> *)getFrontSensorsWithError:(FlutterError *_Nullable *_Nonnull)error {
  return [SensorsController getSensors:AVCaptureDevicePositionFront];
}

- (nullable NSArray<PigeonSensorTypeDevice *> *)getBackSensorsWithError:(FlutterError *_Nullable *_Nonnull)error {
  return [SensorsController getSensors:AVCaptureDevicePositionBack];
}

- (void)setSensorSensors:(nonnull NSArray<PigeonSensor *> *)sensors error:(FlutterError * _Nullable __autoreleasing * _Nonnull)error {
  if (self.camera == nil && self.multiCamera == nil) {
    *error = [FlutterError errorWithCode:@"CAMERA_MUST_BE_INIT" message:@"init must be call before start" details:nil];
    return;
  }
  
  if (sensors != nil && [sensors count] > 1 && self.multiCamera != nil) {
    if ([self.multiCamera.sensors count] != [sensors count]) {
      *error = [FlutterError errorWithCode:@"SENSORS_COUNT_INVALID" message:@"sensors count seems to be different, you can only update current sensors, adding or deleting is impossible for now" details:nil];
      return;
    }
    
    [self.multiCamera setSensors:sensors];
  } else {
    [self.camera setSensor:sensors.firstObject];
  }
}

#pragma mark - Filter methods

- (void)setFilterMatrix:(NSArray<NSNumber *> *)matrix error:(FlutterError *_Nullable *_Nonnull)error {
  // TODO: try to use CIFilter when taking a picture
}

#pragma mark - Multi camera methods

- (nullable NSNumber *)isMultiCamSupportedWithError:(FlutterError * _Nullable __autoreleasing * _Nonnull)error {
  return [NSNumber numberWithBool: [MultiCameraController isMultiCamSupported]];
}

- (void)bgra8888toJpegBgra8888image:(nonnull AnalysisImageWrapper *)bgra8888image jpegQuality:(nonnull NSNumber *)jpegQuality completion:(nonnull void (^)(AnalysisImageWrapper * _Nullable, FlutterError * _Nullable))completion {
  dispatch_async(_dispatchQueueAnalysis, ^{
    [AnalysisController bgra8888toJpegBgra8888image:bgra8888image jpegQuality:jpegQuality completion:completion];
  });
}

- (void)nv21toJpegNv21Image:(nonnull AnalysisImageWrapper *)nv21Image jpegQuality:(nonnull NSNumber *)jpegQuality completion:(nonnull void (^)(AnalysisImageWrapper * _Nullable, FlutterError * _Nullable))completion {
  [AnalysisController nv21toJpegNv21Image:nv21Image jpegQuality:jpegQuality completion:completion];
}

- (void)yuv420toJpegYuvImage:(nonnull AnalysisImageWrapper *)yuvImage jpegQuality:(nonnull NSNumber *)jpegQuality completion:(nonnull void (^)(AnalysisImageWrapper * _Nullable, FlutterError * _Nullable))completion {
  [AnalysisController yuv420toJpegYuvImage:yuvImage jpegQuality:jpegQuality completion:completion];
}

- (void)yuv420toNv21YuvImage:(nonnull AnalysisImageWrapper *)yuvImage completion:(nonnull void (^)(AnalysisImageWrapper * _Nullable, FlutterError * _Nullable))completion {
  [AnalysisController yuv420toNv21YuvImage:yuvImage completion:completion];
}

@end



================================================
FILE: ios/camerawesome/Sources/camerawesome/PrivacyInfo.xcprivacy
================================================
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
	<key>NSPrivacyTrackingDomains</key>
	<array/>
	<key>NSPrivacyAccessedAPITypes</key>
	<array/>
	<key>NSPrivacyCollectedDataTypes</key>
	<array/>
	<key>NSPrivacyTracking</key>
	<false/>
</dict>
</plist>



================================================
FILE: ios/camerawesome/Sources/camerawesome/CameraPreview/CameraDeviceInfo/CameraDeviceInfo.m
================================================
//
//  CameraDeviceInfo.m
//  camerawesome
//
//  Created by Dimitri Dessus on 29/03/2023.
//

#import "CameraDeviceInfo.h"

@implementation CameraDeviceInfo

@end



================================================
FILE: ios/camerawesome/Sources/camerawesome/CameraPreview/CameraPreviewTexture/CameraPreviewTexture.m
================================================
//
//  CameraPreviewTexture.m
//  camerawesome
//
//  Created by Dimitri Dessus on 28/03/2023.
//

#import "CameraPreviewTexture.h"

@implementation CameraPreviewTexture

- (instancetype)init {
  if (self = [super init]) {
    
  }
  
  return self;
}

- (void)updateBuffer:(CMSampleBufferRef)sampleBuffer {
  // TODO: add Atomic(...)
  CVPixelBufferRef newBuffer = CMSampleBufferGetImageBuffer(sampleBuffer);
  CFRetain(newBuffer);
  CVPixelBufferRef old = atomic_load(&_latestPixelBuffer);
  while (!atomic_compare_exchange_strong(&_latestPixelBuffer, &old, newBuffer)) {
    old = atomic_load(&_latestPixelBuffer);
  }
  if (old != nil) {
    CFRelease(old);
  }
}

/// Used to copy pixels to in-memory buffer
- (CVPixelBufferRef _Nullable)copyPixelBuffer {
  CVPixelBufferRef pixelBuffer = atomic_load(&_latestPixelBuffer);
  while (!atomic_compare_exchange_strong(&_latestPixelBuffer, &pixelBuffer, nil)) {
    pixelBuffer = atomic_load(&_latestPixelBuffer);
  }
  
  return pixelBuffer;
}

- (void)dealloc {
  if (self.latestPixelBuffer) {
    CFRelease(self.latestPixelBuffer);
  }
}

@end



================================================
FILE: ios/camerawesome/Sources/camerawesome/CameraPreview/MultiCameraPreview/MultiCameraPreview.m
================================================
//
//  MultiCameraPreview.m
//  camerawesome
//
//  Created by Dimitri Dessus on 28/03/2023.
//

#import "MultiCameraPreview.h"

@implementation MultiCameraPreview

- (instancetype)initWithSensors:(NSArray<PigeonSensor *> *)sensors
              mirrorFrontCamera:(BOOL)mirrorFrontCamera
           enablePhysicalButton:(BOOL)enablePhysicalButton
                aspectRatioMode:(AspectRatio)aspectRatioMode
                    captureMode:(CaptureModes)captureMode
                  dispatchQueue:(dispatch_queue_t)dispatchQueue {
  if (self = [super init]) {
    _dispatchQueue = dispatchQueue;
    
    _textures = [NSMutableArray new];
    _devices = [NSMutableArray new];
    
    _aspectRatio = aspectRatioMode;
    _mirrorFrontCamera = mirrorFrontCamera;
    
    _motionController = [[MotionController alloc] init];
    _locationController = [[LocationController alloc] init];
    _physicalButtonController = [[PhysicalButtonController alloc] init];
    
    if (enablePhysicalButton) {
      [_physicalButtonController startListening];
    }
    
    [_motionController startMotionDetection];
    
    [self configInitialSession:sensors];
  }
  
  return self;
}

/// Set orientation stream Flutter sink
- (void)setOrientationEventSink:(FlutterEventSink)orientationEventSink {
  if (_motionController != nil) {
    [_motionController setOrientationEventSink:orientationEventSink];
  }
}

/// Set physical button Flutter sink
- (void)setPhysicalButtonEventSink:(FlutterEventSink)physicalButtonEventSink {
  if (_physicalButtonController != nil) {
    [_physicalButtonController setPhysicalButtonEventSink:physicalButtonEventSink];
  }
}

- (void)dispose {
  [self stop];
  [self cleanSession];
}

- (void)stop {
  [self.cameraSession stopRunning];
}

- (void)cleanSession {
  [self.cameraSession beginConfiguration];
  
  for (CameraDeviceInfo *camera in self.devices) {
    [self.cameraSession removeConnection:camera.captureConnection];
    [self.cameraSession removeInput:camera.deviceInput];
    [self.cameraSession removeOutput:camera.videoDataOutput];
  }
  
  [self.devices removeAllObjects];
}

// Get max zoom level
- (CGFloat)getMaxZoom {
  CGFloat maxZoom = self.devices.firstObject.device.activeFormat.videoMaxZoomFactor;
  // Not sure why on iPhone 14 Pro, zoom at 90 not working, so let's block to 50 which is very high
  return maxZoom > 50.0 ? 50.0 : maxZoom;
}

/// Set zoom level
- (void)setZoom:(float)value error:(FlutterError * _Nullable __autoreleasing * _Nonnull)error {
  AVCaptureDevice *mainDevice = self.devices.firstObject.device;
  
  CGFloat maxZoom = [self getMaxZoom];
  CGFloat scaledZoom = value * (maxZoom - 1.0f) + 1.0f;
  
  NSError *zoomError;
  if ([mainDevice lockForConfiguration:&zoomError]) {
    mainDevice.videoZoomFactor = scaledZoom;
    [mainDevice unlockForConfiguration];
  } else {
    *error = [FlutterError errorWithCode:@"ZOOM_NOT_SET" message:@"can't set the zoom value" details:[zoomError localizedDescription]];
  }
}

- (void)focusOnPoint:(CGPoint)position preview:(CGSize)preview error:(FlutterError * _Nullable __autoreleasing * _Nonnull)error {
  AVCaptureDevice *mainDevice = self.devices.firstObject.device;
  NSError *lockError;
  if ([mainDevice isFocusModeSupported:AVCaptureFocusModeAutoFocus] && [mainDevice isFocusPointOfInterestSupported]) {
    if ([mainDevice lockForConfiguration:&lockError]) {
      if (lockError != nil) {
        *error = [FlutterError errorWithCode:@"FOCUS_ERROR" message:@"impossible to set focus point" details:@""];
        return;
      }
      
      [mainDevice setFocusPointOfInterest:position];
      [mainDevice setFocusMode:AVCaptureFocusModeContinuousAutoFocus];
      
      [mainDevice unlockForConfiguration];
    }
  }
}

- (void)setExifPreferencesGPSLocation:(bool)gpsLocation completion:(void(^)(NSNumber *_Nullable, FlutterError *_Nullable))completion {
  _saveGPSLocation = gpsLocation;
  
  if (_saveGPSLocation) {
    [_locationController requestWhenInUseAuthorizationOnGranted:^{
      completion(@(YES), nil);
    } declined:^{
      completion(@(NO), nil);
    }];
  } else {
    completion(@(YES), nil);
  }
}

- (void)setMirrorFrontCamera:(bool)value error:(FlutterError * _Nullable __autoreleasing * _Nonnull)error {
  _mirrorFrontCamera = value;
}

- (void)setBrightness:(NSNumber *)brightness error:(FlutterError * _Nullable __autoreleasing * _Nonnull)error {
  AVCaptureDevice *mainDevice = self.devices.firstObject.device;
  NSError *brightnessError = nil;
  if ([mainDevice lockForConfiguration:&brightnessError]) {
    AVCaptureExposureMode exposureMode = AVCaptureExposureModeContinuousAutoExposure;
    if ([mainDevice isExposureModeSupported:exposureMode]) {
      [mainDevice setExposureMode:exposureMode];
    }
    
    CGFloat minExposureTargetBias = mainDevice.minExposureTargetBias;
    CGFloat maxExposureTargetBias = mainDevice.maxExposureTargetBias;
    
    CGFloat exposureTargetBias = minExposureTargetBias + (maxExposureTargetBias - minExposureTargetBias) * [brightness floatValue];
    exposureTargetBias = MAX(minExposureTargetBias, MIN(maxExposureTargetBias, exposureTargetBias));
    
    [mainDevice setExposureTargetBias:exposureTargetBias completionHandler:nil];
    [mainDevice unlockForConfiguration];
  } else {
    *error = [FlutterError errorWithCode:@"BRIGHTNESS_NOT_SET" message:@"can't set the brightness value" details:[brightnessError localizedDescription]];
  }
}

/// Set flash mode
- (void)setFlashMode:(CameraFlashMode)flashMode error:(FlutterError * _Nullable __autoreleasing * _Nonnull)error {
  AVCaptureDevice *mainDevice = self.devices.firstObject.device;
  
  if (![mainDevice hasFlash]) {
    *error = [FlutterError errorWithCode:@"FLASH_UNSUPPORTED" message:@"flash is not supported on this device" details:@""];
    return;
  }
  
  if (mainDevice.position == AVCaptureDevicePositionFront) {
    *error = [FlutterError errorWithCode:@"FLASH_UNSUPPORTED" message:@"can't set flash for portrait mode" details:@""];
    return;
  }
  
  NSError *lockError;
  [self.devices.firstObject.device lockForConfiguration:&lockError];
  if (lockError != nil) {
    *error = [FlutterError errorWithCode:@"FLASH_ERROR" message:@"impossible to change configuration" details:@""];
    return;
  }
  
  switch (flashMode) {
    case None:
      _torchMode = AVCaptureTorchModeOff;
      _flashMode = AVCaptureFlashModeOff;
      break;
    case On:
      _torchMode = AVCaptureTorchModeOff;
      _flashMode = AVCaptureFlashModeOn;
      break;
    case Auto:
      _torchMode = AVCaptureTorchModeAuto;
      _flashMode = AVCaptureFlashModeAuto;
      break;
    case Always:
      _torchMode = AVCaptureTorchModeOn;
      _flashMode = AVCaptureFlashModeOn;
      break;
    default:
      _torchMode = AVCaptureTorchModeAuto;
      _flashMode = AVCaptureFlashModeAuto;
      break;
  }
  
  [mainDevice setTorchMode:_torchMode];
  [mainDevice unlockForConfiguration];
}

- (void)refresh {
  if ([self.cameraSession isRunning]) {
    [self.cameraSession stopRunning];
  }
  [self.cameraSession startRunning];
}

- (void)configInitialSession:(NSArray<PigeonSensor *> *)sensors {  
  self.cameraSession = [[AVCaptureMultiCamSession alloc] init];
  
  for (int i = 0; i < [sensors count]; i++) {
    CameraPreviewTexture *previewTexture = [[CameraPreviewTexture alloc] init];
    [self.textures addObject:previewTexture];
  }
  
  [self setSensors:sensors];
  
  [self.cameraSession commitConfiguration];
}

- (void)setSensors:(NSArray<PigeonSensor *> *)sensors {
  [self cleanSession];
  
  _sensors = sensors;
  
  for (int i = 0; i < [sensors count]; i++) {
    PigeonSensor *sensor = sensors[i];
    [self addSensor:sensor withIndex:i];
  }
  
  [self.cameraSession commitConfiguration];
}

- (void)start {
  [self.cameraSession startRunning];
}

- (CGSize)getEffectivPreviewSize {
  // TODO
  return CGSizeMake(1920, 1080);
}

- (BOOL)addSensor:(PigeonSensor *)sensor withIndex:(int)index {
  AVCaptureDevice *device = [self selectAvailableCamera:sensor];;
  
  if (device == nil) {
    return NO;
  }
  
  NSError *error = nil;
  AVCaptureDeviceInput *deviceInput = [[AVCaptureDeviceInput alloc] initWithDevice:device error:&error];
  if (![self.cameraSession canAddInput:deviceInput]) {
    return NO;
  }
  [self.cameraSession addInputWithNoConnections:deviceInput];
  
  AVCaptureVideoDataOutput *videoDataOutput = [[AVCaptureVideoDataOutput alloc] init];
  videoDataOutput.videoSettings = @{(__bridge NSString *)kCVPixelBufferPixelFormatTypeKey : @(kCVPixelFormatType_32BGRA)};
  [videoDataOutput setSampleBufferDelegate:self queue:self.dispatchQueue];
  
  if (![self.cameraSession canAddOutput:videoDataOutput]) {
    return NO;
  }
  [self.cameraSession addOutputWithNoConnections:videoDataOutput];
  
  AVCaptureInputPort *port = [[deviceInput portsWithMediaType:AVMediaTypeVideo
                                             sourceDeviceType:device.deviceType
                                         sourceDevicePosition:device.position] firstObject];
  AVCaptureConnection *captureConnection = [[AVCaptureConnection alloc] initWithInputPorts:@[port] output:videoDataOutput];
  
  if (![self.cameraSession canAddConnection:captureConnection]) {
    return NO;
  }
  [self.cameraSession addConnection:captureConnection];
  
  [captureConnection setVideoOrientation:AVCaptureVideoOrientationPortrait];
  [captureConnection setAutomaticallyAdjustsVideoMirroring:NO];
  [captureConnection setVideoMirrored:sensor.position == PigeonSensorPositionFront];
  
  // Creating photo output
  AVCapturePhotoOutput *capturePhotoOutput = [AVCapturePhotoOutput new];
  [capturePhotoOutput setHighResolutionCaptureEnabled:YES];
  [self.cameraSession addOutput:capturePhotoOutput];
  
  // move this all this in the cameradevice object
  CameraDeviceInfo *cameraDevice = [[CameraDeviceInfo alloc] init];
  cameraDevice.captureConnection = captureConnection;
  cameraDevice.deviceInput = deviceInput;
  cameraDevice.videoDataOutput = videoDataOutput;
  cameraDevice.device = device;
  cameraDevice.capturePhotoOutput = capturePhotoOutput;
  
  [_devices addObject:cameraDevice];
  
  return YES;
}

/// Get the first available camera on device (front or rear)
- (AVCaptureDevice *)selectAvailableCamera:(PigeonSensor *)sensor {
  if (sensor.deviceId != nil) {
    return [AVCaptureDevice deviceWithUniqueID:sensor.deviceId];
  }
  
  // TODO: add dual & triple camera
  NSArray<AVCaptureDevice *> *devices = [[NSArray alloc] init];
  AVCaptureDeviceDiscoverySession *discoverySession = [AVCaptureDeviceDiscoverySession discoverySessionWithDeviceTypes:@[ AVCaptureDeviceTypeBuiltInWideAngleCamera, AVCaptureDeviceTypeBuiltInTelephotoCamera, AVCaptureDeviceTypeBuiltInUltraWideCamera, ]
                                                                                                             mediaType:AVMediaTypeVideo
                                                                                                              position:AVCaptureDevicePositionUnspecified];
  devices = discoverySession.devices;
  
  for (AVCaptureDevice *device in devices) {
    if (sensor.type != PigeonSensorTypeUnknown) {
      AVCaptureDeviceType deviceType = [SensorUtils deviceTypeFromSensorType:sensor.type];
      if ([device deviceType] == deviceType) {
        return [AVCaptureDevice deviceWithUniqueID:[device uniqueID]];
      }
    } else if (sensor.position != PigeonSensorPositionUnknown) {
      NSInteger cameraType = (sensor.position == PigeonSensorPositionFront) ? AVCaptureDevicePositionFront : AVCaptureDevicePositionBack;
      if ([device position] == cameraType) {
        return [AVCaptureDevice deviceWithUniqueID:[device uniqueID]];
      }
    }
  }
  return nil;
}

- (void)setAspectRatio:(AspectRatio)ratio {
  _aspectRatio = ratio;
}

- (void)setPreviewSize:(CGSize)previewSize error:(FlutterError * _Nullable __autoreleasing * _Nonnull)error {
  // TODO:
}

- (void)takePhotoSensors:(nonnull NSArray<PigeonSensor *> *)sensors paths:(nonnull NSArray<NSString *> *)paths completion:(nonnull void (^)(NSNumber * _Nullable, FlutterError * _Nullable))completion {
  for (int i = 0; i < [sensors count]; i++) {
    PigeonSensor *sensor = [sensors objectAtIndex:i];
    NSString *path = [paths objectAtIndex:i];
    
    // TODO: take pictures for each sensors
    CameraPictureController *cameraPicture = [[CameraPictureController alloc] initWithPath:path
                                                                               orientation:_motionController.deviceOrientation
                                                                            sensorPosition:sensor.position
                                                                           saveGPSLocation:_saveGPSLocation
                                                                         mirrorFrontCamera:_mirrorFrontCamera
                                                                               aspectRatio:_aspectRatio
                                                                                completion:completion
                                                                                  callback:^{
      // If flash mode is always on, restore it back after photo is taken
      if (self->_torchMode == AVCaptureTorchModeOn) {
        [self->_devices.firstObject.device lockForConfiguration:nil];
        [self->_devices.firstObject.device setTorchMode:AVCaptureTorchModeOn];
        [self->_devices.firstObject.device unlockForConfiguration];
      }
      
      completion(@(YES), nil);
    }];
    
    // Create settings instance
    AVCapturePhotoSettings *settings = [AVCapturePhotoSettings photoSettings];
    [settings setHighResolutionPhotoEnabled:YES];
    [self.devices[i].capturePhotoOutput setPhotoSettingsForSceneMonitoring:settings];
    
    [self.devices[i].capturePhotoOutput capturePhotoWithSettings:settings
                                                        delegate:cameraPicture];
  }
}

- (void)captureOutput:(AVCaptureOutput *)output didOutputSampleBuffer:(CMSampleBufferRef)sampleBuffer fromConnection:(AVCaptureConnection *)connection {
  int index = 0;
  for (CameraDeviceInfo *device in _devices) {
    if (device.videoDataOutput == output) {
      [_textures[index] updateBuffer:sampleBuffer];
      if (_onPreviewFrameAvailable) {
        _onPreviewFrameAvailable(@(index));
      }
    }
    
    index++;
  }
}

@end



================================================
FILE: ios/camerawesome/Sources/camerawesome/CameraPreview/SingleCameraPreview/SingleCameraPreview.m
================================================
//
//  CameraPreview.m
//  camerawesome
//
//  Created by Dimitri Dessus on 23/07/2020.
//

#import "SingleCameraPreview.h"

@implementation SingleCameraPreview {
  dispatch_queue_t _dispatchQueue;
}

- (instancetype)initWithCameraSensor:(PigeonSensorPosition)sensor
                        videoOptions:(nullable CupertinoVideoOptions *)videoOptions
                    recordingQuality:(VideoRecordingQuality)recordingQuality
                        streamImages:(BOOL)streamImages
                   mirrorFrontCamera:(BOOL)mirrorFrontCamera
                enablePhysicalButton:(BOOL)enablePhysicalButton
                     aspectRatioMode:(AspectRatio)aspectRatioMode
                         captureMode:(CaptureModes)captureMode
                          completion:(nonnull void (^)(NSNumber * _Nullable, FlutterError * _Nullable))completion
                       dispatchQueue:(dispatch_queue_t)dispatchQueue {
  self = [super init];
  
  _completion = completion;
  _dispatchQueue = dispatchQueue;
  
  _previewTexture = [[CameraPreviewTexture alloc] init];
  
  _cameraSensorPosition = sensor;
  _aspectRatio = aspectRatioMode;
  _mirrorFrontCamera = mirrorFrontCamera;
  _videoOptions = videoOptions;
  _recordingQuality = recordingQuality;
  
  // Creating capture session
  _captureSession = [[AVCaptureSession alloc] init];
  _captureVideoOutput = [AVCaptureVideoDataOutput new];
  _captureVideoOutput.videoSettings = @{(NSString*)kCVPixelBufferPixelFormatTypeKey: @(kCVPixelFormatType_32BGRA)};
  [_captureVideoOutput setAlwaysDiscardsLateVideoFrames:YES];
  [_captureVideoOutput setSampleBufferDelegate:self queue:dispatch_get_main_queue()];
  [_captureSession addOutputWithNoConnections:_captureVideoOutput];
  
  [self initCameraPreview:sensor];
  
  [_captureConnection setAutomaticallyAdjustsVideoMirroring:NO];
  if (mirrorFrontCamera && [_captureConnection isVideoMirroringSupported]) {
    [_captureConnection setVideoMirrored:mirrorFrontCamera];
  }
  
  _captureMode = captureMode;
  
  // By default enable auto flash mode
  _flashMode = AVCaptureFlashModeOff;
  _torchMode = AVCaptureTorchModeOff;
  
  _previewLayer = [AVCaptureVideoPreviewLayer layerWithSession:_captureSession];
  _previewLayer.videoGravity = AVLayerVideoGravityResizeAspectFill;
  
  // Controllers init
  _videoController = [[VideoController alloc] init];
  _imageStreamController = [[ImageStreamController alloc] initWithStreamImages:streamImages];
  _motionController = [[MotionController alloc] init];
  _locationController = [[LocationController alloc] init];
  _physicalButtonController = [[PhysicalButtonController alloc] init];
  
  [_motionController startMotionDetection];
  
  if (enablePhysicalButton) {
    [_physicalButtonController startListening];
  }
  
  [self setBestPreviewQuality];
  
  return self;
}

- (void)setAspectRatio:(AspectRatio)ratio {
  _aspectRatio = ratio;
}

/// Set image stream Flutter sink
- (void)setImageStreamEvent:(FlutterEventSink)imageStreamEventSink {
  if (_imageStreamController != nil) {
    [_imageStreamController setImageStreamEventSink:imageStreamEventSink];
  }
}

/// Set orientation stream Flutter sink
- (void)setOrientationEventSink:(FlutterEventSink)orientationEventSink {
  if (_motionController != nil) {
    [_motionController setOrientationEventSink:orientationEventSink];
  }
}

/// Set physical button Flutter sink
- (void)setPhysicalButtonEventSink:(FlutterEventSink)physicalButtonEventSink {
  if (_physicalButtonController != nil) {
    [_physicalButtonController setPhysicalButtonEventSink:physicalButtonEventSink];
  }
}

// TODO: move this to a QualityController
/// Assign the default preview qualities
- (void)setBestPreviewQuality {
  NSArray *qualities = [CameraQualities captureFormatsForDevice:_captureDevice];
  PreviewSize *firstPreviewSize = [qualities count] > 0 ? qualities.lastObject : [PreviewSize makeWithWidth:@3840 height:@2160];
  
  CGSize firstSize = CGSizeMake([firstPreviewSize.width floatValue], [firstPreviewSize.height floatValue]);
  [self setCameraPreset:firstSize];
}

/// Save exif preferences when taking picture
- (void)setExifPreferencesGPSLocation:(bool)gpsLocation completion:(void(^)(NSNumber *_Nullable, FlutterError *_Nullable))completion {
  _saveGPSLocation = gpsLocation;
  
  if (_saveGPSLocation) {
    [_locationController requestWhenInUseAuthorizationOnGranted:^{
      completion(@(YES), nil);
    } declined:^{
      completion(@(NO), nil);
    }];
  } else {
    completion(@(YES), nil);
  }
}

/// Init camera preview with Front or Rear sensor
- (void)initCameraPreview:(PigeonSensorPosition)sensor {
  // Here we set a preset which wont crash the device before switching to front or back
  [_captureSession setSessionPreset:AVCaptureSessionPresetPhoto];
  
  NSError *error;
  _captureDevice = [AVCaptureDevice deviceWithUniqueID:[self selectAvailableCamera:sensor]];
  _captureVideoInput = [AVCaptureDeviceInput deviceInputWithDevice:_captureDevice error:&error];
  
  if (error != nil) {
    _completion(nil, [FlutterError errorWithCode:@"CANNOT_OPEN_CAMERA" message:@"can't attach device to input" details:[error localizedDescription]]);
    return;
  }
  
  // Create connection
  _captureConnection = [AVCaptureConnection connectionWithInputPorts:_captureVideoInput.ports
                                                              output:_captureVideoOutput];
  
  // TODO: works but deprecated...
  //  if ([_captureConnection isVideoMinFrameDurationSupported] && [_captureConnection isVideoMaxFrameDurationSupported]) {
  //    CMTime frameDuration = CMTimeMake(1, 12);
  //    [_captureConnection setVideoMinFrameDuration:frameDuration];
  //    [_captureConnection setVideoMaxFrameDuration:frameDuration];
  //  } else {
  //    NSLog(@"Failed to set frame duration");
  //  }
  
  // Attaching to session
  [_captureSession addInputWithNoConnections:_captureVideoInput];
  [_captureSession addConnection:_captureConnection];
  
  // Creating photo output
  _capturePhotoOutput = [AVCapturePhotoOutput new];
  [_capturePhotoOutput setHighResolutionCaptureEnabled:YES];
  [_captureSession addOutput:_capturePhotoOutput];
  
  // Mirror the preview only on portrait mode
  [_captureConnection setAutomaticallyAdjustsVideoMirroring:NO];
  [_captureConnection setVideoMirrored:(_cameraSensorPosition == PigeonSensorPositionFront)];
  [_captureConnection setVideoOrientation:AVCaptureVideoOrientationPortrait];
}

- (void)dealloc {
  [self.motionController startMotionDetection];
}

/// Set camera preview size
- (void)setCameraPreset:(CGSize)currentPreviewSize {
  CGSize targetSize = currentPreviewSize;

  // Determine the target size based on the current mode and settings
  if (_captureMode == Video || _videoController.isRecording) {
      // If recording video, prioritize the recording quality setting
      // TODO: Need a way to get the CGSize from the _recordingQuality enum or _videoOptions
      // For now, let's assume a helper function or default high quality if direct mapping isn't obvious.
      // Placeholder: If video options exist, try to use them, otherwise fall back.
      // If no direct mapping, maybe use the highest available preset suitable for video?
      // Or just pass CGSizeZero to let selectVideoCapturePreset pick the best for video?
      // For now, let's pass CGSizeZero to select the best default for video capture.
      if (_videoOptions != nil) {
         // Hypothetical: Get size from VideoOptions quality. Needs actual implementation.
         // targetSize = [CameraQualities sizeFromQuality:_recordingQuality];
         // If no direct mapping, maybe use the highest available preset suitable for video?
         // Or just pass CGSizeZero to let selectVideoCapturePreset pick the best for video?
         // For now, let's pass CGSizeZero to select the best default for video capture.
         targetSize = CGSizeZero; 
      } else if (!CGSizeEqualToSize(currentPreviewSize, CGSizeZero)){
         // Use provided size if valid and no video options
         targetSize = currentPreviewSize;
      } else {
         // Fallback to best quality if no specific size or options given
         targetSize = CGSizeZero;
      }
  } else if (_imageStreamController.streamImages) {
      // If only streaming (not recording), force 720p for potential stability (based on commit history)
      targetSize = CGSizeMake(720, 1280);
  } else if (CGSizeEqualToSize(currentPreviewSize, CGSizeZero)) {
      // If neither recording nor streaming, and no size provided, use best quality
      targetSize = CGSizeZero;
  } 
  // else: Use the non-zero currentPreviewSize passed in.

  NSString *presetSelected;
  if (!CGSizeEqualToSize(CGSizeZero, targetSize)) {
    // Try to get the quality requested based on the determined target size
    presetSelected = [CameraQualities selectVideoCapturePreset:targetSize session:_captureSession device:_captureDevice];
  } else {
    // Compute the best quality supported by the camera device if targetSize is Zero
    presetSelected = [CameraQualities selectVideoCapturePreset:_captureSession device:_captureDevice];
  }

  // Check if the preset needs to be changed
  if (![_captureSession.sessionPreset isEqualToString:presetSelected]) {
      // Check if the session is running before changing the preset
      BOOL sessionIsRunning = _captureSession.isRunning;
      if (sessionIsRunning) {
          [_captureSession stopRunning];
      }

      [_captureSession setSessionPreset:presetSelected];
      _currentPreset = presetSelected;

      if (sessionIsRunning) {
          [_captureSession startRunning];
      }
  } else {
      _currentPreset = _captureSession.sessionPreset;
  }

  // Use the corrected method name
  _currentPreviewSize = [CameraQualities getSizeForPreset:_currentPreset];

  [_videoController setPreviewSize:_currentPreviewSize];
}

/// Get current video prewiew size
- (CGSize)getEffectivPreviewSize {
  return _currentPreviewSize;
}

// Get max zoom level
- (CGFloat)getMaxZoom {
  CGFloat maxZoom = _captureDevice.activeFormat.videoMaxZoomFactor;
  // Not sure why on iPhone 14 Pro, zoom at 90 not working, so let's block to 50 which is very high
  return maxZoom > 50.0 ? 50.0 : maxZoom;
}

/// Dispose camera inputs & outputs
- (void)dispose {
  [self stop];
  [self.physicalButtonController stopListening];
  
  for (AVCaptureInput *input in [_captureSession inputs]) {
    [_captureSession removeInput:input];
  }
  for (AVCaptureOutput *output in [_captureSession outputs]) {
    [_captureSession removeOutput:output];
  }
}

/// Set preview size resolution
- (void)setPreviewSize:(CGSize)previewSize error:(FlutterError * _Nullable __autoreleasing * _Nonnull)error {
  if (_videoController.isRecording) {
    *error = [FlutterError errorWithCode:@"PREVIEW_SIZE" message:@"impossible to change preview size, video already recording" details:@""];
    return;
  }
  
  [self setCameraPreset:previewSize];
}

/// Start camera preview
- (void)start {
  [_captureSession startRunning];
}

/// Stop camera preview
- (void)stop {
  [_captureSession stopRunning];
}

/// Set sensor between Front & Rear camera
- (void)setSensor:(PigeonSensor *)sensor {
  // First remove all input & output
  [_captureSession beginConfiguration];
  
  // Only remove camera channel but keep audio
  for (AVCaptureInput *input in [_captureSession inputs]) {
    for (AVCaptureInputPort *port in input.ports) {
      if ([[port mediaType] isEqual:AVMediaTypeVideo]) {
        [_captureSession removeInput:input];
        break;
      }
    }
  }
  [_videoController setAudioIsDisconnected:YES];
  
  [_captureSession removeOutput:_capturePhotoOutput];
  [_captureSession removeConnection:_captureConnection];
  
  _cameraSensorPosition = sensor.position;
  _captureDeviceId = sensor.deviceId;
  
  // Init the camera preview with the selected sensor
  [self initCameraPreview:sensor.position];
  
  [self setBestPreviewQuality];
  
  [_captureSession commitConfiguration];
}

/// Set zoom level
- (void)setZoom:(float)value error:(FlutterError * _Nullable __autoreleasing * _Nonnull)error {
  CGFloat maxZoom = [self getMaxZoom];
  CGFloat scaledZoom = value * (maxZoom - 1.0f) + 1.0f;
  
  NSError *zoomError;
  if ([_captureDevice lockForConfiguration:&zoomError]) {
    _captureDevice.videoZoomFactor = scaledZoom;
    [_captureDevice unlockForConfiguration];
  } else {
    *error = [FlutterError errorWithCode:@"ZOOM_NOT_SET" message:@"can't set the zoom value" details:[zoomError localizedDescription]];
  }
}

- (void)setBrightness:(NSNumber *)brightness error:(FlutterError * _Nullable __autoreleasing * _Nonnull)error {
  NSError *brightnessError = nil;
  if ([_captureDevice lockForConfiguration:&brightnessError]) {
    AVCaptureExposureMode exposureMode = AVCaptureExposureModeContinuousAutoExposure;
    if ([_captureDevice isExposureModeSupported:exposureMode]) {
      [_captureDevice setExposureMode:exposureMode];
    }
    
    CGFloat minExposureTargetBias = _captureDevice.minExposureTargetBias;
    CGFloat maxExposureTargetBias = _captureDevice.maxExposureTargetBias;
    
    CGFloat exposureTargetBias = minExposureTargetBias + (maxExposureTargetBias - minExposureTargetBias) * [brightness floatValue];
    exposureTargetBias = MAX(minExposureTargetBias, MIN(maxExposureTargetBias, exposureTargetBias));
    
    [_captureDevice setExposureTargetBias:exposureTargetBias completionHandler:nil];
    [_captureDevice unlockForConfiguration];
  } else {
    *error = [FlutterError errorWithCode:@"BRIGHTNESS_NOT_SET" message:@"can't set the brightness value" details:[brightnessError localizedDescription]];
  }
}

- (void)setMirrorFrontCamera:(bool)value error:(FlutterError * _Nullable __autoreleasing * _Nonnull)error {
  _mirrorFrontCamera = value;
  
  if ([_captureConnection isVideoMirroringSupported]) {
      [_captureConnection setVideoMirrored:value];
  }
}

/// Set flash mode
- (void)setFlashMode:(CameraFlashMode)flashMode error:(FlutterError * _Nullable __autoreleasing * _Nonnull)error {
  if (![_captureDevice hasFlash]) {
    *error = [FlutterError errorWithCode:@"FLASH_UNSUPPORTED" message:@"flash is not supported on this device" details:@""];
    return;
  }
  
  if (_cameraSensorPosition == PigeonSensorPositionFront) {
    *error = [FlutterError errorWithCode:@"FLASH_UNSUPPORTED" message:@"can't set flash for portrait mode" details:@""];
    return;
  }
  
  NSError *lockError;
  [_captureDevice lockForConfiguration:&lockError];
  if (lockError != nil) {
    *error = [FlutterError errorWithCode:@"FLASH_ERROR" message:@"impossible to change configuration" details:@""];
    return;
  }
  
  switch (flashMode) {
    case None:
      _torchMode = AVCaptureTorchModeOff;
      _flashMode = AVCaptureFlashModeOff;
      break;
    case On:
      _torchMode = AVCaptureTorchModeOff;
      _flashMode = AVCaptureFlashModeOn;
      break;
    case Auto:
      _torchMode = AVCaptureTorchModeAuto;
      _flashMode = AVCaptureFlashModeAuto;
      break;
    case Always:
      _torchMode = AVCaptureTorchModeOn;
      _flashMode = AVCaptureFlashModeOn;
      break;
    default:
      _torchMode = AVCaptureTorchModeAuto;
      _flashMode = AVCaptureFlashModeAuto;
      break;
  }
  [_captureDevice setTorchMode:_torchMode];
  [_captureDevice unlockForConfiguration];
}

/// Trigger focus on device at the specific point of the preview
- (void)focusOnPoint:(CGPoint)position preview:(CGSize)preview error:(FlutterError * _Nullable __autoreleasing * _Nonnull)error {
  NSError *lockError;
  if ([_captureDevice isFocusModeSupported:AVCaptureFocusModeAutoFocus] && [_captureDevice isFocusPointOfInterestSupported]) {
    if ([_captureDevice lockForConfiguration:&lockError]) {
      if (lockError != nil) {
        *error = [FlutterError errorWithCode:@"FOCUS_ERROR" message:@"impossible to set focus point" details:@""];
        return;
      }
      
      [_captureDevice setFocusPointOfInterest:position];
      [_captureDevice setFocusMode:AVCaptureFocusModeContinuousAutoFocus];
      
      [_captureDevice unlockForConfiguration];
    }
  }
}

- (void)receivedImageFromStream {
  [self.imageStreamController receivedImageFromStream];
}

/// Get the first available camera on device (front or rear)
- (NSString *)selectAvailableCamera:(PigeonSensorPosition)sensor {
  if (_captureDeviceId != nil) {
    return _captureDeviceId;
  }
  
  // TODO: add dual & triple camera
  NSArray<AVCaptureDevice *> *devices = [[NSArray alloc] init];
  AVCaptureDeviceDiscoverySession *discoverySession = [AVCaptureDeviceDiscoverySession
                                                       discoverySessionWithDeviceTypes:@[ AVCaptureDeviceTypeBuiltInWideAngleCamera, ]
                                                       mediaType:AVMediaTypeVideo
                                                       position:AVCaptureDevicePositionUnspecified];
  devices = discoverySession.devices;
  
  NSInteger cameraType = (sensor == PigeonSensorPositionFront) ? AVCaptureDevicePositionFront : AVCaptureDevicePositionBack;
  for (AVCaptureDevice *device in devices) {
    if ([device position] == cameraType) {
      return [device uniqueID];
    }
  }
  return nil;
}

/// Set capture mode between Photo & Video mode
- (void)setCaptureMode:(CaptureModes)captureMode error:(FlutterError * _Nullable __autoreleasing * _Nonnull)error {
  if (_videoController.isRecording) {
    *error = [FlutterError errorWithCode:@"CAPTURE_MODE" message:@"impossible to change capture mode, video already recording" details:@""];
    return;
  }
  
  _captureMode = captureMode;
  
  if (captureMode == Video) {
    [self setUpCaptureSessionForAudioError:^(NSError *audioError) {
      *error = [FlutterError errorWithCode:@"VIDEO_ERROR" message:@"error when trying to setup audio" details:[audioError localizedDescription]];
    }];
  }
}

- (void)refresh {
  if ([_captureSession isRunning]) {
    [self stop];
  }
  [self start];
}

# pragma mark - Camera picture

/// Take the picture into the given path
- (void)takePictureAtPath:(NSString *)path completion:(nonnull void (^)(NSNumber * _Nullable, FlutterError * _Nullable))completion {
  // Instanciate camera picture obj
  CameraPictureController *cameraPicture = [[CameraPictureController alloc] initWithPath:path
                                                                             orientation:_motionController.deviceOrientation
                                                                          sensorPosition:_cameraSensorPosition
                                                                         saveGPSLocation:_saveGPSLocation
                                                                       mirrorFrontCamera:_mirrorFrontCamera
                                                                             aspectRatio:_aspectRatio
                                                                              completion:completion
                                                                                callback:^{
    // If flash mode is always on, restore it back after photo is taken
    if (self->_torchMode == AVCaptureTorchModeOn) {
      [self->_captureDevice lockForConfiguration:nil];
      [self->_captureDevice setTorchMode:AVCaptureTorchModeOn];
      [self->_captureDevice unlockForConfiguration];
    }
    
    completion(@(YES), nil);
  }];
  
  // Create settings instance
  AVCapturePhotoSettings *settings = [AVCapturePhotoSettings photoSettings];
  [settings setFlashMode:_flashMode];
  [settings setHighResolutionPhotoEnabled:YES];
  
  [_capturePhotoOutput capturePhotoWithSettings:settings
                                       delegate:cameraPicture];
  
}

# pragma mark - Camera video
/// Record video into the given path
- (void)recordVideoAtPath:(NSString *)path completion:(nonnull void (^)(FlutterError * _Nullable))completion {
  if (!_videoController.isRecording) {
    [_videoController recordVideoAtPath:path captureDevice:_captureDevice orientation:_deviceOrientation audioSetupCallback:^{
      [self setUpCaptureSessionForAudioError:^(NSError *error) {
        completion([FlutterError errorWithCode:@"VIDEO_ERROR" message:@"error when trying to setup audio" details:[error localizedDescription]]);
      }];
    } videoWriterCallback:^{
      if (self->_videoController.isAudioEnabled) {
        [self->_audioOutput setSampleBufferDelegate:self queue:self->_dispatchQueue];
      }
      [self->_captureVideoOutput setSampleBufferDelegate:self queue:self->_dispatchQueue];
      
      completion(nil);
    } options:_videoOptions quality: _recordingQuality completion:completion];
  } else {
    completion([FlutterError errorWithCode:@"VIDEO_ERROR" message:@"already recording video" details:@""]);
  }
}

/// Pause video recording
- (void)pauseVideoRecording {
  [_videoController pauseVideoRecording];
}

/// Resume video recording after being paused
- (void)resumeVideoRecording {
  [_videoController resumeVideoRecording];
}

/// Stop recording video
- (void)stopRecordingVideo:(nonnull void (^)(NSNumber * _Nullable, FlutterError * _Nullable))completion {
  if (_videoController.isRecording) {
    [_videoController stopRecordingVideo:completion];
  } else {
    completion(@(NO), [FlutterError errorWithCode:@"VIDEO_ERROR" message:@"video is not recording" details:@""]);
  }
}

/// Set audio recording mode
- (void)setRecordingAudioMode:(bool)isAudioEnabled completion:(void(^)(NSNumber *_Nullable, FlutterError *_Nullable))completion {
  if (_videoController.isRecording) {
    completion(@(NO), [FlutterError errorWithCode:@"CHANGE_AUDIO_MODE" message:@"impossible to change audio mode, video already recording" details:@""]);
    return;
  }
  
  [_captureSession beginConfiguration];
  [_videoController setIsAudioEnabled:isAudioEnabled];
  [_videoController setIsAudioSetup:NO];
  [_videoController setAudioIsDisconnected:YES];
  
  // Only remove audio channel input but keep video
  for (AVCaptureInput *input in [_captureSession inputs]) {
    for (AVCaptureInputPort *port in input.ports) {
      if ([[port mediaType] isEqual:AVMediaTypeAudio]) {
        [_captureSession removeInput:input];
        break;
      }
    }
  }
  // Only remove audio channel output but keep video
  [_captureSession removeOutput:_audioOutput];
  
  if (_videoController.isRecording) {
    [self setUpCaptureSessionForAudioError:^(NSError *error) {
      completion(@(NO), [FlutterError errorWithCode:@"VIDEO_ERROR" message:@"error when trying to setup audio" details:[error localizedDescription]]);
    }];
  }
  
  [_captureSession commitConfiguration];
}

# pragma mark - Audio
/// Setup audio channel to record audio
- (void)setUpCaptureSessionForAudioError:(nonnull void (^)(NSError *))error {
  NSError *audioError = nil;
  // Create a device input with the device and add it to the session.
  // Setup the audio input.
  AVCaptureDevice *audioDevice = [AVCaptureDevice defaultDeviceWithMediaType:AVMediaTypeAudio];
  AVCaptureDeviceInput *audioInput = [AVCaptureDeviceInput deviceInputWithDevice:audioDevice
                                                                           error:&audioError];
  if (audioError) {
    error(audioError);
  }
  
  // Setup the audio output.
  _audioOutput = [[AVCaptureAudioDataOutput alloc] init];
  
  if ([_captureSession canAddInput:audioInput]) {
    [_captureSession addInput:audioInput];
    
    if ([_captureSession canAddOutput:_audioOutput]) {
      [_captureSession addOutput:_audioOutput];
      [_videoController setIsAudioSetup:YES];
    } else {
      [_videoController setIsAudioSetup:NO];
    }
  }
}

# pragma mark - Camera Delegates

- (void)captureOutput:(AVCaptureOutput *)output didOutputSampleBuffer:(CMSampleBufferRef)sampleBuffer fromConnection:(AVCaptureConnection *)connection {
  if (output == _captureVideoOutput) {
    [self.previewTexture updateBuffer:sampleBuffer];
    if (_onPreviewFrameAvailable) {
      _onPreviewFrameAvailable();
    }

    // Send to image stream controller if enabled
    if (_imageStreamController.streamImages) {
        [_imageStreamController captureOutput:output didOutputSampleBuffer:sampleBuffer fromConnection:connection orientation:_motionController.deviceOrientation];
    }

    // Send to video recording controller if recording
    if (_videoController.isRecording) {
      // Ensure VideoController's captureOutput can handle being called multiple times for the same timestamp (once for video, once for audio)
      // or ensure it only processes the video buffer here.
      // Assuming it can differentiate based on 'output' or buffer type.
      [_videoController captureOutput:output didOutputSampleBuffer:sampleBuffer fromConnection:connection captureVideoOutput:_captureVideoOutput];
    }
  } else if (output == _audioOutput) {
    // Send audio buffers only to video recording controller if recording & audio enabled
    if (_videoController.isRecording && _videoController.isAudioEnabled) {
      [_videoController captureOutput:output didOutputSampleBuffer:sampleBuffer fromConnection:connection captureVideoOutput:nil]; // Pass nil for video output for audio
    }
  }
}

@end



================================================
FILE: ios/camerawesome/Sources/camerawesome/Controllers/Analysis/AnalysisController.m
================================================
//
//  AnalysisController.m
//  camerawesome
//
//  Created by Dimitri Dessus on 04/04/2023.
//

#import "AnalysisController.h"

@implementation AnalysisController

+ (void)bgra8888toJpegBgra8888image:(nonnull AnalysisImageWrapper *)bgra8888image jpegQuality:(nonnull NSNumber *)jpegQuality completion:(nonnull void (^)(AnalysisImageWrapper * _Nullable, FlutterError * _Nullable))completion {
  NSData *bgra8888Data = bgra8888image.bytes.data;
  CFDataRef cfData = (__bridge CFDataRef)bgra8888Data;
  
  CGDataProviderRef dataProvider = CGDataProviderCreateWithCFData(cfData);
  
  CGColorSpaceRef colorSpace = CGColorSpaceCreateDeviceRGB();
  CGImageRef cgImage = CGImageCreate(bgra8888image.width.intValue,
                                     bgra8888image.height.intValue,
                                     8,
                                     32,
                                     [bgra8888image.planes.firstObject.bytesPerRow intValue],
                                     colorSpace,
                                     kCGBitmapByteOrder32Big |
                                     kCGImageAlphaPremultipliedLast,
                                     dataProvider,
                                     NULL,
                                     true,
                                     kCGRenderingIntentDefault);
  
  UIImage *image = [UIImage imageWithCGImage:cgImage];
  NSData *jpegData = UIImageJPEGRepresentation(image, [jpegQuality floatValue]);
  
  
  FlutterStandardTypedData *dataFlutter = [FlutterStandardTypedData typedDataWithBytes:jpegData];
  
  
  AnalysisImageWrapper *jpegImage = [AnalysisImageWrapper makeWithFormat:AnalysisImageFormatJpeg
                                                                   bytes:dataFlutter
                                                                   width:bgra8888image.width
                                                                  height:bgra8888image.height
                                                                  planes:bgra8888image.planes
                                                                cropRect:bgra8888image.cropRect
                                                                rotation:bgra8888image.rotation];
  
  completion(jpegImage, nil);
  
  CGColorSpaceRelease(colorSpace);
  CGImageRelease(cgImage);
  CGDataProviderRelease(dataProvider);
}

+ (void)nv21toJpegNv21Image:(nonnull AnalysisImageWrapper *)nv21Image jpegQuality:(nonnull NSNumber *)jpegQuality completion:(nonnull void (^)(AnalysisImageWrapper * _Nullable, FlutterError * _Nullable))completion {
  completion(nil, [FlutterError errorWithCode:@"NOT_SUPPORTED" message:@"this format is currently not supported on iOS" details:nil]);
}

+ (void)yuv420toJpegYuvImage:(nonnull AnalysisImageWrapper *)yuvImage jpegQuality:(nonnull NSNumber *)jpegQuality completion:(nonnull void (^)(AnalysisImageWrapper * _Nullable, FlutterError * _Nullable))completion {
  completion(nil, [FlutterError errorWithCode:@"NOT_SUPPORTED" message:@"this format is currently not supported on iOS" details:nil]);
}

+ (void)yuv420toNv21YuvImage:(nonnull AnalysisImageWrapper *)yuvImage completion:(nonnull void (^)(AnalysisImageWrapper * _Nullable, FlutterError * _Nullable))completion {
  completion(nil, [FlutterError errorWithCode:@"NOT_SUPPORTED" message:@"this format is currently not supported on iOS" details:nil]);
}

@end



================================================
FILE: ios/camerawesome/Sources/camerawesome/Controllers/Exif/ExifContainer.m
================================================
//
//  ExifContainer.m
//  camerawesome
//
//  Created by Dimitri Dessus on 30/09/2022.
//

#import <ImageIO/ImageIO.h>
#import <CoreLocation/CoreLocation.h>
#import "ExifContainer.h"

NSString const * kCGImagePropertyProjection = @"ProjectionType";

@interface ExifContainer ()

@property (nonatomic, strong) NSMutableDictionary *imageMetadata;

@property (nonatomic, strong, readonly) NSMutableDictionary *exifDictionary;
@property (nonatomic, strong, readonly) NSMutableDictionary *tiffDictionary;
@property (nonatomic, strong, readonly) NSMutableDictionary *gpsDictionary;
@end

@implementation ExifContainer

- (instancetype)init {
  self = [super init];
  
  if (self) {
    _imageMetadata = [[NSMutableDictionary alloc] init];
  }
  
  return self;
}

- (void)addLocation:(CLLocation *)currentLocation {
  CLLocationDegrees latitude  = currentLocation.coordinate.latitude;
  CLLocationDegrees longitude = currentLocation.coordinate.longitude;
  
  NSString *latitudeRef = nil;
  NSString *longitudeRef = nil;
  
  if (latitude < 0.0) {
    
    latitude *= -1;
    latitudeRef = @"S";
    
  } else {
    latitudeRef = @"N";
  }
  
  if (longitude < 0.0) {
    longitude *= -1;
    longitudeRef = @"W";
  } else {
    longitudeRef = @"E";
  }
  
  self.gpsDictionary[(NSString*)kCGImagePropertyGPSTimeStamp] = [self getUTCFormattedDate:currentLocation.timestamp];
  
  self.gpsDictionary[(NSString*)kCGImagePropertyGPSLatitudeRef] = latitudeRef;
  self.gpsDictionary[(NSString*)kCGImagePropertyGPSLatitude] = [NSNumber numberWithFloat:latitude];
  
  self.gpsDictionary[(NSString*)kCGImagePropertyGPSLongitudeRef] = longitudeRef;
  self.gpsDictionary[(NSString*)kCGImagePropertyGPSLongitude] = [NSNumber numberWithFloat:longitude];
  
  self.gpsDictionary[(NSString*)kCGImagePropertyGPSDOP] = [NSNumber numberWithFloat:currentLocation.horizontalAccuracy];
  self.gpsDictionary[(NSString*)kCGImagePropertyGPSAltitude] = [NSNumber numberWithFloat:currentLocation.altitude];
}

- (void)addUserComment:(NSString*)comment {
  NSString *key = (__bridge_transfer NSString *)kCGImagePropertyExifUserComment;
  [self setValue:comment forExifKey:key];
}

- (void)addCreationDate:(NSDate *)date {
  NSString *dateString = [self getUTCFormattedDate:date];
  NSString *key = (__bridge_transfer NSString *)kCGImagePropertyExifDateTimeOriginal;
  [self setValue:dateString forExifKey:key];
  
}

- (void)addDescription:(NSString*)description {
  [self.tiffDictionary setObject:description forKey:(NSString *)kCGImagePropertyTIFFImageDescription];
}

- (void)addProjection:(NSString *)projection {
  [self setValue:projection forExifKey:kCGImagePropertyProjection];
}

- (void)setValue:(NSString *)key forExifKey:(NSString *)value {
  [self.exifDictionary setObject:value forKey:key];
}

- (NSDictionary *)exifData {
  return self.imageMetadata;
}

#pragma mark - Getters

- (NSMutableDictionary *)exifDictionary {
  return [self dictionaryForKey:(NSString*)kCGImagePropertyExifDictionary];
}

- (NSMutableDictionary *)tiffDictionary {
  return [self dictionaryForKey:(NSString*)kCGImagePropertyTIFFDictionary];
}

- (NSMutableDictionary *)gpsDictionary {
  return [self dictionaryForKey:(NSString*)kCGImagePropertyGPSDictionary];
}

- (NSMutableDictionary *)dictionaryForKey:(NSString *)key {
  NSMutableDictionary *dict = self.imageMetadata[key];
  
  if (!dict) {
    dict = [[NSMutableDictionary alloc] init];
    self.imageMetadata[key] = dict;
  }
  
  return dict;
}

#pragma mark - Helpers

- (NSString *)getUTCFormattedDate:(NSDate *)localDate {
  
  static NSDateFormatter *dateFormatter = nil;
  
  static dispatch_once_t onceToken;
  dispatch_once(&onceToken, ^{
    
    dateFormatter = [[NSDateFormatter alloc] init];
    [dateFormatter setDateFormat:@"yyyy:MM:dd HH:mm:ss"];
    
  });
  
  
  return [dateFormatter stringFromDate:localDate];
}

@end



================================================
FILE: ios/camerawesome/Sources/camerawesome/Controllers/Exif/NSData+Exif.m
================================================
//
//  UIImage+Exif.m
//  camerawesome
//
//  Created by Dimitri Dessus on 30/09/2022.
//

#import "NSData+Exif.h"

#import <ImageIO/ImageIO.h>
#import "NSData+Exif.h"
#import "ExifContainer.h"

@implementation NSData (Exif)

- (NSData *)addExif:(ExifContainer *)container {
  CGImageSourceRef source = CGImageSourceCreateWithData((__bridge CFDataRef) self, NULL);
  
  CFStringRef UTI = CGImageSourceGetType(source);
  
  NSMutableData *dest_data = [NSMutableData data];
  CGImageDestinationRef destination = CGImageDestinationCreateWithData((__bridge CFMutableDataRef)dest_data, UTI, 1, NULL);
  
  if (!destination) {
    NSLog(@"Error: Could not create image destination");
  }
  
  CGImageDestinationAddImageFromSource(destination, source, 0, (__bridge CFDictionaryRef) container.exifData);
  BOOL success = NO;
  success = CGImageDestinationFinalize(destination);
  
  if (!success) {
    NSLog(@"Error: Could not create data from image destination");
  }
  
  CFRelease(destination);
  CFRelease(source);
  
  return dest_data;
}

@end



================================================
FILE: ios/camerawesome/Sources/camerawesome/Controllers/ImageStream/ImageStreamController.m
================================================
//
//  ImageStreamController.m
//  camerawesome
//
//  Created by Dimitri Dessus on 17/12/2020.
//

#import "ImageStreamController.h"

@implementation ImageStreamController

NSInteger const MaxPendingProcessedImage = 4;

- (instancetype)initWithStreamImages:(bool)streamImages {
  self = [super init];
  _streamImages = streamImages;
  _processingImage = 0;
  return self;
}

# pragma mark - Camera Delegates
- (void)captureOutput:(AVCaptureOutput *)output didOutputSampleBuffer:(CMSampleBufferRef)sampleBuffer fromConnection:(AVCaptureConnection *)connection orientation:(UIDeviceOrientation)orientation {
  if (_imageStreamEventSink == nil) {
    return;
  }
  
  bool shouldFPSGuard = [self fpsGuard];
  bool shouldOverflowCrashingGuard = [self overflowCrashingGuard];
  
  if (shouldFPSGuard || shouldOverflowCrashingGuard) {
    return;
  }
  
  _processingImage++;
  
  CVPixelBufferRef pixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer);
  CVPixelBufferLockBaseAddress(pixelBuffer, kCVPixelBufferLock_ReadOnly);
  
  size_t imageWidth = CVPixelBufferGetWidth(pixelBuffer);
  size_t imageHeight = CVPixelBufferGetHeight(pixelBuffer);
  
  NSMutableArray *planes = [NSMutableArray array];
  
  const Boolean isPlanar = CVPixelBufferIsPlanar(pixelBuffer);
  size_t planeCount;
  if (isPlanar) {
    planeCount = CVPixelBufferGetPlaneCount(pixelBuffer);
  } else {
    planeCount = 1;
  }
  
  for (int i = 0; i < planeCount; i++) {
    void *planeAddress;
    size_t bytesPerRow;
    size_t height;
    size_t width;
    
    if (isPlanar) {
      planeAddress = CVPixelBufferGetBaseAddressOfPlane(pixelBuffer, i);
      bytesPerRow = CVPixelBufferGetBytesPerRowOfPlane(pixelBuffer, i);
      height = CVPixelBufferGetHeightOfPlane(pixelBuffer, i);
      width = CVPixelBufferGetWidthOfPlane(pixelBuffer, i);
    } else {
      planeAddress = CVPixelBufferGetBaseAddress(pixelBuffer);
      bytesPerRow = CVPixelBufferGetBytesPerRow(pixelBuffer);
      height = CVPixelBufferGetHeight(pixelBuffer);
      width = CVPixelBufferGetWidth(pixelBuffer);
    }
    
    NSNumber *length = @(bytesPerRow * height);
    NSData *bytes = [NSData dataWithBytes:planeAddress length:length.unsignedIntegerValue];
    
    [planes addObject:@{
      @"bytesPerRow": @(bytesPerRow),
      @"width": @(width),
      @"height": @(height),
      @"bytes": [FlutterStandardTypedData typedDataWithBytes:bytes],
    }];
  }
  
  CVPixelBufferUnlockBaseAddress(pixelBuffer, kCVPixelBufferLock_ReadOnly);
  
  NSDictionary *imageBuffer = @{
    @"width": [NSNumber numberWithUnsignedLong:imageWidth],
    @"height": [NSNumber numberWithUnsignedLong:imageHeight],
    @"format": @"bgra8888", // TODO: change this dynamically
    @"planes": planes,
    @"rotation": [self getInputImageOrientation:orientation]
  };
  
  dispatch_async(dispatch_get_main_queue(), ^{
    self->_imageStreamEventSink(imageBuffer);
  });
  
}

- (NSString *)getInputImageOrientation:(UIDeviceOrientation)orientation {
  switch (orientation) {
    case UIDeviceOrientationLandscapeLeft:
      return @"rotation90deg";
    case UIDeviceOrientationLandscapeRight:
      return @"rotation270deg";
    case UIDeviceOrientationPortrait:
      return @"rotation0deg";
    case UIDeviceOrientationPortraitUpsideDown:
      return @"rotation180deg";
    default:
      return @"rotation0deg";
  }
}

#pragma mark - Guards

- (bool)fpsGuard {
  // calculate time interval between latest emitted frame
  NSDate *nowDate = [NSDate date];
  NSTimeInterval secondsBetween = [nowDate timeIntervalSinceDate:_latestEmittedFrame];
  
  // fps limit check, ignored if nil or == 0
  if (_maxFramesPerSecond && _maxFramesPerSecond > 0) {
    if (secondsBetween <= (1 / _maxFramesPerSecond)) {
      // skip image because out of time
      return YES;
    }
  }
  
  return NO;
}

- (bool)overflowCrashingGuard {
  // overflow crash prevent condition
  if (_processingImage > MaxPendingProcessedImage) {
    // too many frame are pending processing, skipping...
    // this prevent crashing on older phones like iPhone 6, 7...
    return YES;
  }
  
  return NO;
}

// This is used to know the exact time when the image was received on the Flutter part
- (void)receivedImageFromStream {
  // used for the fps limit condition
  _latestEmittedFrame = [NSDate date];
  
  // used for the overflow prevent crashing condition
  if (_processingImage >= 0) {
    _processingImage--;
  }
}

#pragma mark - Setters

- (void)setImageStreamEventSink:(FlutterEventSink)imageStreamEventSink {
  _imageStreamEventSink = imageStreamEventSink;
}

- (void)setMaxFramesPerSecond:(float)maxFramesPerSecond {
  _maxFramesPerSecond = maxFramesPerSecond;
}

- (void)setStreamImages:(bool)streamImages {
  _streamImages = streamImages;
}

@end



================================================
FILE: ios/camerawesome/Sources/camerawesome/Controllers/Location/LocationController.m
================================================
//
//  LocationController.m
//  camerawesome
//
//  Created by Dimitri Dessus on 07/09/2022.
//

#import "LocationController.h"

@implementation LocationController

- (instancetype)init {
  if (self = [super init]) {
    self.locationManager = [[CLLocationManager alloc] init];
    self.locationManager.delegate = self;
    
    self.locationManager.distanceFilter = kCLDistanceFilterNone;
    self.locationManager.desiredAccuracy = kCLLocationAccuracyBest;
  }
  
  return self;
}

- (void)requestWhenInUseAuthorizationOnGranted:(OnAuthorizationGranted)granted declined:(OnAuthorizationDeclined)declined {
  _grantedBlock = granted;
  _declinedBlock = declined;
  
  if (self.locationManager.authorizationStatus ==  kCLAuthorizationStatusNotDetermined) {
    if ([self.locationManager respondsToSelector:@selector(requestWhenInUseAuthorization)]) {
      [self.locationManager requestWhenInUseAuthorization];
    }
  } else if (self.locationManager.authorizationStatus ==  kCLAuthorizationStatusAuthorizedAlways || self.locationManager.authorizationStatus == kCLAuthorizationStatusAuthorizedWhenInUse) {
    _grantedBlock();
  } else {
    _declinedBlock();
  }
}

- (void)locationManagerDidChangeAuthorization:(CLLocationManager *)manager {
  if (manager.authorizationStatus ==  kCLAuthorizationStatusAuthorizedAlways || manager.authorizationStatus == kCLAuthorizationStatusAuthorizedWhenInUse) {
    if (_grantedBlock != nil) {
      _grantedBlock();
    }
    
  } else {
    if (_declinedBlock != nil) {
      _declinedBlock();
    }
    
  }
}

@end



================================================
FILE: ios/camerawesome/Sources/camerawesome/Controllers/Motion/MotionController.m
================================================
//
//  MotionController.m
//  camerawesome
//
//  Created by Dimitri Dessus on 17/12/2020.
//

#import "MotionController.h"

@implementation MotionController

- (instancetype)init {
  self = [super init];
  _motionManager = [[CMMotionManager alloc] init];
  _motionManager.deviceMotionUpdateInterval = 0.2f;
  return self;
}

- (void)setOrientationEventSink:(FlutterEventSink)orientationEventSink {
  _orientationEventSink = orientationEventSink;
}

/// Start live motion detection
- (void)startMotionDetection {
  [_motionManager startDeviceMotionUpdatesToQueue:[NSOperationQueue mainQueue]
                                      withHandler:^(CMDeviceMotion *data, NSError *error) {
    UIDeviceOrientation newOrientation;
    if(fabs(data.gravity.x) > fabs(data.gravity.y)) {
      // Landscape
      newOrientation = (data.gravity.x >= 0) ? UIDeviceOrientationLandscapeLeft : UIDeviceOrientationLandscapeRight;
    } else {
      // Portrait
      newOrientation = (data.gravity.y >= 0) ? UIDeviceOrientationPortraitUpsideDown : UIDeviceOrientationPortrait;
    }
    if (self->_deviceOrientation != newOrientation) {
      self->_deviceOrientation = newOrientation;
      
      NSString *orientationString;
      switch (newOrientation) {
        case UIDeviceOrientationLandscapeLeft:
          orientationString = @"LANDSCAPE_LEFT";
          break;
        case UIDeviceOrientationLandscapeRight:
          orientationString = @"LANDSCAPE_RIGHT";
          break;
        case UIDeviceOrientationPortrait:
          orientationString = @"PORTRAIT_UP";
          break;
        case UIDeviceOrientationPortraitUpsideDown:
          orientationString = @"PORTRAIT_DOWN";
          break;
        default:
          break;
      }
      if (self->_orientationEventSink != nil) {
        self->_orientationEventSink(orientationString);
      }
    }
  }];
}

/// Stop motion update
- (void)stopMotionDetection {
  [_motionManager stopDeviceMotionUpdates];
}

@end



================================================
FILE: ios/camerawesome/Sources/camerawesome/Controllers/MultiCamera/MultiCameraController.m
================================================
//
//  MultiCameraController.m
//  camerawesome
//
//  Created by Dimitri Dessus on 29/03/2023.
//

#import "MultiCameraController.h"

@implementation MultiCameraController

+ (BOOL)isMultiCamSupported {
  return AVCaptureMultiCamSession.isMultiCamSupported;
}

@end



================================================
FILE: ios/camerawesome/Sources/camerawesome/Controllers/Permissions/PermissionsController.m
================================================
//
//  PermissionsController.m
//  _NIODataStructures
//
//  Created by Dimitri Dessus on 27/12/2022.
//

#import "PermissionsController.h"

@implementation CameraPermissionsController

+ (BOOL)checkPermission {
  AVAuthorizationStatus authStatus = [AVCaptureDevice authorizationStatusForMediaType:AVMediaTypeVideo];
  
  return (authStatus == AVAuthorizationStatusAuthorized);
}

+ (BOOL)checkAndRequestPermission {
  NSString *mediaType = AVMediaTypeVideo;
  AVAuthorizationStatus authStatus = [AVCaptureDevice authorizationStatusForMediaType:mediaType];
  
  __block BOOL permissionsGranted;
  if (authStatus == AVAuthorizationStatusNotDetermined) {
    dispatch_semaphore_t sem = dispatch_semaphore_create(0);
    [AVCaptureDevice requestAccessForMediaType:mediaType completionHandler:^(BOOL granted) {
      permissionsGranted = granted;
      dispatch_semaphore_signal(sem);
    }];
    dispatch_semaphore_wait(sem, DISPATCH_TIME_FOREVER);
  } else {
    permissionsGranted = (authStatus == AVAuthorizationStatusAuthorized);
  }
  
  return permissionsGranted;
}

@end

@implementation MicrophonePermissionsController

+ (BOOL)checkPermission {
  AVAudioSessionRecordPermission permissionStatus = [[AVAudioSession sharedInstance] recordPermission];
  
  return (permissionStatus == AVAudioSessionRecordPermissionGranted);
}

+ (BOOL)checkAndRequestPermission {
  AVAudioSessionRecordPermission permissionStatus = [[AVAudioSession sharedInstance] recordPermission];
  
  __block BOOL permissionsGranted;
  if (permissionStatus == AVAudioSessionRecordPermissionUndetermined) {
    dispatch_semaphore_t sem = dispatch_semaphore_create(0);
    [[AVAudioSession sharedInstance] requestRecordPermission:^(BOOL granted) {
      permissionsGranted = granted;
      dispatch_semaphore_signal(sem);
    }];
    dispatch_semaphore_wait(sem, DISPATCH_TIME_FOREVER);
  } else {
    permissionsGranted = (permissionStatus == AVAudioSessionRecordPermissionGranted);
  }
  
  return permissionsGranted;
}

@end



================================================
FILE: ios/camerawesome/Sources/camerawesome/Controllers/PhysicalButtons/PhysicalButtonController.m
================================================
//
//  PhysicalButtonsController.m
//  Pods
//
//  Created by Dimitri Dessus on 20/03/2023.
//

#import "PhysicalButtonController.h"

@implementation PhysicalButtonController

- (instancetype)init {
  self = [super init];
//   self.volumeButtonHandler = [JPSVolumeButtonHandler volumeButtonHandlerWithUpBlock:^{
//     [self tryToSendPhysicalEvent:volume_up];
//   } downBlock:^{
//     [self tryToSendPhysicalEvent:volume_down];
//   }];
  return self;
}

// - (void)tryToSendPhysicalEvent:(PhysicalButton)physicalButton {
//   if (debounceTimer != nil) {
//     [debounceTimer invalidate];
//     debounceTimer = nil;
//   }
  
//   debounceTimer = [NSTimer scheduledTimerWithTimeInterval:0.3
//                                                      repeats:NO
//                                                        block:^(NSTimer *timer) {
//     if (self->_physicalButtonEventSink != nil) {
//       NSString *physicalButtonString;
//       switch (physicalButton) {
//         case volume_up:
//           physicalButtonString = @"VOLUME_UP";
//           break;
//         case volume_down:
//           physicalButtonString = @"VOLUME_DOWN";
//           break;
//         default:
//           return;
//       }
      
//       self->_physicalButtonEventSink(physicalButtonString);
//     }
//   }];
// }

- (void)startListening {
//   [self.volumeButtonHandler startHandler:YES];
}

- (void)stopListening {
//   if (self.volumeButtonHandler != nil) {
//     [self.volumeButtonHandler stopHandler];
//   }
}

- (void)setPhysicalButtonEventSink:(FlutterEventSink)physicalButtonEventSink {
  _physicalButtonEventSink = physicalButtonEventSink;
}

@end



================================================
FILE: ios/camerawesome/Sources/camerawesome/Controllers/Picture/CameraPictureController.m
================================================
//
//  CameraPicture.m
//  camerawesome
//
//  Created by Dimitri Dessus on 24/07/2020.
//

#import "CameraPictureController.h"
#import "ExifContainer.h"
#import "NSData+Exif.h"

@implementation CameraPictureController {
  CameraPictureController *selfReference;
}

- (instancetype)initWithPath:(NSString *)path
                     orientation:(NSInteger)orientation
                  sensorPosition:(PigeonSensorPosition)sensorPosition
                 saveGPSLocation:(bool)saveGPSLocation
               mirrorFrontCamera:(bool)mirrorFrontCamera
                     aspectRatio:(AspectRatio)aspectRatio
                      completion:(nonnull void (^)(NSNumber * _Nullable, FlutterError * _Nullable))completion
                        callback:(OnPictureTaken)callback {
  self = [super init];
  NSAssert(self, @"super init cannot be nil");
  _path = path;
  _completion = completion;
  _orientation = orientation;
  _completionBlock = callback;
  _sensorPosition = sensorPosition;
  _saveGPSLocation = saveGPSLocation;
  _aspectRatioType = aspectRatio;
  _mirrorFrontCamera = mirrorFrontCamera;
  
  if (aspectRatio == Ratio4_3) {
    _aspectRatio = 4.0/3.0;
  } else if(aspectRatio == Ratio16_9) {
    _aspectRatio = 16.0/9.0;
  } else {
    _aspectRatio = 1;
  }
  
  selfReference = self;
  return self;
}

- (NSData *)writeMetadataIntoImageData:(NSData *)imageData metadata:(NSMutableDictionary *)metadata {
  // create an imagesourceref
  CGImageSourceRef source = CGImageSourceCreateWithData((__bridge CFDataRef) imageData, NULL);
  
  // this is the type of image (e.g., public.jpeg)
  CFStringRef UTI = CGImageSourceGetType(source);
  
  // create a new data object and write the new image into it
  NSMutableData *dest_data = [NSMutableData data];
  CGImageDestinationRef destination = CGImageDestinationCreateWithData((__bridge CFMutableDataRef)dest_data, UTI, 1, NULL);
  if (!destination) {
    NSLog(@"Error: Could not create image destination");
  }
  // add the image contained in the image source to the destination, overidding the old metadata with our modified metadata
  CGImageDestinationAddImageFromSource(destination, source, 0, (__bridge CFDictionaryRef) metadata);
  BOOL success = NO;
  success = CGImageDestinationFinalize(destination);
  if (!success) {
    NSLog(@"Error: Could not create data from image destination");
  }
  CFRelease(destination);
  CFRelease(source);
  return dest_data;
}

#pragma clang diagnostic push
#pragma clang diagnostic ignored "-Wdeprecated-implementations"
- (void)captureOutput:(AVCapturePhotoOutput *)output
didFinishProcessingPhotoSampleBuffer:(CMSampleBufferRef)photoSampleBuffer
previewPhotoSampleBuffer:(CMSampleBufferRef)previewPhotoSampleBuffer
     resolvedSettings:(AVCaptureResolvedPhotoSettings *)resolvedSettings
      bracketSettings:(AVCaptureBracketedStillImageSettings *)bracketSettings
                error:(NSError *)error {
#pragma clang diagnostic pop
  
  selfReference = nil;
  if (error) {
    _completion(nil, [FlutterError errorWithCode:@"CAPTURE ERROR" message:error.description details:@""]);
    return;
  }
  
  // Add exif data
  ExifContainer *container = [[ExifContainer alloc] init];
  [container addCreationDate:[NSDate date]];
  
  // Save GPS location only if provided
  if (_saveGPSLocation) {
    CLLocationManager *locationManager = [CLLocationManager new];
    CLLocation *location = [locationManager location];
    [container addLocation:location];
  }
  
  // we ignore this error because plugin can only be installed on iOS 11+
#pragma clang diagnostic push
#pragma clang diagnostic ignored "-Wdeprecated-declarations"
  NSData *data = [AVCapturePhotoOutput JPEGPhotoDataRepresentationForJPEGSampleBuffer:photoSampleBuffer
                                                             previewPhotoSampleBuffer:previewPhotoSampleBuffer];
#pragma clang diagnostic pop
  
  UIImage *image = [UIImage imageWithCGImage:[UIImage imageWithData:data].CGImage
                                       scale:1.0
                                 orientation:[self getJpegOrientation]];
  float originalWidth = image.size.width;
  float originalHeight = image.size.height;
  
  float originalImageAspectRatio = originalWidth / originalHeight;
  
  float outputWidth = originalWidth;
  float outputHeight = originalHeight;
  if (originalImageAspectRatio != _aspectRatio) {
    if (originalImageAspectRatio > _aspectRatio) {
      outputWidth = originalHeight * _aspectRatio;
    } else if (originalImageAspectRatio < _aspectRatio) {
      outputHeight = originalWidth / _aspectRatio;
    }
  }
  
  UIImage *imageConverted = [self imageByCroppingImage:image toSize:CGSizeMake(outputWidth, outputHeight)];
  
  image = [UIImage imageWithCGImage:[imageConverted CGImage] scale:0.0 orientation:[self getJpegOrientation]];

  NSData *imageWithExif = [UIImageJPEGRepresentation(image, 1.0) addExif:container];
  
  bool success = [imageWithExif writeToFile:_path atomically:YES];
  if (!success) {
    _completion(nil, [FlutterError errorWithCode:@"IOError" message:@"unable to write file" details:nil]);
    return;
  }
  _completionBlock();
  
}

- (UIImage *)imageByCroppingImage:(UIImage *)image toSize:(CGSize)size {
  double newCropWidth, newCropHeight;

  if(image.size.width < image.size.height) {
    if (image.size.width < size.width) {
      newCropWidth = size.width;
    } else {
      newCropWidth = image.size.width;
    }
    newCropHeight = (newCropWidth * size.height)/size.width;
  } else {
    if (image.size.height < size.height) {
      newCropHeight = size.height;
    } else {
      newCropHeight = image.size.height;
    }
    newCropWidth = (newCropHeight * size.width)/size.height;
  }
  
  double imageHeightDivided = image.size.height/2.0;
  double imageWidthDivided = image.size.width/2.0;
  
  double x = imageWidthDivided - newCropWidth/2.0;
  double y = imageHeightDivided - newCropHeight/2.0;
  
  CGRect cropRect;
  if (UIDeviceOrientationIsLandscape(_orientation)) {
    cropRect = CGRectMake(x, y, newCropWidth, newCropHeight);
  } else {
    if (_aspectRatioType == Ratio16_9) {
      cropRect = CGRectMake(0, 0, image.size.height, image.size.width);
    } else {
      if (_aspectRatioType == Ratio4_3) {
        double localX = imageHeightDivided - (imageHeightDivided / _aspectRatio);
        cropRect = CGRectMake(localX, 0, image.size.height / _aspectRatio, image.size.width);
      } else {
        cropRect = CGRectMake(y, x, newCropWidth, newCropHeight);
      }
    }
  }
  
  CGImageRef imageRef = CGImageCreateWithImageInRect([image CGImage], cropRect);
  
  UIImage *cropped = [UIImage imageWithCGImage:imageRef];
  CGImageRelease(imageRef);
  
  return cropped;
}

- (UIImageOrientation)getJpegOrientation {
  switch (_orientation) {
    case UIDeviceOrientationPortrait:
      if (self.sensorPosition == PigeonSensorPositionFront && _mirrorFrontCamera) {
        return UIImageOrientationLeftMirrored;
      } else {
        return UIImageOrientationRight;
      }
    case UIDeviceOrientationLandscapeRight:
      return (self.sensorPosition == PigeonSensorPositionBack) ? UIImageOrientationUp : UIImageOrientationDown;
    case UIDeviceOrientationLandscapeLeft:
      return (self.sensorPosition == PigeonSensorPositionBack) ? UIImageOrientationDown : UIImageOrientationUp;
    default:
      return UIImageOrientationLeft;
  }
}

@end



================================================
FILE: ios/camerawesome/Sources/camerawesome/Controllers/Sensors/SensorsController.m
================================================
//
//  SensorsController.m
//  camerawesome
//
//  Created by Dimitri Dessus on 28/03/2023.
//

#import "SensorsController.h"
#import "Pigeon.h"

@implementation SensorsController

+ (NSArray *)getSensors:(AVCaptureDevicePosition)position {
  NSMutableArray *sensors = [NSMutableArray new];
  
  NSArray *sensorsType = @[AVCaptureDeviceTypeBuiltInWideAngleCamera, AVCaptureDeviceTypeBuiltInTelephotoCamera, AVCaptureDeviceTypeBuiltInUltraWideCamera, AVCaptureDeviceTypeBuiltInTrueDepthCamera];
  
  AVCaptureDeviceDiscoverySession *discoverySession = [AVCaptureDeviceDiscoverySession
                                                       discoverySessionWithDeviceTypes:sensorsType
                                                       mediaType:AVMediaTypeVideo
                                                       position:AVCaptureDevicePositionUnspecified];
  
  for (AVCaptureDevice *device in discoverySession.devices) {
    PigeonSensorType type;
    if (device.deviceType == AVCaptureDeviceTypeBuiltInTelephotoCamera) {
      type = PigeonSensorTypeTelephoto;
    } else if (device.deviceType == AVCaptureDeviceTypeBuiltInUltraWideCamera) {
      type = PigeonSensorTypeUltraWideAngle;
    } else if (device.deviceType == AVCaptureDeviceTypeBuiltInTrueDepthCamera) {
      type = PigeonSensorTypeTrueDepth;
    } else if (device.deviceType == AVCaptureDeviceTypeBuiltInWideAngleCamera) {
      type = PigeonSensorTypeWideAngle;
    } else {
      type = PigeonSensorTypeUnknown;
    }
    
    PigeonSensorTypeDevice *sensorType = [PigeonSensorTypeDevice makeWithSensorType:type name:device.localizedName iso:[NSNumber numberWithFloat:device.ISO] flashAvailable:[NSNumber numberWithBool:device.flashAvailable] uid:device.uniqueID];
    
    if (device.position == position) {
      [sensors addObject:sensorType];
    }
  }
  
  return sensors;
}

@end



================================================
FILE: ios/camerawesome/Sources/camerawesome/Controllers/Video/VideoController.m
================================================
//
//  VideoController.m
//  camerawesome
//
//  Created by Dimitri Dessus on 17/12/2020.
//

#import "VideoController.h"

FourCharCode const videoFormat = kCVPixelFormatType_32BGRA;

@implementation VideoController

- (instancetype)init {
  self = [super init];
  _isRecording = NO;
  _isAudioEnabled = YES;
  _isPaused = NO;
  
  return self;
}

# pragma mark - User video interactions

/// Start recording video at given path
- (void)recordVideoAtPath:(NSString *)path captureDevice:(AVCaptureDevice *)device orientation:(NSInteger)orientation audioSetupCallback:(OnAudioSetup)audioSetupCallback videoWriterCallback:(OnVideoWriterSetup)videoWriterCallback options:(CupertinoVideoOptions *)options quality:(VideoRecordingQuality)quality completion:(nonnull void (^)(FlutterError * _Nullable))completion {
  _options = options;
  _recordingQuality = quality;
  
  // Create audio & video writer
  if (![self setupWriterForPath:path audioSetupCallback:audioSetupCallback options:options completion:completion]) {
    completion([FlutterError errorWithCode:@"VIDEO_ERROR" message:@"impossible to write video at path" details:path]);
    return;
  }
  // Call parent to add delegates for video & audio (if needed)
  videoWriterCallback();
  
  _isRecording = YES;
  _videoTimeOffset = CMTimeMake(0, 1);
  _audioTimeOffset = CMTimeMake(0, 1);
  _videoIsDisconnected = NO;
  _audioIsDisconnected = NO;
  _orientation = orientation;
  _captureDevice = device;
  
  // Change video FPS if provided
  if (_options && _options.fps != nil && _options.fps > 0) {
    [self adjustCameraFPS:_options.fps];
  }
}

/// Stop recording video
- (void)stopRecordingVideo:(nonnull void (^)(NSNumber * _Nullable, FlutterError * _Nullable))completion {
  if (_options && _options.fps != nil && _options.fps > 0) {
    // Reset camera FPS
    [self adjustCameraFPS:@(30)];
  }
  
  if (_isRecording) {
    _isRecording = NO;
    if (_videoWriter.status != AVAssetWriterStatusUnknown) {
      [_videoWriter finishWritingWithCompletionHandler:^{
        if (self->_videoWriter.status == AVAssetWriterStatusCompleted) {
          completion(@(YES), nil);
        } else {
          completion(@(NO), [FlutterError errorWithCode:@"VIDEO_ERROR" message:@"impossible to completely write video" details:@""]);
        }
      }];
    }
  } else {
    completion(@(NO), [FlutterError errorWithCode:@"VIDEO_ERROR" message:@"video is not recording" details:@""]);
  }
}

- (void)pauseVideoRecording {
  _isPaused = YES;
}

- (void)resumeVideoRecording {
  _isPaused = NO;
}

# pragma mark - Audio & Video writers

/// Setup video channel & write file on path
- (BOOL)setupWriterForPath:(NSString *)path audioSetupCallback:(OnAudioSetup)audioSetupCallback options:(CupertinoVideoOptions *)options completion:(nonnull void (^)(FlutterError * _Nullable))completion {
  NSError *error = nil;
  NSURL *outputURL;
  if (path != nil) {
    outputURL = [NSURL fileURLWithPath:path];
  } else {
    return NO;
  }
  if (_isAudioEnabled && !_isAudioSetup) {
    audioSetupCallback();
  }
  
  // Read from options if available
  AVVideoCodecType codecType = [self getBestCodecTypeAccordingOptions:options];
  AVFileType fileType = [self getBestFileTypeAccordingOptions:options];
  CGSize videoSize = [self getBestVideoSizeAccordingQuality: _recordingQuality];
    
  NSDictionary *videoSettings = @{
    AVVideoCodecKey   : codecType,
    AVVideoWidthKey   : @(videoSize.height),
    AVVideoHeightKey  : @(videoSize.width),
  };
  
  _videoWriterInput = [AVAssetWriterInput assetWriterInputWithMediaType:AVMediaTypeVideo outputSettings:videoSettings];
  [_videoWriterInput setTransform:[self getVideoOrientation]];
  
  _videoAdaptor = [AVAssetWriterInputPixelBufferAdaptor
                   assetWriterInputPixelBufferAdaptorWithAssetWriterInput:_videoWriterInput
                   sourcePixelBufferAttributes:@{
    (NSString *)kCVPixelBufferPixelFormatTypeKey: @(videoFormat)
  }];
  
  NSParameterAssert(_videoWriterInput);
  _videoWriterInput.expectsMediaDataInRealTime = YES;
  
  _videoWriter = [[AVAssetWriter alloc] initWithURL:outputURL
                                           fileType:fileType
                                              error:&error];
  NSParameterAssert(_videoWriter);
  if (error) {
    completion([FlutterError errorWithCode:@"VIDEO_ERROR" message:@"impossible to create video writer, check your options" details:error.description]);
    return NO;
  }
  
  [_videoWriter addInput:_videoWriterInput];
  
  if (_isAudioEnabled) {
    AudioChannelLayout acl;
    bzero(&acl, sizeof(acl));
    acl.mChannelLayoutTag = kAudioChannelLayoutTag_Mono;
    NSDictionary *audioOutputSettings = nil;
    
    audioOutputSettings = [NSDictionary
                           dictionaryWithObjectsAndKeys:[NSNumber numberWithInt:kAudioFormatMPEG4AAC], AVFormatIDKey,
                           [NSNumber numberWithFloat:44100.0], AVSampleRateKey,
                           [NSNumber numberWithInt:1], AVNumberOfChannelsKey,
                           [NSData dataWithBytes:&acl length:sizeof(acl)],
                           AVChannelLayoutKey, nil];
    _audioWriterInput = [AVAssetWriterInput assetWriterInputWithMediaType:AVMediaTypeAudio
                                                           outputSettings:audioOutputSettings];
    _audioWriterInput.expectsMediaDataInRealTime = YES;
    
    [_videoWriter addInput:_audioWriterInput];
  }
  
  return YES;
}

- (CGAffineTransform)getVideoOrientation {
  CGAffineTransform transform;
  
  switch ([[UIDevice currentDevice] orientation]) {
    case UIDeviceOrientationLandscapeLeft:
      transform = CGAffineTransformMakeRotation(-M_PI_2);
      break;
    case UIDeviceOrientationLandscapeRight:
      transform = CGAffineTransformMakeRotation(M_PI_2);
      break;
    case UIDeviceOrientationPortraitUpsideDown:
      transform = CGAffineTransformMakeRotation(M_PI);
      break;
    default:
      transform = CGAffineTransformIdentity;
      break;
  }
  
  return transform;
}

/// Append audio data
- (void)newAudioSample:(CMSampleBufferRef)sampleBuffer {
  if (_videoWriter.status != AVAssetWriterStatusWriting) {
    if (_videoWriter.status == AVAssetWriterStatusFailed) {
      //      *error = [FlutterError errorWithCode:@"VIDEO_ERROR" message:@"writing video failed" details:_videoWriter.error];
    }
    return;
  }
  if (_audioWriterInput.readyForMoreMediaData) {
    if (![_audioWriterInput appendSampleBuffer:sampleBuffer]) {
      //      *error = [FlutterError errorWithCode:@"VIDEO_ERROR" message:@"adding audio channel failed" details:_videoWriter.error];
    }
  }
}

/// Adjust time to sync audio & video
- (CMSampleBufferRef)adjustTime:(CMSampleBufferRef)sample by:(CMTime)offset CF_RETURNS_RETAINED {
  CMItemCount count;
  CMSampleBufferGetSampleTimingInfoArray(sample, 0, nil, &count);
  CMSampleTimingInfo *pInfo = malloc(sizeof(CMSampleTimingInfo) * count);
  CMSampleBufferGetSampleTimingInfoArray(sample, count, pInfo, &count);
  for (CMItemCount i = 0; i < count; i++) {
    pInfo[i].decodeTimeStamp = CMTimeSubtract(pInfo[i].decodeTimeStamp, offset);
    pInfo[i].presentationTimeStamp = CMTimeSubtract(pInfo[i].presentationTimeStamp, offset);
  }
  CMSampleBufferRef sout;
  CMSampleBufferCreateCopyWithNewTiming(nil, sample, count, pInfo, &sout);
  free(pInfo);
  return sout;
}

/// Adjust video preview & recording to specified FPS
- (void)adjustCameraFPS:(NSNumber *)fps {
  NSArray *frameRateRanges = _captureDevice.activeFormat.videoSupportedFrameRateRanges;
  
  if (frameRateRanges.count > 0) {
    AVFrameRateRange *frameRateRange = frameRateRanges.firstObject;
    NSError *error = nil;
    
    if ([_captureDevice lockForConfiguration:&error]) {
      CMTime frameDuration = CMTimeMake(1, [fps intValue]);
      if (CMTIME_COMPARE_INLINE(frameDuration, <=, frameRateRange.maxFrameDuration) && CMTIME_COMPARE_INLINE(frameDuration, >=, frameRateRange.minFrameDuration)) {
        _captureDevice.activeVideoMinFrameDuration = frameDuration;
      }
      [_captureDevice unlockForConfiguration];
    }
  }
}

# pragma mark - Camera Delegates
- (void)captureOutput:(AVCaptureOutput *)output didOutputSampleBuffer:(CMSampleBufferRef)sampleBuffer fromConnection:(AVCaptureConnection *)connection captureVideoOutput:(AVCaptureVideoDataOutput *)captureVideoOutput {
  
  if (self.isPaused) {
    return;
  }
  
  if (_videoWriter.status == AVAssetWriterStatusFailed) {
    //    _result([FlutterError errorWithCode:@"VIDEO_ERROR" message:@"impossible to write video " details:_videoWriter.error]);
    return;
  }
  
  CFRetain(sampleBuffer);
  CMTime currentSampleTime = CMSampleBufferGetPresentationTimeStamp(sampleBuffer);
  
  if (_videoWriter.status != AVAssetWriterStatusWriting) {
    [_videoWriter startWriting];
    [_videoWriter startSessionAtSourceTime:currentSampleTime];
  }
  
  if (output == captureVideoOutput) {
    if (_videoIsDisconnected) {
      _videoIsDisconnected = NO;
      
      if (_videoTimeOffset.value == 0) {
        _videoTimeOffset = CMTimeSubtract(currentSampleTime, _lastVideoSampleTime);
      } else {
        CMTime offset = CMTimeSubtract(currentSampleTime, _lastVideoSampleTime);
        _videoTimeOffset = CMTimeAdd(_videoTimeOffset, offset);
      }
      
      return;
    }
    
    _lastVideoSampleTime = currentSampleTime;
    
    CVPixelBufferRef nextBuffer = CMSampleBufferGetImageBuffer(sampleBuffer);
    CMTime nextSampleTime = CMTimeSubtract(_lastVideoSampleTime, _videoTimeOffset);
    [_videoAdaptor appendPixelBuffer:nextBuffer withPresentationTime:nextSampleTime];
  } else {
    CMTime dur = CMSampleBufferGetDuration(sampleBuffer);
    
    if (dur.value > 0) {
      currentSampleTime = CMTimeAdd(currentSampleTime, dur);
    }
    if (_audioIsDisconnected) {
      _audioIsDisconnected = NO;
      
      if (_audioTimeOffset.value == 0) {
        _audioTimeOffset = CMTimeSubtract(currentSampleTime, _lastAudioSampleTime);
      } else {
        CMTime offset = CMTimeSubtract(currentSampleTime, _lastAudioSampleTime);
        _audioTimeOffset = CMTimeAdd(_audioTimeOffset, offset);
      }
      
      return;
    }
    
    _lastAudioSampleTime = currentSampleTime;
    
    if (_audioTimeOffset.value != 0) {
      CFRelease(sampleBuffer);
      sampleBuffer = [self adjustTime:sampleBuffer by:_audioTimeOffset];
    }
    
    [self newAudioSample:sampleBuffer];
  }
  
  CFRelease(sampleBuffer);
}

# pragma mark - Settings converters

- (AVFileType)getBestFileTypeAccordingOptions:(CupertinoVideoOptions *)options {
  AVFileType fileType = AVFileTypeQuickTimeMovie;
  
  if (options && options != (id)[NSNull null]) {
    CupertinoFileType type = options.fileType;
    switch (type) {
      case CupertinoFileTypeQuickTimeMovie:
        fileType = AVFileTypeQuickTimeMovie;
        break;
      case CupertinoFileTypeMpeg4:
        fileType = AVFileTypeMPEG4;
        break;
      case CupertinoFileTypeAppleM4V:
        fileType = AVFileTypeAppleM4V;
        break;
      case CupertinoFileTypeType3GPP:
        fileType = AVFileType3GPP;
        break;
      case CupertinoFileTypeType3GPP2:
        fileType = AVFileType3GPP2;
        break;
      default:
        break;
    }
  }
  
  return fileType;
}

- (AVVideoCodecType)getBestCodecTypeAccordingOptions:(CupertinoVideoOptions *)options {
  AVVideoCodecType codecType = AVVideoCodecTypeH264;
  if (options && options != (id)[NSNull null]) {
    CupertinoCodecType codec = options.codec;
    switch (codec) {
      case CupertinoCodecTypeH264:
        codecType = AVVideoCodecTypeH264;
        break;
      case CupertinoCodecTypeHevc:
        codecType = AVVideoCodecTypeHEVC;
        break;
      case CupertinoCodecTypeHevcWithAlpha:
        codecType = AVVideoCodecTypeHEVCWithAlpha;
        break;
      case CupertinoCodecTypeJpeg:
        codecType = AVVideoCodecTypeJPEG;
        break;
      case CupertinoCodecTypeAppleProRes4444:
        codecType = AVVideoCodecTypeAppleProRes4444;
        break;
      case CupertinoCodecTypeAppleProRes422:
        codecType = AVVideoCodecTypeAppleProRes422;
        break;
      case CupertinoCodecTypeAppleProRes422HQ:
        codecType = AVVideoCodecTypeAppleProRes422HQ;
        break;
      case CupertinoCodecTypeAppleProRes422LT:
        codecType = AVVideoCodecTypeAppleProRes422LT;
        break;
      case CupertinoCodecTypeAppleProRes422Proxy:
        codecType = AVVideoCodecTypeAppleProRes422Proxy;
        break;
      default:
        break;
    }
  }
  return codecType;
}

- (CGSize)getBestVideoSizeAccordingQuality:(VideoRecordingQuality)quality {
  CGSize size;
  switch (quality) {
    case VideoRecordingQualityUhd:
    case VideoRecordingQualityHighest:
      if (@available(iOS 9.0, *)) {
        if ([_captureDevice supportsAVCaptureSessionPreset:AVCaptureSessionPreset3840x2160]) {
          size = CGSizeMake(3840, 2160);
        } else {
          size = CGSizeMake(1920, 1080);
        }
      } else {
        return CGSizeMake(1920, 1080);
      }
      break;
    case VideoRecordingQualityFhd:
      size = CGSizeMake(1920, 1080);
      break;
    case VideoRecordingQualityHd:
      size = CGSizeMake(1280, 720);
      break;
    case VideoRecordingQualitySd:
    case VideoRecordingQualityLowest:
      size = CGSizeMake(960, 540);
      break;
  }
    
  // ensure video output size does not exceed capture session size
  if (size.width > _previewSize.width) {
    size = _previewSize;
  }
  
  return size;
}

# pragma mark - Setter
- (void)setIsAudioEnabled:(bool)isAudioEnabled {
  _isAudioEnabled = isAudioEnabled;
}
- (void)setIsAudioSetup:(bool)isAudioSetup {
  _isAudioSetup = isAudioSetup;
}

- (void)setPreviewSize:(CGSize)previewSize {
  _previewSize = previewSize;
}

- (void)setVideoIsDisconnected:(bool)videoIsDisconnected {
  _videoIsDisconnected = videoIsDisconnected;
}

- (void)setAudioIsDisconnected:(bool)audioIsDisconnected {
  _audioIsDisconnected = audioIsDisconnected;
}

@end



================================================
FILE: ios/camerawesome/Sources/camerawesome/include/AnalysisController.h
================================================
//
//  AnalysisController.h
//  camerawesome
//
//  Created by Dimitri Dessus on 04/04/2023.
//

#import <Foundation/Foundation.h>
#import <Flutter/Flutter.h>
#import "Pigeon.h"

NS_ASSUME_NONNULL_BEGIN

@interface AnalysisController : NSObject

+ (void)bgra8888toJpegBgra8888image:(nonnull AnalysisImageWrapper *)bgra8888image jpegQuality:(nonnull NSNumber *)jpegQuality completion:(nonnull void (^)(AnalysisImageWrapper * _Nullable, FlutterError * _Nullable))completion;
+ (void)nv21toJpegNv21Image:(nonnull AnalysisImageWrapper *)nv21Image jpegQuality:(nonnull NSNumber *)jpegQuality completion:(nonnull void (^)(AnalysisImageWrapper * _Nullable, FlutterError * _Nullable))completion;
+ (void)yuv420toJpegYuvImage:(nonnull AnalysisImageWrapper *)yuvImage jpegQuality:(nonnull NSNumber *)jpegQuality completion:(nonnull void (^)(AnalysisImageWrapper * _Nullable, FlutterError * _Nullable))completion;
+ (void)yuv420toNv21YuvImage:(nonnull AnalysisImageWrapper *)yuvImage completion:(nonnull void (^)(AnalysisImageWrapper * _Nullable, FlutterError * _Nullable))completion;

@end

NS_ASSUME_NONNULL_END



================================================
FILE: ios/camerawesome/Sources/camerawesome/include/AspectRatio.h
================================================
//
//  AspectRatio.h
//  camerawesome
//
//  Created by Dimitri Dessus on 14/11/2022.
//

#ifndef AspectRatio_h
#define AspectRatio_h

typedef enum {
  Ratio4_3,
  Ratio16_9,
  Ratio1_1,
} AspectRatio;

#endif /* AspectRatio_h */



================================================
FILE: ios/camerawesome/Sources/camerawesome/include/AspectRatioUtils.h
================================================
//
//  AspectRatioUtils.h
//  camerawesome
//
//  Created by Dimitri Dessus on 29/03/2023.
//

#import <Foundation/Foundation.h>
#import "AspectRatio.h"

NS_ASSUME_NONNULL_BEGIN

@interface AspectRatioUtils : NSObject

+ (AspectRatio)convertAspectRatio:(NSString *)aspectRatioStr;

@end

NS_ASSUME_NONNULL_END



================================================
FILE: ios/camerawesome/Sources/camerawesome/include/CameraDeviceInfo.h
================================================
//
//  CameraDeviceInfo.h
//  camerawesome
//
//  Created by Dimitri Dessus on 29/03/2023.
//

#import <Foundation/Foundation.h>
#import <AVFoundation/AVFoundation.h>

NS_ASSUME_NONNULL_BEGIN

@interface CameraDeviceInfo : NSObject

@property (nonatomic, strong) AVCaptureDevice *device;
@property (nonatomic, strong) AVCaptureDeviceInput *deviceInput;
@property (nonatomic, strong) AVCaptureVideoDataOutput *videoDataOutput;
@property (nonatomic, strong) AVCaptureConnection *captureConnection;
@property (nonatomic, strong) AVCapturePhotoOutput *capturePhotoOutput;

@end

NS_ASSUME_NONNULL_END



================================================
FILE: ios/camerawesome/Sources/camerawesome/include/CameraFlash.h
================================================
//
//  CameraFlash.h
//  camerawesome
//
//  Created by Dimitri Dessus on 27/07/2020.
//

#ifndef CameraFlash_h
#define CameraFlash_h

typedef enum {
  None,   // Flash is disabled
  On,     // Flash is always enabled when photo is taken
  Auto,   // Flash is enabled when user take a photo only if necessary
  Always, // Flash is enabled anytime, then trigger Auto mode when a photo is taken
} CameraFlashMode;

#endif /* CameraFlash_h */



================================================
FILE: ios/camerawesome/Sources/camerawesome/include/CameraPictureController.h
================================================
//
//  CameraPictureController.h
//  camerawesome
//
//  Created by Dimitri Dessus on 08/12/2020.
//

#import <Flutter/Flutter.h>
#import <AVFoundation/AVFoundation.h>
#import <Foundation/Foundation.h>
#import <CoreMotion/CoreMotion.h>
#import <CoreLocation/CoreLocation.h>

#import "CameraSensor.h"
#import "AspectRatio.h"
#import "Pigeon.h"

NS_ASSUME_NONNULL_BEGIN

typedef void(^OnPictureTaken)(void);

@interface CameraPictureController : NSObject <AVCapturePhotoCaptureDelegate>
@property(readonly, nonatomic) NSString *path;
@property(readonly, nonatomic) bool saveGPSLocation;
@property(readonly, nonatomic) bool mirrorFrontCamera;
@property(readonly, copy) void (^completion)(NSNumber * _Nullable, FlutterError * _Nullable);
@property(readonly, nonatomic) PigeonSensorPosition sensorPosition;
@property(readonly, nonatomic) float aspectRatio;
@property(readonly, nonatomic) AspectRatio aspectRatioType;
@property NSInteger orientation;
@property (nonatomic, copy) OnPictureTaken completionBlock;
@property(readonly, nonatomic) CMMotionManager *motionManager;
@property(readonly, nonatomic) AVCaptureDevicePosition cameraPosition;

- (instancetype)initWithPath:(NSString *)path
                orientation:(NSInteger)orientation
            sensorPosition:(PigeonSensorPosition)sensorPosition
            saveGPSLocation:(bool)saveGPSLocation
          mirrorFrontCamera:(bool)mirrorFrontCamera
                aspectRatio:(AspectRatio)aspectRatio
                completion:(nonnull void (^)(NSNumber * _Nullable, FlutterError * _Nullable))completion
                  callback:(OnPictureTaken)callback;
@end

NS_ASSUME_NONNULL_END




================================================
FILE: ios/camerawesome/Sources/camerawesome/include/CameraPreviewTexture.h
================================================
//
//  CameraPreviewTexture.h
//  camerawesome
//
//  Created by Dimitri Dessus on 28/03/2023.
//

#include <stdatomic.h>
#import <libkern/OSAtomic.h>
#import <Foundation/Foundation.h>
#import <Flutter/Flutter.h>

NS_ASSUME_NONNULL_BEGIN

@interface CameraPreviewTexture : NSObject<FlutterTexture>

- (instancetype)init;
- (void)updateBuffer:(CMSampleBufferRef)sampleBuffer;
- (void)dealloc;

@property(readonly) _Atomic(CVPixelBufferRef) latestPixelBuffer;

@end

NS_ASSUME_NONNULL_END



================================================
FILE: ios/camerawesome/Sources/camerawesome/include/CameraQualities.h
================================================
//
//  CameraQualities.h
//  camerawesome
//
//  Created by Dimitri Dessus on 24/07/2020.
//

#import <AVFoundation/AVFoundation.h>
#import "Pigeon.h"

NS_ASSUME_NONNULL_BEGIN

@interface CameraQualities : NSObject

+ (AVCaptureSessionPreset)selectVideoCapturePreset:(CGSize)size session:(AVCaptureSession *)session device:(AVCaptureDevice *)device;
+ (AVCaptureSessionPreset)selectVideoCapturePreset:(AVCaptureSession *)session device:(AVCaptureDevice *)device;
+ (CGSize)getSizeForPreset:(NSString *)preset;
+ (NSArray *)captureFormatsForDevice:(AVCaptureDevice *)device;

@end

NS_ASSUME_NONNULL_END



================================================
FILE: ios/camerawesome/Sources/camerawesome/include/CameraSensor.h
================================================
//
//  CameraSensor.h
//  Pods
//
//  Created by Dimitri Dessus on 23/07/2020.
//

#ifndef CameraSensor_h
#define CameraSensor_h

//typedef enum {
//  Front,
//  Back,
//} CameraSensor;

#endif /* CameraSensor_h */



================================================
FILE: ios/camerawesome/Sources/camerawesome/include/CameraSensorType.h
================================================
//
//  CameraSensorType.h
//  camerawesome
//
//  Created by Dimitri Dessus on 02/01/2023.
//

#ifndef CameraSensorType_h
#define CameraSensorType_h

typedef enum {
  // back
  wideAngle,
  ultraWideAngle,
  telephoto,
  
  // front
  trueDepth
} CameraSensorType;

#endif /* CameraSensorType_h */



================================================
FILE: ios/camerawesome/Sources/camerawesome/include/CamerawesomePlugin.h
================================================
#import <Flutter/Flutter.h>

NS_ASSUME_NONNULL_BEGIN

@interface CamerawesomePlugin : NSObject<FlutterPlugin, FlutterStreamHandler>

@end

NS_ASSUME_NONNULL_END



================================================
FILE: ios/camerawesome/Sources/camerawesome/include/CaptureModes.h
================================================
//
//  CaptureModes.h
//  Pods
//
//  Created by Dimitri Dessus on 26/11/2020.
//

#ifndef CaptureModes_h
#define CaptureModes_h

typedef enum {
  Photo,
  Video,
  Preview,
  AnalysisOnly
} CaptureModes;

#endif /* CaptureModes_h */



================================================
FILE: ios/camerawesome/Sources/camerawesome/include/CaptureModeUtils.h
================================================
//
//  CaptureModeUtils.h
//  camerawesome
//
//  Created by Dimitri Dessus on 10/05/2023.
//

#import <Foundation/Foundation.h>
#import "CaptureModes.h"

NS_ASSUME_NONNULL_BEGIN

@interface CaptureModeUtils : NSObject

+ (CaptureModes)captureModeFromCaptureModeType:(NSString *)captureModeType;

@end

NS_ASSUME_NONNULL_END



================================================
FILE: ios/camerawesome/Sources/camerawesome/include/ExifContainer.h
================================================
//
//  ExifContainer.h
//  camerawesome
//
//  Created by Dimitri Dessus on 30/09/2022.
//  Taken from: https://github.com/Nikita2k/SimpleExif

#import <Foundation/Foundation.h>

NS_ASSUME_NONNULL_BEGIN

@class CLLocation;

@interface ExifContainer : NSObject

- (void)addLocation:(CLLocation *)currentLocation;
- (void)addUserComment:(NSString *)comment;
- (void)addCreationDate:(NSDate *)date;
- (void)addDescription:(NSString *)description;
- (void)addProjection:(NSString *)projection;

- (void)setValue:(NSString *)key forExifKey:(NSString *)value;

- (NSDictionary *)exifData;
@end

NS_ASSUME_NONNULL_END



================================================
FILE: ios/camerawesome/Sources/camerawesome/include/FlashModeUtils.h
================================================
//
//  FlashModeUtils.h
//  camerawesome
//
//  Created by Dimitri Dessus on 29/03/2023.
//

#import <Foundation/Foundation.h>
#import "CameraFlash.h"

NS_ASSUME_NONNULL_BEGIN

@interface FlashModeUtils : NSObject

+ (CameraFlashMode)flashFromString:(NSString *)mode;

@end

NS_ASSUME_NONNULL_END



================================================
FILE: ios/camerawesome/Sources/camerawesome/include/ImageStreamController.h
================================================
//
//  ImageStreamController.h
//  camerawesome
//
//  Created by Dimitri Dessus on 17/12/2020.
//

#import <Flutter/Flutter.h>
#import <Foundation/Foundation.h>
#import <AVFoundation/AVFoundation.h>
#import "InputImageRotation.h"

NS_ASSUME_NONNULL_BEGIN

@interface ImageStreamController : NSObject

@property(readonly, nonatomic) bool streamImages;
@property(readonly, nonatomic) float maxFramesPerSecond;
@property(readonly, nonatomic) NSDate *latestEmittedFrame;
@property(nonatomic) FlutterEventSink imageStreamEventSink;

@property(readonly, nonatomic) NSInteger processingImage;

- (instancetype)initWithStreamImages:(bool)streamImages;
- (void)captureOutput:(AVCaptureOutput *)output didOutputSampleBuffer:(CMSampleBufferRef)sampleBuffer fromConnection:(AVCaptureConnection *)connection orientation:(UIDeviceOrientation)orientation;
- (void)setImageStreamEventSink:(FlutterEventSink)imageStreamEventSink;
- (void)setStreamImages:(bool)streamImages;
- (void)receivedImageFromStream;
- (void)setMaxFramesPerSecond:(float)maxFramesPerSecond;

@end

NS_ASSUME_NONNULL_END



================================================
FILE: ios/camerawesome/Sources/camerawesome/include/InputAnalysisImageFormat.h
================================================
//
//  InputAnalysisImageFormat.h
//  Pods
//
//  Created by Dimitri Dessus on 26/12/2022.
//

#ifndef InputAnalysisImageFormat_h
#define InputAnalysisImageFormat_h

typedef enum {
  yuv_420_888,
  nv21,
  yv12,
  jpeg,
  yuv420,
  bgra8888,
  unknown
} InputAnalysisImageFormat;

#endif /* InputAnalysisImageFormat_h */



================================================
FILE: ios/camerawesome/Sources/camerawesome/include/InputImageRotation.h
================================================
//
//  InputImageRotation.h
//  camerawesome
//
//  Created by Dimitri Dessus on 26/12/2022.
//

#ifndef InputImageRotation_h
#define InputImageRotation_h

typedef enum {
  rotation0deg,
  rotation90deg,
  rotation180deg,
  rotation270deg
} InputImageRotation;

#endif /* InputImageRotation_h */



================================================
FILE: ios/camerawesome/Sources/camerawesome/include/LocationController.h
================================================
//
//  LocationController.h
//  camerawesome
//
//  Created by Dimitri Dessus on 07/09/2022.
//

#import <Flutter/Flutter.h>
#import <Foundation/Foundation.h>
#import <CoreLocation/CoreLocation.h>

NS_ASSUME_NONNULL_BEGIN

typedef void(^OnAuthorizationDeclined)(void);
typedef void(^OnAuthorizationGranted)(void);

@interface LocationController : NSObject<CLLocationManagerDelegate>

@property (strong, nonatomic, nonnull) CLLocationManager *locationManager;
@property (nonatomic, copy) OnAuthorizationDeclined declinedBlock;
@property (nonatomic, copy) OnAuthorizationGranted grantedBlock;

- (instancetype)init;
- (void)requestWhenInUseAuthorizationOnGranted:(OnAuthorizationGranted)granted declined:(OnAuthorizationDeclined)declined;

@end

NS_ASSUME_NONNULL_END



================================================
FILE: ios/camerawesome/Sources/camerawesome/include/MotionController.h
================================================
//
//  MotionController.h
//  camerawesome
//
//  Created by Dimitri Dessus on 17/12/2020.
//

#import <Foundation/Foundation.h>
#import <CoreMotion/CoreMotion.h>
#import <Flutter/Flutter.h>

NS_ASSUME_NONNULL_BEGIN

@interface MotionController : NSObject

@property(nonatomic) FlutterEventSink orientationEventSink;
@property(readonly, nonatomic) UIDeviceOrientation deviceOrientation;
@property(readonly, nonatomic) CMMotionManager *motionManager;

- (instancetype)init;
- (void)startMotionDetection;
- (void)stopMotionDetection;
- (void)setOrientationEventSink:(FlutterEventSink)orientationEventSink;

@end

NS_ASSUME_NONNULL_END



================================================
FILE: ios/camerawesome/Sources/camerawesome/include/MultiCameraController.h
================================================
//
//  MultiCameraController.h
//  camerawesome
//
//  Created by Dimitri Dessus on 29/03/2023.
//

#import <Foundation/Foundation.h>
#import <AVFoundation/AVFoundation.h>

NS_ASSUME_NONNULL_BEGIN

@interface MultiCameraController : NSObject

+ (BOOL)isMultiCamSupported;

@end

NS_ASSUME_NONNULL_END



================================================
FILE: ios/camerawesome/Sources/camerawesome/include/MultiCameraPreview.h
================================================
//
//  MultiCameraPreview.h
//  camerawesome
//
//  Created by Dimitri Dessus on 28/03/2023.
//

#import <Foundation/Foundation.h>
#import <AVFoundation/AVFoundation.h>

// #import "MultiCameraPreview.h"
#import "CameraPreviewTexture.h"
#import "CameraQualities.h"
#import "CameraDeviceInfo.h"
#import "CameraPictureController.h"
#import "MotionController.h"
#import "ImageStreamController.h"
#import "PhysicalButtonController.h"
#import "AspectRatio.h"
#import "LocationController.h"
#import "CameraFlash.h"
#import "CaptureModes.h"
#import "SensorUtils.h"

NS_ASSUME_NONNULL_BEGIN

@interface MultiCameraPreview : NSObject<AVCaptureVideoDataOutputSampleBufferDelegate,
AVCaptureAudioDataOutputSampleBufferDelegate>

@property (nonatomic, strong) AVCaptureMultiCamSession  *cameraSession;

@property (nonatomic, strong) NSArray<PigeonSensor *> *sensors;
@property (nonatomic, strong) NSMutableArray<CameraDeviceInfo *> *devices;
@property (nonatomic, strong) dispatch_queue_t dispatchQueue;
@property(readonly, nonatomic) AVCaptureFlashMode flashMode;
@property(readonly, nonatomic) AVCaptureTorchMode torchMode;
@property(readonly, nonatomic) AspectRatio aspectRatio;
@property(readonly, nonatomic) LocationController *locationController;
@property(readonly, nonatomic) MotionController *motionController;
@property(readonly, nonatomic) PhysicalButtonController *physicalButtonController;
@property(readonly, nonatomic) bool saveGPSLocation;
@property(readonly, nonatomic) bool mirrorFrontCamera;
@property(nonatomic, nonatomic) NSMutableArray<CameraPreviewTexture *> *textures;
@property(nonatomic, copy) void (^onPreviewFrameAvailable)(NSNumber * _Nullable);

- (instancetype)initWithSensors:(NSArray<PigeonSensor *> *)sensors mirrorFrontCamera:(BOOL)mirrorFrontCamera
           enablePhysicalButton:(BOOL)enablePhysicalButton
                aspectRatioMode:(AspectRatio)aspectRatioMode
                    captureMode:(CaptureModes)captureMode
                  dispatchQueue:(dispatch_queue_t)dispatchQueue;
- (void)configInitialSession:(NSArray<PigeonSensor *> *)sensors;
- (void)setSensors:(NSArray<PigeonSensor *> *)sensors;
- (void)setMirrorFrontCamera:(bool)value error:(FlutterError * _Nullable __autoreleasing * _Nonnull)error;
- (void)setBrightness:(NSNumber *)brightness error:(FlutterError * _Nullable __autoreleasing * _Nonnull)error;
- (void)setFlashMode:(CameraFlashMode)flashMode error:(FlutterError * _Nullable __autoreleasing * _Nonnull)error;
- (void)focusOnPoint:(CGPoint)position preview:(CGSize)preview error:(FlutterError * _Nullable __autoreleasing * _Nonnull)error;
- (void)setZoom:(float)value error:(FlutterError * _Nullable __autoreleasing * _Nonnull)error;
- (void)start;
- (void)stop;
- (void)refresh;
- (CGFloat)getMaxZoom;
- (void)setPreviewSize:(CGSize)previewSize error:(FlutterError * _Nullable __autoreleasing * _Nonnull)error;
- (CGSize)getEffectivPreviewSize;
- (void)takePhotoSensors:(nonnull NSArray<PigeonSensor *> *)sensors paths:(nonnull NSArray<NSString *> *)paths completion:(nonnull void (^)(NSNumber * _Nullable, FlutterError * _Nullable))completion;
- (void)dispose;
- (void)setAspectRatio:(AspectRatio)ratio;
- (void)setExifPreferencesGPSLocation:(bool)gpsLocation completion:(void(^)(NSNumber *_Nullable, FlutterError *_Nullable))completion;
- (void)setOrientationEventSink:(FlutterEventSink)orientationEventSink;
- (void)setPhysicalButtonEventSink:(FlutterEventSink)physicalButtonEventSink;

@end

NS_ASSUME_NONNULL_END



================================================
FILE: ios/camerawesome/Sources/camerawesome/include/NSData+Exif.h
================================================
//
//  UIImage+Exif.h
//  camerawesome
//
//  Created by Dimitri Dessus on 30/09/2022.
//  Modified from: https://github.com/Nikita2k/SimpleExif

#import <Foundation/Foundation.h>

NS_ASSUME_NONNULL_BEGIN

@class ExifContainer;

@interface NSData (Exif)

- (NSData *)addExif:(ExifContainer *)container;

@end

NS_ASSUME_NONNULL_END



================================================
FILE: ios/camerawesome/Sources/camerawesome/include/Permissions.h
================================================
//
//  Permissions.h
//  camerawesome
//
//  Created by Dimitri Dessus on 20/01/2023.
//

#ifndef Permissions_h
#define Permissions_h

typedef enum {
  storage,
  camera,
  location,
  record_audio,
} Permissions;

#endif /* Permissions_h */



================================================
FILE: ios/camerawesome/Sources/camerawesome/include/PermissionsController.h
================================================
//
//  PermissionsController.h
//  _NIODataStructures
//
//  Created by Dimitri Dessus on 27/12/2022.
//

#import <Foundation/Foundation.h>
#import <AVFoundation/AVFoundation.h>

NS_ASSUME_NONNULL_BEGIN

@interface CameraPermissionsController : NSObject

+ (BOOL)checkPermission;
+ (BOOL)checkAndRequestPermission;

@end

@interface MicrophonePermissionsController : NSObject

+ (BOOL)checkPermission;
+ (BOOL)checkAndRequestPermission;

@end

NS_ASSUME_NONNULL_END



================================================
FILE: ios/camerawesome/Sources/camerawesome/include/PhysicalButton.h
================================================
//
//  PhysicalButton.h
//  camerawesome
//
//  Created by Dimitri Dessus on 20/03/2023.
//

#ifndef PhysicalButton_h
#define PhysicalButton_h

typedef enum {
  volume_up,
  volume_down,
} PhysicalButton;

#endif /* PhysicalButton_h */



================================================
FILE: ios/camerawesome/Sources/camerawesome/include/PhysicalButtonController.h
================================================
//
//  PhysicalButtonsController.h
//  Pods
//
//  Created by Dimitri Dessus on 20/03/2023.
//

#import <Foundation/Foundation.h>
#import <Flutter/Flutter.h>
#import "PhysicalButton.h"

NS_ASSUME_NONNULL_BEGIN

@interface PhysicalButtonController : NSObject {
  NSTimer *debounceTimer;
}

@property(nonatomic) FlutterEventSink physicalButtonEventSink;
// @property(nonatomic) JPSVolumeButtonHandler *volumeButtonHandler;

- (instancetype)init;
- (void)stopListening;
- (void)startListening;
- (void)setPhysicalButtonEventSink:(FlutterEventSink)physicalButtonEventSink;

@end

NS_ASSUME_NONNULL_END



================================================
FILE: ios/camerawesome/Sources/camerawesome/include/Pigeon.h
================================================
// Autogenerated from Pigeon (v9.2.5), do not edit directly.
// See also: https://pub.dev/packages/pigeon

#import <Foundation/Foundation.h>

@protocol FlutterBinaryMessenger;
@protocol FlutterMessageCodec;
@class FlutterError;
@class FlutterStandardTypedData;

NS_ASSUME_NONNULL_BEGIN

typedef NS_ENUM(NSUInteger, PigeonSensorPosition) {
  PigeonSensorPositionBack = 0,
  PigeonSensorPositionFront = 1,
  PigeonSensorPositionUnknown = 2,
};

/// Video recording quality, from [sd] to [uhd], with [highest] and [lowest] to
/// let the device choose the best/worst quality available.
/// [highest] is the default quality.
///
/// Qualities are defined like this:
/// [sd] < [hd] < [fhd] < [uhd]
typedef NS_ENUM(NSUInteger, VideoRecordingQuality) {
  VideoRecordingQualityLowest = 0,
  VideoRecordingQualitySd = 1,
  VideoRecordingQualityHd = 2,
  VideoRecordingQualityFhd = 3,
  VideoRecordingQualityUhd = 4,
  VideoRecordingQualityHighest = 5,
};

/// If the specified [VideoRecordingQuality] is not available on the device,
/// the [VideoRecordingQuality] will fallback to [higher] or [lower] quality.
/// [higher] is the default fallback strategy.
typedef NS_ENUM(NSUInteger, QualityFallbackStrategy) {
  QualityFallbackStrategyHigher = 0,
  QualityFallbackStrategyLower = 1,
};

typedef NS_ENUM(NSUInteger, CupertinoFileType) {
  CupertinoFileTypeQuickTimeMovie = 0,
  CupertinoFileTypeMpeg4 = 1,
  CupertinoFileTypeAppleM4V = 2,
  CupertinoFileTypeType3GPP = 3,
  CupertinoFileTypeType3GPP2 = 4,
};

typedef NS_ENUM(NSUInteger, CupertinoCodecType) {
  CupertinoCodecTypeH264 = 0,
  CupertinoCodecTypeHevc = 1,
  CupertinoCodecTypeHevcWithAlpha = 2,
  CupertinoCodecTypeJpeg = 3,
  CupertinoCodecTypeAppleProRes4444 = 4,
  CupertinoCodecTypeAppleProRes422 = 5,
  CupertinoCodecTypeAppleProRes422HQ = 6,
  CupertinoCodecTypeAppleProRes422LT = 7,
  CupertinoCodecTypeAppleProRes422Proxy = 8,
};

typedef NS_ENUM(NSUInteger, PigeonSensorType) {
  /// A built-in wide-angle camera.
  ///
  /// The wide angle sensor is the default sensor for iOS
  PigeonSensorTypeWideAngle = 0,
  /// A built-in camera with a shorter focal length than that of the wide-angle camera.
  PigeonSensorTypeUltraWideAngle = 1,
  /// A built-in camera device with a longer focal length than the wide-angle camera.
  PigeonSensorTypeTelephoto = 2,
  /// A device that consists of two cameras, one Infrared and one YUV.
  ///
  /// iOS only
  PigeonSensorTypeTrueDepth = 3,
  PigeonSensorTypeUnknown = 4,
};

typedef NS_ENUM(NSUInteger, CamerAwesomePermission) {
  CamerAwesomePermissionStorage = 0,
  CamerAwesomePermissionCamera = 1,
  CamerAwesomePermissionLocation = 2,
  CamerAwesomePermissionRecord_audio = 3,
};

typedef NS_ENUM(NSUInteger, AnalysisImageFormat) {
  AnalysisImageFormatYuv_420 = 0,
  AnalysisImageFormatBgra8888 = 1,
  AnalysisImageFormatJpeg = 2,
  AnalysisImageFormatNv21 = 3,
  AnalysisImageFormatUnknown = 4,
};

typedef NS_ENUM(NSUInteger, AnalysisRotation) {
  AnalysisRotationRotation0deg = 0,
  AnalysisRotationRotation90deg = 1,
  AnalysisRotationRotation180deg = 2,
  AnalysisRotationRotation270deg = 3,
};

@class PreviewSize;
@class ExifPreferences;
@class PigeonSensor;
@class VideoOptions;
@class AndroidVideoOptions;
@class CupertinoVideoOptions;
@class PigeonSensorTypeDevice;
@class AndroidFocusSettings;
@class PlaneWrapper;
@class CropRectWrapper;
@class AnalysisImageWrapper;

@interface PreviewSize : NSObject
/// `init` unavailable to enforce nonnull fields, see the `make` class method.
- (instancetype)init NS_UNAVAILABLE;
+ (instancetype)makeWithWidth:(NSNumber *)width
    height:(NSNumber *)height;
@property(nonatomic, strong) NSNumber * width;
@property(nonatomic, strong) NSNumber * height;
@end

@interface ExifPreferences : NSObject
/// `init` unavailable to enforce nonnull fields, see the `make` class method.
- (instancetype)init NS_UNAVAILABLE;
+ (instancetype)makeWithSaveGPSLocation:(NSNumber *)saveGPSLocation;
@property(nonatomic, strong) NSNumber * saveGPSLocation;
@end

@interface PigeonSensor : NSObject
/// `init` unavailable to enforce nonnull fields, see the `make` class method.
- (instancetype)init NS_UNAVAILABLE;
+ (instancetype)makeWithPosition:(PigeonSensorPosition)position
    type:(PigeonSensorType)type
    deviceId:(nullable NSString *)deviceId;
@property(nonatomic, assign) PigeonSensorPosition position;
@property(nonatomic, assign) PigeonSensorType type;
@property(nonatomic, copy, nullable) NSString * deviceId;
@end

/// Video recording options. Some of them are specific to each platform.
@interface VideoOptions : NSObject
/// `init` unavailable to enforce nonnull fields, see the `make` class method.
- (instancetype)init NS_UNAVAILABLE;
+ (instancetype)makeWithEnableAudio:(NSNumber *)enableAudio
    quality:(VideoRecordingQuality)quality
    android:(nullable AndroidVideoOptions *)android
    ios:(nullable CupertinoVideoOptions *)ios;
/// Enable audio while video recording
@property(nonatomic, strong) NSNumber * enableAudio;
/// The quality of the video recording, defaults to [VideoRecordingQuality.highest].
@property(nonatomic, assign) VideoRecordingQuality quality;
@property(nonatomic, strong, nullable) AndroidVideoOptions * android;
@property(nonatomic, strong, nullable) CupertinoVideoOptions * ios;
@end

@interface AndroidVideoOptions : NSObject
+ (instancetype)makeWithBitrate:(nullable NSNumber *)bitrate
    fallbackStrategy:(QualityFallbackStrategy)fallbackStrategy;
/// The bitrate of the video recording. Only set it if a custom bitrate is
/// desired.
@property(nonatomic, strong, nullable) NSNumber * bitrate;
@property(nonatomic, assign) QualityFallbackStrategy fallbackStrategy;
@end

@interface CupertinoVideoOptions : NSObject
+ (instancetype)makeWithFileType:(CupertinoFileType)fileType
    codec:(CupertinoCodecType)codec
    fps:(nullable NSNumber *)fps;
/// Specify video file type, defaults to [AVFileTypeQuickTimeMovie].
@property(nonatomic, assign) CupertinoFileType fileType;
/// Specify video codec, defaults to [AVVideoCodecTypeH264].
@property(nonatomic, assign) CupertinoCodecType codec;
/// Specify video fps, defaults to [30].
@property(nonatomic, strong, nullable) NSNumber * fps;
@end

@interface PigeonSensorTypeDevice : NSObject
/// `init` unavailable to enforce nonnull fields, see the `make` class method.
- (instancetype)init NS_UNAVAILABLE;
+ (instancetype)makeWithSensorType:(PigeonSensorType)sensorType
    name:(NSString *)name
    iso:(NSNumber *)iso
    flashAvailable:(NSNumber *)flashAvailable
    uid:(NSString *)uid;
@property(nonatomic, assign) PigeonSensorType sensorType;
/// A localized device name for display in the user interface.
@property(nonatomic, copy) NSString * name;
/// The current exposure ISO value.
@property(nonatomic, strong) NSNumber * iso;
/// A Boolean value that indicates whether the flash is currently available for use.
@property(nonatomic, strong) NSNumber * flashAvailable;
/// An identifier that uniquely identifies the device.
@property(nonatomic, copy) NSString * uid;
@end

@interface AndroidFocusSettings : NSObject
/// `init` unavailable to enforce nonnull fields, see the `make` class method.
- (instancetype)init NS_UNAVAILABLE;
+ (instancetype)makeWithAutoCancelDurationInMillis:(NSNumber *)autoCancelDurationInMillis;
/// The auto focus will be canceled after the given [autoCancelDurationInMillis].
/// If [autoCancelDurationInMillis] is equals to 0 (or less), the auto focus
/// will **not** be canceled. A manual `focusOnPoint` call will be needed to
/// focus on an other point.
/// Minimal duration of [autoCancelDurationInMillis] is 1000 ms. If set
/// between 0 (exclusive) and 1000 (exclusive), it will be raised to 1000.
@property(nonatomic, strong) NSNumber * autoCancelDurationInMillis;
@end

@interface PlaneWrapper : NSObject
/// `init` unavailable to enforce nonnull fields, see the `make` class method.
- (instancetype)init NS_UNAVAILABLE;
+ (instancetype)makeWithBytes:(FlutterStandardTypedData *)bytes
    bytesPerRow:(NSNumber *)bytesPerRow
    bytesPerPixel:(nullable NSNumber *)bytesPerPixel
    width:(nullable NSNumber *)width
    height:(nullable NSNumber *)height;
@property(nonatomic, strong) FlutterStandardTypedData * bytes;
@property(nonatomic, strong) NSNumber * bytesPerRow;
@property(nonatomic, strong, nullable) NSNumber * bytesPerPixel;
@property(nonatomic, strong, nullable) NSNumber * width;
@property(nonatomic, strong, nullable) NSNumber * height;
@end

@interface CropRectWrapper : NSObject
/// `init` unavailable to enforce nonnull fields, see the `make` class method.
- (instancetype)init NS_UNAVAILABLE;
+ (instancetype)makeWithLeft:(NSNumber *)left
    top:(NSNumber *)top
    width:(NSNumber *)width
    height:(NSNumber *)height;
@property(nonatomic, strong) NSNumber * left;
@property(nonatomic, strong) NSNumber * top;
@property(nonatomic, strong) NSNumber * width;
@property(nonatomic, strong) NSNumber * height;
@end

@interface AnalysisImageWrapper : NSObject
/// `init` unavailable to enforce nonnull fields, see the `make` class method.
- (instancetype)init NS_UNAVAILABLE;
+ (instancetype)makeWithFormat:(AnalysisImageFormat)format
    bytes:(nullable FlutterStandardTypedData *)bytes
    width:(NSNumber *)width
    height:(NSNumber *)height
    planes:(nullable NSArray<PlaneWrapper *> *)planes
    cropRect:(nullable CropRectWrapper *)cropRect
    rotation:(AnalysisRotation)rotation;
@property(nonatomic, assign) AnalysisImageFormat format;
@property(nonatomic, strong, nullable) FlutterStandardTypedData * bytes;
@property(nonatomic, strong) NSNumber * width;
@property(nonatomic, strong) NSNumber * height;
@property(nonatomic, strong, nullable) NSArray<PlaneWrapper *> * planes;
@property(nonatomic, strong, nullable) CropRectWrapper * cropRect;
@property(nonatomic, assign) AnalysisRotation rotation;
@end

/// The codec used by AnalysisImageUtils.
NSObject<FlutterMessageCodec> *AnalysisImageUtilsGetCodec(void);

@protocol AnalysisImageUtils
- (void)nv21toJpegNv21Image:(AnalysisImageWrapper *)nv21Image jpegQuality:(NSNumber *)jpegQuality completion:(void (^)(AnalysisImageWrapper *_Nullable, FlutterError *_Nullable))completion;
- (void)yuv420toJpegYuvImage:(AnalysisImageWrapper *)yuvImage jpegQuality:(NSNumber *)jpegQuality completion:(void (^)(AnalysisImageWrapper *_Nullable, FlutterError *_Nullable))completion;
- (void)yuv420toNv21YuvImage:(AnalysisImageWrapper *)yuvImage completion:(void (^)(AnalysisImageWrapper *_Nullable, FlutterError *_Nullable))completion;
- (void)bgra8888toJpegBgra8888image:(AnalysisImageWrapper *)bgra8888image jpegQuality:(NSNumber *)jpegQuality completion:(void (^)(AnalysisImageWrapper *_Nullable, FlutterError *_Nullable))completion;
@end

extern void AnalysisImageUtilsSetup(id<FlutterBinaryMessenger> binaryMessenger, NSObject<AnalysisImageUtils> *_Nullable api);

/// The codec used by CameraInterface.
NSObject<FlutterMessageCodec> *CameraInterfaceGetCodec(void);

@protocol CameraInterface
- (void)setupCameraSensors:(NSArray<PigeonSensor *> *)sensors aspectRatio:(NSString *)aspectRatio zoom:(NSNumber *)zoom mirrorFrontCamera:(NSNumber *)mirrorFrontCamera enablePhysicalButton:(NSNumber *)enablePhysicalButton flashMode:(NSString *)flashMode captureMode:(NSString *)captureMode enableImageStream:(NSNumber *)enableImageStream exifPreferences:(ExifPreferences *)exifPreferences videoOptions:(nullable VideoOptions *)videoOptions completion:(void (^)(NSNumber *_Nullable, FlutterError *_Nullable))completion;
/// @return `nil` only when `error != nil`.
- (nullable NSArray<NSString *> *)checkPermissionsPermissions:(NSArray<NSString *> *)permissions error:(FlutterError *_Nullable *_Nonnull)error;
/// Returns given [CamerAwesomePermission] list (as String). Location permission might be
/// refused but the app should still be able to run.
- (void)requestPermissionsSaveGpsLocation:(NSNumber *)saveGpsLocation completion:(void (^)(NSArray<NSString *> *_Nullable, FlutterError *_Nullable))completion;
/// @return `nil` only when `error != nil`.
- (nullable NSNumber *)getPreviewTextureIdCameraPosition:(NSNumber *)cameraPosition error:(FlutterError *_Nullable *_Nonnull)error;
- (void)takePhotoSensors:(NSArray<PigeonSensor *> *)sensors paths:(NSArray<NSString *> *)paths completion:(void (^)(NSNumber *_Nullable, FlutterError *_Nullable))completion;
- (void)recordVideoSensors:(NSArray<PigeonSensor *> *)sensors paths:(NSArray<NSString *> *)paths completion:(void (^)(FlutterError *_Nullable))completion;
- (void)pauseVideoRecordingWithError:(FlutterError *_Nullable *_Nonnull)error;
- (void)resumeVideoRecordingWithError:(FlutterError *_Nullable *_Nonnull)error;
- (void)receivedImageFromStreamWithError:(FlutterError *_Nullable *_Nonnull)error;
- (void)stopRecordingVideoWithCompletion:(void (^)(NSNumber *_Nullable, FlutterError *_Nullable))completion;
/// @return `nil` only when `error != nil`.
- (nullable NSArray<PigeonSensorTypeDevice *> *)getFrontSensorsWithError:(FlutterError *_Nullable *_Nonnull)error;
/// @return `nil` only when `error != nil`.
- (nullable NSArray<PigeonSensorTypeDevice *> *)getBackSensorsWithError:(FlutterError *_Nullable *_Nonnull)error;
/// @return `nil` only when `error != nil`.
- (nullable NSNumber *)startWithError:(FlutterError *_Nullable *_Nonnull)error;
/// @return `nil` only when `error != nil`.
- (nullable NSNumber *)stopWithError:(FlutterError *_Nullable *_Nonnull)error;
- (void)setFlashModeMode:(NSString *)mode error:(FlutterError *_Nullable *_Nonnull)error;
- (void)handleAutoFocusWithError:(FlutterError *_Nullable *_Nonnull)error;
/// Starts auto focus on a point at ([x], [y]).
///
/// On Android, you can control after how much time you want to switch back
/// to passive focus mode with [androidFocusSettings].
- (void)focusOnPointPreviewSize:(PreviewSize *)previewSize x:(NSNumber *)x y:(NSNumber *)y androidFocusSettings:(nullable AndroidFocusSettings *)androidFocusSettings error:(FlutterError *_Nullable *_Nonnull)error;
- (void)setZoomZoom:(NSNumber *)zoom error:(FlutterError *_Nullable *_Nonnull)error;
- (void)setMirrorFrontCameraMirror:(NSNumber *)mirror error:(FlutterError *_Nullable *_Nonnull)error;
- (void)setSensorSensors:(NSArray<PigeonSensor *> *)sensors error:(FlutterError *_Nullable *_Nonnull)error;
- (void)setCorrectionBrightness:(NSNumber *)brightness error:(FlutterError *_Nullable *_Nonnull)error;
/// @return `nil` only when `error != nil`.
- (nullable NSNumber *)getMinZoomWithError:(FlutterError *_Nullable *_Nonnull)error;
/// @return `nil` only when `error != nil`.
- (nullable NSNumber *)getMaxZoomWithError:(FlutterError *_Nullable *_Nonnull)error;
- (void)setCaptureModeMode:(NSString *)mode error:(FlutterError *_Nullable *_Nonnull)error;
- (void)setRecordingAudioModeEnableAudio:(NSNumber *)enableAudio completion:(void (^)(NSNumber *_Nullable, FlutterError *_Nullable))completion;
/// @return `nil` only when `error != nil`.
- (nullable NSArray<PreviewSize *> *)availableSizesWithError:(FlutterError *_Nullable *_Nonnull)error;
- (void)refreshWithError:(FlutterError *_Nullable *_Nonnull)error;
- (nullable PreviewSize *)getEffectivPreviewSizeIndex:(NSNumber *)index error:(FlutterError *_Nullable *_Nonnull)error;
- (void)setPhotoSizeSize:(PreviewSize *)size error:(FlutterError *_Nullable *_Nonnull)error;
- (void)setPreviewSizeSize:(PreviewSize *)size error:(FlutterError *_Nullable *_Nonnull)error;
- (void)setAspectRatioAspectRatio:(NSString *)aspectRatio error:(FlutterError *_Nullable *_Nonnull)error;
- (void)setupImageAnalysisStreamFormat:(NSString *)format width:(NSNumber *)width maxFramesPerSecond:(nullable NSNumber *)maxFramesPerSecond autoStart:(NSNumber *)autoStart error:(FlutterError *_Nullable *_Nonnull)error;
- (void)setExifPreferencesExifPreferences:(ExifPreferences *)exifPreferences completion:(void (^)(NSNumber *_Nullable, FlutterError *_Nullable))completion;
- (void)startAnalysisWithError:(FlutterError *_Nullable *_Nonnull)error;
- (void)stopAnalysisWithError:(FlutterError *_Nullable *_Nonnull)error;
- (void)setFilterMatrix:(NSArray<NSNumber *> *)matrix error:(FlutterError *_Nullable *_Nonnull)error;
- (void)isVideoRecordingAndImageAnalysisSupportedSensor:(PigeonSensorPosition)sensor completion:(void (^)(NSNumber *_Nullable, FlutterError *_Nullable))completion;
/// @return `nil` only when `error != nil`.
- (nullable NSNumber *)isMultiCamSupportedWithError:(FlutterError *_Nullable *_Nonnull)error;
@end

extern void CameraInterfaceSetup(id<FlutterBinaryMessenger> binaryMessenger, NSObject<CameraInterface> *_Nullable api);

NS_ASSUME_NONNULL_END



================================================
FILE: ios/camerawesome/Sources/camerawesome/include/SensorsController.h
================================================
//
//  SensorsController.h
//  camerawesome
//
//  Created by Dimitri Dessus on 28/03/2023.
//

#import <Foundation/Foundation.h>
#import <AVFoundation/AVFoundation.h>

NS_ASSUME_NONNULL_BEGIN

@interface SensorsController : NSObject

+ (NSArray *)getSensors:(AVCaptureDevicePosition)position;

@end

NS_ASSUME_NONNULL_END



================================================
FILE: ios/camerawesome/Sources/camerawesome/include/SensorUtils.h
================================================
//
//  SensorUtils.h
//  camerawesome
//
//  Created by Dimitri Dessus on 30/03/2023.
//

#import <Foundation/Foundation.h>
#import <AVFoundation/AVFoundation.h>
#import "Pigeon.h"

NS_ASSUME_NONNULL_BEGIN

@interface SensorUtils : NSObject

+ (PigeonSensorType)sensorTypeFromDeviceType:(AVCaptureDeviceType)type;
+ (AVCaptureDeviceType)deviceTypeFromSensorType:(PigeonSensorType)sensorType;

@end

NS_ASSUME_NONNULL_END



================================================
FILE: ios/camerawesome/Sources/camerawesome/include/SingleCameraPreview.h
================================================
//
//  CameraPreview.h
//  camerawesome
//
//  Created by Dimitri Dessus on 23/07/2020.
//

#include <stdatomic.h>

#import <Flutter/Flutter.h>
#import <AVFoundation/AVFoundation.h>
#import <libkern/OSAtomic.h>
#import <Foundation/Foundation.h>

#import "MotionController.h"
#import "LocationController.h"
#import "VideoController.h"
#import "ImageStreamController.h"
#import "CameraSensor.h"
#import "CaptureModes.h"
#import "CameraFlash.h"
#import "CameraQualities.h"
#import "CameraPictureController.h"
#import "PermissionsController.h"
#import "AspectRatio.h"
#import "CameraSensorType.h"
#import "PhysicalButtonController.h"
#import "InputAnalysisImageFormat.h"
#import "CameraPreviewTexture.h"
#import "MultiCameraPreview.h"

NS_ASSUME_NONNULL_BEGIN

@interface SingleCameraPreview : NSObject<AVCaptureVideoDataOutputSampleBufferDelegate,
AVCaptureAudioDataOutputSampleBufferDelegate>

// TODO: move this to a single camera ?
@property(readonly, nonatomic) AVCaptureSession *captureSession;
@property(readonly, nonatomic) AVCaptureDevice *captureDevice;
@property(readonly, nonatomic) AVCaptureInput *captureVideoInput;
@property(readonly, nonatomic) AVCaptureConnection *captureConnection;
@property(readonly, nonatomic) AVCaptureVideoDataOutput *captureVideoOutput;
@property(readonly, nonatomic) AVCaptureVideoPreviewLayer *previewLayer;
@property(readonly, nonatomic) AVCapturePhotoOutput *capturePhotoOutput;

@property(readonly, nonatomic) UIDeviceOrientation deviceOrientation;
@property(readonly, nonatomic) AVCaptureFlashMode flashMode;
@property(readonly, nonatomic) AVCaptureTorchMode torchMode;
@property(readonly, nonatomic) AVCaptureAudioDataOutput *audioOutput;
@property(readonly, nonatomic) PigeonSensorPosition cameraSensorPosition;
@property(readonly, nonatomic) NSString *captureDeviceId;
@property(readonly, nonatomic) CaptureModes captureMode;
@property(readonly, nonatomic) NSString *currentPreset;
@property(readonly, nonatomic) AspectRatio aspectRatio;
@property(readonly, nonatomic) CupertinoVideoOptions *videoOptions;
@property(readonly, nonatomic) VideoRecordingQuality recordingQuality;
@property(readonly, nonatomic) CameraPreviewTexture* previewTexture;
@property(readonly, nonatomic) bool saveGPSLocation;
@property(readonly, nonatomic) bool mirrorFrontCamera;
@property(readonly, nonatomic) CGSize currentPreviewSize;
@property(readonly, nonatomic) ImageStreamController *imageStreamController;
@property(readonly, nonatomic) MotionController *motionController;
@property(readonly, nonatomic) LocationController *locationController;
@property(readonly, nonatomic) VideoController *videoController;
@property(readonly, nonatomic) PhysicalButtonController *physicalButtonController;
@property(readonly, copy) void (^completion)(NSNumber * _Nullable, FlutterError * _Nullable);
@property(nonatomic, copy) void (^onPreviewFrameAvailable)(void);

- (instancetype)initWithCameraSensor:(PigeonSensorPosition)sensor
                        videoOptions:(nullable CupertinoVideoOptions *)videoOptions
                    recordingQuality:(VideoRecordingQuality)recordingQuality
                        streamImages:(BOOL)streamImages
                   mirrorFrontCamera:(BOOL)mirrorFrontCamera
                enablePhysicalButton:(BOOL)enablePhysicalButton
                     aspectRatioMode:(AspectRatio)aspectRatioMode
                         captureMode:(CaptureModes)captureMode
                          completion:(nonnull void (^)(NSNumber * _Nullable, FlutterError * _Nullable))completion
                       dispatchQueue:(dispatch_queue_t)dispatchQueue;
- (void)setImageStreamEvent:(FlutterEventSink)imageStreamEventSink;
- (void)setOrientationEventSink:(FlutterEventSink)orientationEventSink;
- (void)setPhysicalButtonEventSink:(FlutterEventSink)physicalButtonEventSink;
- (void)setPreviewSize:(CGSize)previewSize error:(FlutterError * _Nullable __autoreleasing * _Nonnull)error;
- (void)setFlashMode:(CameraFlashMode)flashMode error:(FlutterError * _Nullable __autoreleasing * _Nonnull)error;
- (void)setCaptureMode:(CaptureModes)captureMode error:(FlutterError * _Nullable __autoreleasing * _Nonnull)error;
- (void)setCameraPreset:(CGSize)currentPreviewSize;
- (void)setRecordingAudioMode:(bool)enableAudio completion:(void(^)(NSNumber *_Nullable, FlutterError *_Nullable))completion;
- (void)pauseVideoRecording;
- (void)resumeVideoRecording;
- (void)receivedImageFromStream;
- (void)setAspectRatio:(AspectRatio)ratio;
- (void)setExifPreferencesGPSLocation:(bool)gpsLocation completion:(void(^)(NSNumber *_Nullable, FlutterError *_Nullable))completion;
- (void)refresh;
- (void)start;
- (void)stop;
- (void)takePictureAtPath:(NSString *)path completion:(nonnull void (^)(NSNumber * _Nullable, FlutterError * _Nullable))completion;
- (void)recordVideoAtPath:(NSString *)path completion:(nonnull void (^)(FlutterError * _Nullable))completion;
- (void)stopRecordingVideo:(nonnull void (^)(NSNumber * _Nullable, FlutterError * _Nullable))completion;
- (void)focusOnPoint:(CGPoint)position preview:(CGSize)preview error:(FlutterError * _Nullable __autoreleasing * _Nonnull)error;
- (void)dispose;
- (void)setSensor:(PigeonSensor *)sensor;
- (void)setZoom:(float)value error:(FlutterError * _Nullable __autoreleasing * _Nonnull)error;
- (void)setMirrorFrontCamera:(bool)value error:(FlutterError * _Nullable __autoreleasing * _Nonnull)error;
- (CGFloat)getMaxZoom;
- (CGSize)getEffectivPreviewSize;
- (void)setUpCaptureSessionForAudioError:(nonnull void (^)(NSError *))error;
- (void)setBrightness:(NSNumber *)brightness error:(FlutterError * _Nullable __autoreleasing * _Nonnull)error;
@end

NS_ASSUME_NONNULL_END



================================================
FILE: ios/camerawesome/Sources/camerawesome/include/VideoController.h
================================================
//
//  VideoController.h
//  camerawesome
//
//  Created by Dimitri Dessus on 17/12/2020.
//

#import <Flutter/Flutter.h>
#import <Foundation/Foundation.h>
#import <AVFoundation/AVFoundation.h>
#import "Pigeon.h"

NS_ASSUME_NONNULL_BEGIN

typedef void(^OnAudioSetup)(void);
typedef void(^OnVideoWriterSetup)(void);

@interface VideoController : NSObject

@property(readonly, nonatomic) bool isRecording;
@property(readonly, nonatomic) bool isPaused;
@property(readonly, nonatomic) bool isAudioEnabled;
@property(readonly, nonatomic) bool isAudioSetup;
@property(readonly, nonatomic) VideoRecordingQuality recordingQuality;
@property(readonly, nonatomic) CupertinoVideoOptions *options;
@property NSInteger orientation;
@property(readonly, nonatomic) AVCaptureDevice *captureDevice;
@property(readonly, nonatomic) AVAssetWriter *videoWriter;
@property(readonly, nonatomic) AVAssetWriterInput *videoWriterInput;
@property(readonly, nonatomic) AVAssetWriterInput *audioWriterInput;
@property(readonly, nonatomic) AVAssetWriterInputPixelBufferAdaptor *videoAdaptor;
@property(readonly, nonatomic) bool videoIsDisconnected;
@property(readonly, nonatomic) bool audioIsDisconnected;
@property(readonly, nonatomic) CGSize previewSize;
@property(assign, nonatomic) CMTime lastVideoSampleTime;
@property(assign, nonatomic) CMTime lastAudioSampleTime;
@property(assign, nonatomic) CMTime videoTimeOffset;
@property(assign, nonatomic) CMTime audioTimeOffset;

- (instancetype)init;
- (void)recordVideoAtPath:(NSString *)path captureDevice:(AVCaptureDevice *)device orientation:(NSInteger)orientation audioSetupCallback:(OnAudioSetup)audioSetupCallback videoWriterCallback:(OnVideoWriterSetup)videoWriterCallback options:(CupertinoVideoOptions *)options quality:(VideoRecordingQuality)quality completion:(nonnull void (^)(FlutterError * _Nullable))completion;
- (void)stopRecordingVideo:(nonnull void (^)(NSNumber * _Nullable, FlutterError * _Nullable))completion;
- (void)pauseVideoRecording;
- (void)resumeVideoRecording;
- (void)captureOutput:(AVCaptureOutput *)output didOutputSampleBuffer:(CMSampleBufferRef)sampleBuffer fromConnection:(AVCaptureConnection *)connection captureVideoOutput:(AVCaptureVideoDataOutput *)captureVideoOutput;
- (void)setIsAudioEnabled:(bool)isAudioEnabled;
- (void)setIsAudioSetup:(bool)isAudioSetup;
- (void)setVideoIsDisconnected:(bool)videoIsDisconnected;
- (void)setAudioIsDisconnected:(bool)audioIsDisconnected;
- (void)setPreviewSize:(CGSize)previewSize;

@end

NS_ASSUME_NONNULL_END



================================================
FILE: ios/camerawesome/Sources/camerawesome/Pigeon/Pigeon.m
================================================
// Autogenerated from Pigeon (v9.2.5), do not edit directly.
// See also: https://pub.dev/packages/pigeon

#import "Pigeon.h"
#import <Flutter/Flutter.h>

#if !__has_feature(objc_arc)
#error File requires ARC to be enabled.
#endif

static NSArray *wrapResult(id result, FlutterError *error) {
  if (error) {
    return @[
      error.code ?: [NSNull null], error.message ?: [NSNull null], error.details ?: [NSNull null]
    ];
  }
  return @[ result ?: [NSNull null] ];
}
static id GetNullableObjectAtIndex(NSArray *array, NSInteger key) {
  id result = array[key];
  return (result == [NSNull null]) ? nil : result;
}

@interface PreviewSize ()
+ (PreviewSize *)fromList:(NSArray *)list;
+ (nullable PreviewSize *)nullableFromList:(NSArray *)list;
- (NSArray *)toList;
@end

@interface ExifPreferences ()
+ (ExifPreferences *)fromList:(NSArray *)list;
+ (nullable ExifPreferences *)nullableFromList:(NSArray *)list;
- (NSArray *)toList;
@end

@interface PigeonSensor ()
+ (PigeonSensor *)fromList:(NSArray *)list;
+ (nullable PigeonSensor *)nullableFromList:(NSArray *)list;
- (NSArray *)toList;
@end

@interface VideoOptions ()
+ (VideoOptions *)fromList:(NSArray *)list;
+ (nullable VideoOptions *)nullableFromList:(NSArray *)list;
- (NSArray *)toList;
@end

@interface AndroidVideoOptions ()
+ (AndroidVideoOptions *)fromList:(NSArray *)list;
+ (nullable AndroidVideoOptions *)nullableFromList:(NSArray *)list;
- (NSArray *)toList;
@end

@interface CupertinoVideoOptions ()
+ (CupertinoVideoOptions *)fromList:(NSArray *)list;
+ (nullable CupertinoVideoOptions *)nullableFromList:(NSArray *)list;
- (NSArray *)toList;
@end

@interface PigeonSensorTypeDevice ()
+ (PigeonSensorTypeDevice *)fromList:(NSArray *)list;
+ (nullable PigeonSensorTypeDevice *)nullableFromList:(NSArray *)list;
- (NSArray *)toList;
@end

@interface AndroidFocusSettings ()
+ (AndroidFocusSettings *)fromList:(NSArray *)list;
+ (nullable AndroidFocusSettings *)nullableFromList:(NSArray *)list;
- (NSArray *)toList;
@end

@interface PlaneWrapper ()
+ (PlaneWrapper *)fromList:(NSArray *)list;
+ (nullable PlaneWrapper *)nullableFromList:(NSArray *)list;
- (NSArray *)toList;
@end

@interface CropRectWrapper ()
+ (CropRectWrapper *)fromList:(NSArray *)list;
+ (nullable CropRectWrapper *)nullableFromList:(NSArray *)list;
- (NSArray *)toList;
@end

@interface AnalysisImageWrapper ()
+ (AnalysisImageWrapper *)fromList:(NSArray *)list;
+ (nullable AnalysisImageWrapper *)nullableFromList:(NSArray *)list;
- (NSArray *)toList;
@end

@implementation PreviewSize
+ (instancetype)makeWithWidth:(NSNumber *)width
    height:(NSNumber *)height {
  PreviewSize* pigeonResult = [[PreviewSize alloc] init];
  pigeonResult.width = width;
  pigeonResult.height = height;
  return pigeonResult;
}
+ (PreviewSize *)fromList:(NSArray *)list {
  PreviewSize *pigeonResult = [[PreviewSize alloc] init];
  pigeonResult.width = GetNullableObjectAtIndex(list, 0);
  NSAssert(pigeonResult.width != nil, @"");
  pigeonResult.height = GetNullableObjectAtIndex(list, 1);
  NSAssert(pigeonResult.height != nil, @"");
  return pigeonResult;
}
+ (nullable PreviewSize *)nullableFromList:(NSArray *)list {
  return (list) ? [PreviewSize fromList:list] : nil;
}
- (NSArray *)toList {
  return @[
    (self.width ?: [NSNull null]),
    (self.height ?: [NSNull null]),
  ];
}
@end

@implementation ExifPreferences
+ (instancetype)makeWithSaveGPSLocation:(NSNumber *)saveGPSLocation {
  ExifPreferences* pigeonResult = [[ExifPreferences alloc] init];
  pigeonResult.saveGPSLocation = saveGPSLocation;
  return pigeonResult;
}
+ (ExifPreferences *)fromList:(NSArray *)list {
  ExifPreferences *pigeonResult = [[ExifPreferences alloc] init];
  pigeonResult.saveGPSLocation = GetNullableObjectAtIndex(list, 0);
  NSAssert(pigeonResult.saveGPSLocation != nil, @"");
  return pigeonResult;
}
+ (nullable ExifPreferences *)nullableFromList:(NSArray *)list {
  return (list) ? [ExifPreferences fromList:list] : nil;
}
- (NSArray *)toList {
  return @[
    (self.saveGPSLocation ?: [NSNull null]),
  ];
}
@end

@implementation PigeonSensor
+ (instancetype)makeWithPosition:(PigeonSensorPosition)position
    type:(PigeonSensorType)type
    deviceId:(nullable NSString *)deviceId {
  PigeonSensor* pigeonResult = [[PigeonSensor alloc] init];
  pigeonResult.position = position;
  pigeonResult.type = type;
  pigeonResult.deviceId = deviceId;
  return pigeonResult;
}
+ (PigeonSensor *)fromList:(NSArray *)list {
  PigeonSensor *pigeonResult = [[PigeonSensor alloc] init];
  pigeonResult.position = [GetNullableObjectAtIndex(list, 0) integerValue];
  pigeonResult.type = [GetNullableObjectAtIndex(list, 1) integerValue];
  pigeonResult.deviceId = GetNullableObjectAtIndex(list, 2);
  return pigeonResult;
}
+ (nullable PigeonSensor *)nullableFromList:(NSArray *)list {
  return (list) ? [PigeonSensor fromList:list] : nil;
}
- (NSArray *)toList {
  return @[
    @(self.position),
    @(self.type),
    (self.deviceId ?: [NSNull null]),
  ];
}
@end

@implementation VideoOptions
+ (instancetype)makeWithEnableAudio:(NSNumber *)enableAudio
    quality:(VideoRecordingQuality)quality
    android:(nullable AndroidVideoOptions *)android
    ios:(nullable CupertinoVideoOptions *)ios {
  VideoOptions* pigeonResult = [[VideoOptions alloc] init];
  pigeonResult.enableAudio = enableAudio;
  pigeonResult.quality = quality;
  pigeonResult.android = android;
  pigeonResult.ios = ios;
  return pigeonResult;
}
+ (VideoOptions *)fromList:(NSArray *)list {
  VideoOptions *pigeonResult = [[VideoOptions alloc] init];
  pigeonResult.enableAudio = GetNullableObjectAtIndex(list, 0);
  NSAssert(pigeonResult.enableAudio != nil, @"");
  pigeonResult.quality = [GetNullableObjectAtIndex(list, 1) integerValue];
  pigeonResult.android = [AndroidVideoOptions nullableFromList:(GetNullableObjectAtIndex(list, 2))];
  pigeonResult.ios = [CupertinoVideoOptions nullableFromList:(GetNullableObjectAtIndex(list, 3))];
  return pigeonResult;
}
+ (nullable VideoOptions *)nullableFromList:(NSArray *)list {
  return (list) ? [VideoOptions fromList:list] : nil;
}
- (NSArray *)toList {
  return @[
    (self.enableAudio ?: [NSNull null]),
    @(self.quality),
    (self.android ? [self.android toList] : [NSNull null]),
    (self.ios ? [self.ios toList] : [NSNull null]),
  ];
}
@end

@implementation AndroidVideoOptions
+ (instancetype)makeWithBitrate:(nullable NSNumber *)bitrate
    fallbackStrategy:(QualityFallbackStrategy)fallbackStrategy {
  AndroidVideoOptions* pigeonResult = [[AndroidVideoOptions alloc] init];
  pigeonResult.bitrate = bitrate;
  pigeonResult.fallbackStrategy = fallbackStrategy;
  return pigeonResult;
}
+ (AndroidVideoOptions *)fromList:(NSArray *)list {
  AndroidVideoOptions *pigeonResult = [[AndroidVideoOptions alloc] init];
  pigeonResult.bitrate = GetNullableObjectAtIndex(list, 0);
  pigeonResult.fallbackStrategy = [GetNullableObjectAtIndex(list, 1) integerValue];
  return pigeonResult;
}
+ (nullable AndroidVideoOptions *)nullableFromList:(NSArray *)list {
  return (list) ? [AndroidVideoOptions fromList:list] : nil;
}
- (NSArray *)toList {
  return @[
    (self.bitrate ?: [NSNull null]),
    @(self.fallbackStrategy),
  ];
}
@end

@implementation CupertinoVideoOptions
+ (instancetype)makeWithFileType:(CupertinoFileType)fileType
    codec:(CupertinoCodecType)codec
    fps:(nullable NSNumber *)fps {
  CupertinoVideoOptions* pigeonResult = [[CupertinoVideoOptions alloc] init];
  pigeonResult.fileType = fileType;
  pigeonResult.codec = codec;
  pigeonResult.fps = fps;
  return pigeonResult;
}
+ (CupertinoVideoOptions *)fromList:(NSArray *)list {
  CupertinoVideoOptions *pigeonResult = [[CupertinoVideoOptions alloc] init];
  pigeonResult.fileType = [GetNullableObjectAtIndex(list, 0) integerValue];
  pigeonResult.codec = [GetNullableObjectAtIndex(list, 1) integerValue];
  pigeonResult.fps = GetNullableObjectAtIndex(list, 2);
  return pigeonResult;
}
+ (nullable CupertinoVideoOptions *)nullableFromList:(NSArray *)list {
  return (list) ? [CupertinoVideoOptions fromList:list] : nil;
}
- (NSArray *)toList {
  return @[
    @(self.fileType),
    @(self.codec),
    (self.fps ?: [NSNull null]),
  ];
}
@end

@implementation PigeonSensorTypeDevice
+ (instancetype)makeWithSensorType:(PigeonSensorType)sensorType
    name:(NSString *)name
    iso:(NSNumber *)iso
    flashAvailable:(NSNumber *)flashAvailable
    uid:(NSString *)uid {
  PigeonSensorTypeDevice* pigeonResult = [[PigeonSensorTypeDevice alloc] init];
  pigeonResult.sensorType = sensorType;
  pigeonResult.name = name;
  pigeonResult.iso = iso;
  pigeonResult.flashAvailable = flashAvailable;
  pigeonResult.uid = uid;
  return pigeonResult;
}
+ (PigeonSensorTypeDevice *)fromList:(NSArray *)list {
  PigeonSensorTypeDevice *pigeonResult = [[PigeonSensorTypeDevice alloc] init];
  pigeonResult.sensorType = [GetNullableObjectAtIndex(list, 0) integerValue];
  pigeonResult.name = GetNullableObjectAtIndex(list, 1);
  NSAssert(pigeonResult.name != nil, @"");
  pigeonResult.iso = GetNullableObjectAtIndex(list, 2);
  NSAssert(pigeonResult.iso != nil, @"");
  pigeonResult.flashAvailable = GetNullableObjectAtIndex(list, 3);
  NSAssert(pigeonResult.flashAvailable != nil, @"");
  pigeonResult.uid = GetNullableObjectAtIndex(list, 4);
  NSAssert(pigeonResult.uid != nil, @"");
  return pigeonResult;
}
+ (nullable PigeonSensorTypeDevice *)nullableFromList:(NSArray *)list {
  return (list) ? [PigeonSensorTypeDevice fromList:list] : nil;
}
- (NSArray *)toList {
  return @[
    @(self.sensorType),
    (self.name ?: [NSNull null]),
    (self.iso ?: [NSNull null]),
    (self.flashAvailable ?: [NSNull null]),
    (self.uid ?: [NSNull null]),
  ];
}
@end

@implementation AndroidFocusSettings
+ (instancetype)makeWithAutoCancelDurationInMillis:(NSNumber *)autoCancelDurationInMillis {
  AndroidFocusSettings* pigeonResult = [[AndroidFocusSettings alloc] init];
  pigeonResult.autoCancelDurationInMillis = autoCancelDurationInMillis;
  return pigeonResult;
}
+ (AndroidFocusSettings *)fromList:(NSArray *)list {
  AndroidFocusSettings *pigeonResult = [[AndroidFocusSettings alloc] init];
  pigeonResult.autoCancelDurationInMillis = GetNullableObjectAtIndex(list, 0);
  NSAssert(pigeonResult.autoCancelDurationInMillis != nil, @"");
  return pigeonResult;
}
+ (nullable AndroidFocusSettings *)nullableFromList:(NSArray *)list {
  return (list) ? [AndroidFocusSettings fromList:list] : nil;
}
- (NSArray *)toList {
  return @[
    (self.autoCancelDurationInMillis ?: [NSNull null]),
  ];
}
@end

@implementation PlaneWrapper
+ (instancetype)makeWithBytes:(FlutterStandardTypedData *)bytes
    bytesPerRow:(NSNumber *)bytesPerRow
    bytesPerPixel:(nullable NSNumber *)bytesPerPixel
    width:(nullable NSNumber *)width
    height:(nullable NSNumber *)height {
  PlaneWrapper* pigeonResult = [[PlaneWrapper alloc] init];
  pigeonResult.bytes = bytes;
  pigeonResult.bytesPerRow = bytesPerRow;
  pigeonResult.bytesPerPixel = bytesPerPixel;
  pigeonResult.width = width;
  pigeonResult.height = height;
  return pigeonResult;
}
+ (PlaneWrapper *)fromList:(NSArray *)list {
  PlaneWrapper *pigeonResult = [[PlaneWrapper alloc] init];
  pigeonResult.bytes = GetNullableObjectAtIndex(list, 0);
  NSAssert(pigeonResult.bytes != nil, @"");
  pigeonResult.bytesPerRow = GetNullableObjectAtIndex(list, 1);
  NSAssert(pigeonResult.bytesPerRow != nil, @"");
  pigeonResult.bytesPerPixel = GetNullableObjectAtIndex(list, 2);
  pigeonResult.width = GetNullableObjectAtIndex(list, 3);
  pigeonResult.height = GetNullableObjectAtIndex(list, 4);
  return pigeonResult;
}
+ (nullable PlaneWrapper *)nullableFromList:(NSArray *)list {
  return (list) ? [PlaneWrapper fromList:list] : nil;
}
- (NSArray *)toList {
  return @[
    (self.bytes ?: [NSNull null]),
    (self.bytesPerRow ?: [NSNull null]),
    (self.bytesPerPixel ?: [NSNull null]),
    (self.width ?: [NSNull null]),
    (self.height ?: [NSNull null]),
  ];
}
@end

@implementation CropRectWrapper
+ (instancetype)makeWithLeft:(NSNumber *)left
    top:(NSNumber *)top
    width:(NSNumber *)width
    height:(NSNumber *)height {
  CropRectWrapper* pigeonResult = [[CropRectWrapper alloc] init];
  pigeonResult.left = left;
  pigeonResult.top = top;
  pigeonResult.width = width;
  pigeonResult.height = height;
  return pigeonResult;
}
+ (CropRectWrapper *)fromList:(NSArray *)list {
  CropRectWrapper *pigeonResult = [[CropRectWrapper alloc] init];
  pigeonResult.left = GetNullableObjectAtIndex(list, 0);
  NSAssert(pigeonResult.left != nil, @"");
  pigeonResult.top = GetNullableObjectAtIndex(list, 1);
  NSAssert(pigeonResult.top != nil, @"");
  pigeonResult.width = GetNullableObjectAtIndex(list, 2);
  NSAssert(pigeonResult.width != nil, @"");
  pigeonResult.height = GetNullableObjectAtIndex(list, 3);
  NSAssert(pigeonResult.height != nil, @"");
  return pigeonResult;
}
+ (nullable CropRectWrapper *)nullableFromList:(NSArray *)list {
  return (list) ? [CropRectWrapper fromList:list] : nil;
}
- (NSArray *)toList {
  return @[
    (self.left ?: [NSNull null]),
    (self.top ?: [NSNull null]),
    (self.width ?: [NSNull null]),
    (self.height ?: [NSNull null]),
  ];
}
@end

@implementation AnalysisImageWrapper
+ (instancetype)makeWithFormat:(AnalysisImageFormat)format
    bytes:(nullable FlutterStandardTypedData *)bytes
    width:(NSNumber *)width
    height:(NSNumber *)height
    planes:(nullable NSArray<PlaneWrapper *> *)planes
    cropRect:(nullable CropRectWrapper *)cropRect
    rotation:(AnalysisRotation)rotation {
  AnalysisImageWrapper* pigeonResult = [[AnalysisImageWrapper alloc] init];
  pigeonResult.format = format;
  pigeonResult.bytes = bytes;
  pigeonResult.width = width;
  pigeonResult.height = height;
  pigeonResult.planes = planes;
  pigeonResult.cropRect = cropRect;
  pigeonResult.rotation = rotation;
  return pigeonResult;
}
+ (AnalysisImageWrapper *)fromList:(NSArray *)list {
  AnalysisImageWrapper *pigeonResult = [[AnalysisImageWrapper alloc] init];
  pigeonResult.format = [GetNullableObjectAtIndex(list, 0) integerValue];
  pigeonResult.bytes = GetNullableObjectAtIndex(list, 1);
  pigeonResult.width = GetNullableObjectAtIndex(list, 2);
  NSAssert(pigeonResult.width != nil, @"");
  pigeonResult.height = GetNullableObjectAtIndex(list, 3);
  NSAssert(pigeonResult.height != nil, @"");
  pigeonResult.planes = GetNullableObjectAtIndex(list, 4);
  pigeonResult.cropRect = [CropRectWrapper nullableFromList:(GetNullableObjectAtIndex(list, 5))];
  pigeonResult.rotation = [GetNullableObjectAtIndex(list, 6) integerValue];
  return pigeonResult;
}
+ (nullable AnalysisImageWrapper *)nullableFromList:(NSArray *)list {
  return (list) ? [AnalysisImageWrapper fromList:list] : nil;
}
- (NSArray *)toList {
  return @[
    @(self.format),
    (self.bytes ?: [NSNull null]),
    (self.width ?: [NSNull null]),
    (self.height ?: [NSNull null]),
    (self.planes ?: [NSNull null]),
    (self.cropRect ? [self.cropRect toList] : [NSNull null]),
    @(self.rotation),
  ];
}
@end

@interface AnalysisImageUtilsCodecReader : FlutterStandardReader
@end
@implementation AnalysisImageUtilsCodecReader
- (nullable id)readValueOfType:(UInt8)type {
  switch (type) {
    case 128: 
      return [AnalysisImageWrapper fromList:[self readValue]];
    case 129: 
      return [CropRectWrapper fromList:[self readValue]];
    case 130: 
      return [PlaneWrapper fromList:[self readValue]];
    default:
      return [super readValueOfType:type];
  }
}
@end

@interface AnalysisImageUtilsCodecWriter : FlutterStandardWriter
@end
@implementation AnalysisImageUtilsCodecWriter
- (void)writeValue:(id)value {
  if ([value isKindOfClass:[AnalysisImageWrapper class]]) {
    [self writeByte:128];
    [self writeValue:[value toList]];
  } else if ([value isKindOfClass:[CropRectWrapper class]]) {
    [self writeByte:129];
    [self writeValue:[value toList]];
  } else if ([value isKindOfClass:[PlaneWrapper class]]) {
    [self writeByte:130];
    [self writeValue:[value toList]];
  } else {
    [super writeValue:value];
  }
}
@end

@interface AnalysisImageUtilsCodecReaderWriter : FlutterStandardReaderWriter
@end
@implementation AnalysisImageUtilsCodecReaderWriter
- (FlutterStandardWriter *)writerWithData:(NSMutableData *)data {
  return [[AnalysisImageUtilsCodecWriter alloc] initWithData:data];
}
- (FlutterStandardReader *)readerWithData:(NSData *)data {
  return [[AnalysisImageUtilsCodecReader alloc] initWithData:data];
}
@end

NSObject<FlutterMessageCodec> *AnalysisImageUtilsGetCodec(void) {
  static FlutterStandardMessageCodec *sSharedObject = nil;
  static dispatch_once_t sPred = 0;
  dispatch_once(&sPred, ^{
    AnalysisImageUtilsCodecReaderWriter *readerWriter = [[AnalysisImageUtilsCodecReaderWriter alloc] init];
    sSharedObject = [FlutterStandardMessageCodec codecWithReaderWriter:readerWriter];
  });
  return sSharedObject;
}

void AnalysisImageUtilsSetup(id<FlutterBinaryMessenger> binaryMessenger, NSObject<AnalysisImageUtils> *api) {
  {
    FlutterBasicMessageChannel *channel =
      [[FlutterBasicMessageChannel alloc]
        initWithName:@"dev.flutter.pigeon.AnalysisImageUtils.nv21toJpeg"
        binaryMessenger:binaryMessenger
        codec:AnalysisImageUtilsGetCodec()];
    if (api) {
      NSCAssert([api respondsToSelector:@selector(nv21toJpegNv21Image:jpegQuality:completion:)], @"AnalysisImageUtils api (%@) doesn't respond to @selector(nv21toJpegNv21Image:jpegQuality:completion:)", api);
      [channel setMessageHandler:^(id _Nullable message, FlutterReply callback) {
        NSArray *args = message;
        AnalysisImageWrapper *arg_nv21Image = GetNullableObjectAtIndex(args, 0);
        NSNumber *arg_jpegQuality = GetNullableObjectAtIndex(args, 1);
        [api nv21toJpegNv21Image:arg_nv21Image jpegQuality:arg_jpegQuality completion:^(AnalysisImageWrapper *_Nullable output, FlutterError *_Nullable error) {
          callback(wrapResult(output, error));
        }];
      }];
    } else {
      [channel setMessageHandler:nil];
    }
  }
  {
    FlutterBasicMessageChannel *channel =
      [[FlutterBasicMessageChannel alloc]
        initWithName:@"dev.flutter.pigeon.AnalysisImageUtils.yuv420toJpeg"
        binaryMessenger:binaryMessenger
        codec:AnalysisImageUtilsGetCodec()];
    if (api) {
      NSCAssert([api respondsToSelector:@selector(yuv420toJpegYuvImage:jpegQuality:completion:)], @"AnalysisImageUtils api (%@) doesn't respond to @selector(yuv420toJpegYuvImage:jpegQuality:completion:)", api);
      [channel setMessageHandler:^(id _Nullable message, FlutterReply callback) {
        NSArray *args = message;
        AnalysisImageWrapper *arg_yuvImage = GetNullableObjectAtIndex(args, 0);
        NSNumber *arg_jpegQuality = GetNullableObjectAtIndex(args, 1);
        [api yuv420toJpegYuvImage:arg_yuvImage jpegQuality:arg_jpegQuality completion:^(AnalysisImageWrapper *_Nullable output, FlutterError *_Nullable error) {
          callback(wrapResult(output, error));
        }];
      }];
    } else {
      [channel setMessageHandler:nil];
    }
  }
  {
    FlutterBasicMessageChannel *channel =
      [[FlutterBasicMessageChannel alloc]
        initWithName:@"dev.flutter.pigeon.AnalysisImageUtils.yuv420toNv21"
        binaryMessenger:binaryMessenger
        codec:AnalysisImageUtilsGetCodec()];
    if (api) {
      NSCAssert([api respondsToSelector:@selector(yuv420toNv21YuvImage:completion:)], @"AnalysisImageUtils api (%@) doesn't respond to @selector(yuv420toNv21YuvImage:completion:)", api);
      [channel setMessageHandler:^(id _Nullable message, FlutterReply callback) {
        NSArray *args = message;
        AnalysisImageWrapper *arg_yuvImage = GetNullableObjectAtIndex(args, 0);
        [api yuv420toNv21YuvImage:arg_yuvImage completion:^(AnalysisImageWrapper *_Nullable output, FlutterError *_Nullable error) {
          callback(wrapResult(output, error));
        }];
      }];
    } else {
      [channel setMessageHandler:nil];
    }
  }
  {
    FlutterBasicMessageChannel *channel =
      [[FlutterBasicMessageChannel alloc]
        initWithName:@"dev.flutter.pigeon.AnalysisImageUtils.bgra8888toJpeg"
        binaryMessenger:binaryMessenger
        codec:AnalysisImageUtilsGetCodec()];
    if (api) {
      NSCAssert([api respondsToSelector:@selector(bgra8888toJpegBgra8888image:jpegQuality:completion:)], @"AnalysisImageUtils api (%@) doesn't respond to @selector(bgra8888toJpegBgra8888image:jpegQuality:completion:)", api);
      [channel setMessageHandler:^(id _Nullable message, FlutterReply callback) {
        NSArray *args = message;
        AnalysisImageWrapper *arg_bgra8888image = GetNullableObjectAtIndex(args, 0);
        NSNumber *arg_jpegQuality = GetNullableObjectAtIndex(args, 1);
        [api bgra8888toJpegBgra8888image:arg_bgra8888image jpegQuality:arg_jpegQuality completion:^(AnalysisImageWrapper *_Nullable output, FlutterError *_Nullable error) {
          callback(wrapResult(output, error));
        }];
      }];
    } else {
      [channel setMessageHandler:nil];
    }
  }
}
@interface CameraInterfaceCodecReader : FlutterStandardReader
@end
@implementation CameraInterfaceCodecReader
- (nullable id)readValueOfType:(UInt8)type {
  switch (type) {
    case 128: 
      return [AndroidFocusSettings fromList:[self readValue]];
    case 129: 
      return [AndroidVideoOptions fromList:[self readValue]];
    case 130: 
      return [CupertinoVideoOptions fromList:[self readValue]];
    case 131: 
      return [ExifPreferences fromList:[self readValue]];
    case 132: 
      return [PigeonSensor fromList:[self readValue]];
    case 133: 
      return [PigeonSensorTypeDevice fromList:[self readValue]];
    case 134: 
      return [PreviewSize fromList:[self readValue]];
    case 135: 
      return [PreviewSize fromList:[self readValue]];
    case 136: 
      return [VideoOptions fromList:[self readValue]];
    default:
      return [super readValueOfType:type];
  }
}
@end

@interface CameraInterfaceCodecWriter : FlutterStandardWriter
@end
@implementation CameraInterfaceCodecWriter
- (void)writeValue:(id)value {
  if ([value isKindOfClass:[AndroidFocusSettings class]]) {
    [self writeByte:128];
    [self writeValue:[value toList]];
  } else if ([value isKindOfClass:[AndroidVideoOptions class]]) {
    [self writeByte:129];
    [self writeValue:[value toList]];
  } else if ([value isKindOfClass:[CupertinoVideoOptions class]]) {
    [self writeByte:130];
    [self writeValue:[value toList]];
  } else if ([value isKindOfClass:[ExifPreferences class]]) {
    [self writeByte:131];
    [self writeValue:[value toList]];
  } else if ([value isKindOfClass:[PigeonSensor class]]) {
    [self writeByte:132];
    [self writeValue:[value toList]];
  } else if ([value isKindOfClass:[PigeonSensorTypeDevice class]]) {
    [self writeByte:133];
    [self writeValue:[value toList]];
  } else if ([value isKindOfClass:[PreviewSize class]]) {
    [self writeByte:134];
    [self writeValue:[value toList]];
  } else if ([value isKindOfClass:[PreviewSize class]]) {
    [self writeByte:135];
    [self writeValue:[value toList]];
  } else if ([value isKindOfClass:[VideoOptions class]]) {
    [self writeByte:136];
    [self writeValue:[value toList]];
  } else {
    [super writeValue:value];
  }
}
@end

@interface CameraInterfaceCodecReaderWriter : FlutterStandardReaderWriter
@end
@implementation CameraInterfaceCodecReaderWriter
- (FlutterStandardWriter *)writerWithData:(NSMutableData *)data {
  return [[CameraInterfaceCodecWriter alloc] initWithData:data];
}
- (FlutterStandardReader *)readerWithData:(NSData *)data {
  return [[CameraInterfaceCodecReader alloc] initWithData:data];
}
@end

NSObject<FlutterMessageCodec> *CameraInterfaceGetCodec(void) {
  static FlutterStandardMessageCodec *sSharedObject = nil;
  static dispatch_once_t sPred = 0;
  dispatch_once(&sPred, ^{
    CameraInterfaceCodecReaderWriter *readerWriter = [[CameraInterfaceCodecReaderWriter alloc] init];
    sSharedObject = [FlutterStandardMessageCodec codecWithReaderWriter:readerWriter];
  });
  return sSharedObject;
}

void CameraInterfaceSetup(id<FlutterBinaryMessenger> binaryMessenger, NSObject<CameraInterface> *api) {
  {
    FlutterBasicMessageChannel *channel =
      [[FlutterBasicMessageChannel alloc]
        initWithName:@"dev.flutter.pigeon.CameraInterface.setupCamera"
        binaryMessenger:binaryMessenger
        codec:CameraInterfaceGetCodec()];
    if (api) {
      NSCAssert([api respondsToSelector:@selector(setupCameraSensors:aspectRatio:zoom:mirrorFrontCamera:enablePhysicalButton:flashMode:captureMode:enableImageStream:exifPreferences:videoOptions:completion:)], @"CameraInterface api (%@) doesn't respond to @selector(setupCameraSensors:aspectRatio:zoom:mirrorFrontCamera:enablePhysicalButton:flashMode:captureMode:enableImageStream:exifPreferences:videoOptions:completion:)", api);
      [channel setMessageHandler:^(id _Nullable message, FlutterReply callback) {
        NSArray *args = message;
        NSArray<PigeonSensor *> *arg_sensors = GetNullableObjectAtIndex(args, 0);
        NSString *arg_aspectRatio = GetNullableObjectAtIndex(args, 1);
        NSNumber *arg_zoom = GetNullableObjectAtIndex(args, 2);
        NSNumber *arg_mirrorFrontCamera = GetNullableObjectAtIndex(args, 3);
        NSNumber *arg_enablePhysicalButton = GetNullableObjectAtIndex(args, 4);
        NSString *arg_flashMode = GetNullableObjectAtIndex(args, 5);
        NSString *arg_captureMode = GetNullableObjectAtIndex(args, 6);
        NSNumber *arg_enableImageStream = GetNullableObjectAtIndex(args, 7);
        ExifPreferences *arg_exifPreferences = GetNullableObjectAtIndex(args, 8);
        VideoOptions *arg_videoOptions = GetNullableObjectAtIndex(args, 9);
        [api setupCameraSensors:arg_sensors aspectRatio:arg_aspectRatio zoom:arg_zoom mirrorFrontCamera:arg_mirrorFrontCamera enablePhysicalButton:arg_enablePhysicalButton flashMode:arg_flashMode captureMode:arg_captureMode enableImageStream:arg_enableImageStream exifPreferences:arg_exifPreferences videoOptions:arg_videoOptions completion:^(NSNumber *_Nullable output, FlutterError *_Nullable error) {
          callback(wrapResult(output, error));
        }];
      }];
    } else {
      [channel setMessageHandler:nil];
    }
  }
  {
    FlutterBasicMessageChannel *channel =
      [[FlutterBasicMessageChannel alloc]
        initWithName:@"dev.flutter.pigeon.CameraInterface.checkPermissions"
        binaryMessenger:binaryMessenger
        codec:CameraInterfaceGetCodec()];
    if (api) {
      NSCAssert([api respondsToSelector:@selector(checkPermissionsPermissions:error:)], @"CameraInterface api (%@) doesn't respond to @selector(checkPermissionsPermissions:error:)", api);
      [channel setMessageHandler:^(id _Nullable message, FlutterReply callback) {
        NSArray *args = message;
        NSArray<NSString *> *arg_permissions = GetNullableObjectAtIndex(args, 0);
        FlutterError *error;
        NSArray<NSString *> *output = [api checkPermissionsPermissions:arg_permissions error:&error];
        callback(wrapResult(output, error));
      }];
    } else {
      [channel setMessageHandler:nil];
    }
  }
  /// Returns given [CamerAwesomePermission] list (as String). Location permission might be
  /// refused but the app should still be able to run.
  {
    FlutterBasicMessageChannel *channel =
      [[FlutterBasicMessageChannel alloc]
        initWithName:@"dev.flutter.pigeon.CameraInterface.requestPermissions"
        binaryMessenger:binaryMessenger
        codec:CameraInterfaceGetCodec()];
    if (api) {
      NSCAssert([api respondsToSelector:@selector(requestPermissionsSaveGpsLocation:completion:)], @"CameraInterface api (%@) doesn't respond to @selector(requestPermissionsSaveGpsLocation:completion:)", api);
      [channel setMessageHandler:^(id _Nullable message, FlutterReply callback) {
        NSArray *args = message;
        NSNumber *arg_saveGpsLocation = GetNullableObjectAtIndex(args, 0);
        [api requestPermissionsSaveGpsLocation:arg_saveGpsLocation completion:^(NSArray<NSString *> *_Nullable output, FlutterError *_Nullable error) {
          callback(wrapResult(output, error));
        }];
      }];
    } else {
      [channel setMessageHandler:nil];
    }
  }
  {
    FlutterBasicMessageChannel *channel =
      [[FlutterBasicMessageChannel alloc]
        initWithName:@"dev.flutter.pigeon.CameraInterface.getPreviewTextureId"
        binaryMessenger:binaryMessenger
        codec:CameraInterfaceGetCodec()];
    if (api) {
      NSCAssert([api respondsToSelector:@selector(getPreviewTextureIdCameraPosition:error:)], @"CameraInterface api (%@) doesn't respond to @selector(getPreviewTextureIdCameraPosition:error:)", api);
      [channel setMessageHandler:^(id _Nullable message, FlutterReply callback) {
        NSArray *args = message;
        NSNumber *arg_cameraPosition = GetNullableObjectAtIndex(args, 0);
        FlutterError *error;
        NSNumber *output = [api getPreviewTextureIdCameraPosition:arg_cameraPosition error:&error];
        callback(wrapResult(output, error));
      }];
    } else {
      [channel setMessageHandler:nil];
    }
  }
  {
    FlutterBasicMessageChannel *channel =
      [[FlutterBasicMessageChannel alloc]
        initWithName:@"dev.flutter.pigeon.CameraInterface.takePhoto"
        binaryMessenger:binaryMessenger
        codec:CameraInterfaceGetCodec()];
    if (api) {
      NSCAssert([api respondsToSelector:@selector(takePhotoSensors:paths:completion:)], @"CameraInterface api (%@) doesn't respond to @selector(takePhotoSensors:paths:completion:)", api);
      [channel setMessageHandler:^(id _Nullable message, FlutterReply callback) {
        NSArray *args = message;
        NSArray<PigeonSensor *> *arg_sensors = GetNullableObjectAtIndex(args, 0);
        NSArray<NSString *> *arg_paths = GetNullableObjectAtIndex(args, 1);
        [api takePhotoSensors:arg_sensors paths:arg_paths completion:^(NSNumber *_Nullable output, FlutterError *_Nullable error) {
          callback(wrapResult(output, error));
        }];
      }];
    } else {
      [channel setMessageHandler:nil];
    }
  }
  {
    FlutterBasicMessageChannel *channel =
      [[FlutterBasicMessageChannel alloc]
        initWithName:@"dev.flutter.pigeon.CameraInterface.recordVideo"
        binaryMessenger:binaryMessenger
        codec:CameraInterfaceGetCodec()];
    if (api) {
      NSCAssert([api respondsToSelector:@selector(recordVideoSensors:paths:completion:)], @"CameraInterface api (%@) doesn't respond to @selector(recordVideoSensors:paths:completion:)", api);
      [channel setMessageHandler:^(id _Nullable message, FlutterReply callback) {
        NSArray *args = message;
        NSArray<PigeonSensor *> *arg_sensors = GetNullableObjectAtIndex(args, 0);
        NSArray<NSString *> *arg_paths = GetNullableObjectAtIndex(args, 1);
        [api recordVideoSensors:arg_sensors paths:arg_paths completion:^(FlutterError *_Nullable error) {
          callback(wrapResult(nil, error));
        }];
      }];
    } else {
      [channel setMessageHandler:nil];
    }
  }
  {
    FlutterBasicMessageChannel *channel =
      [[FlutterBasicMessageChannel alloc]
        initWithName:@"dev.flutter.pigeon.CameraInterface.pauseVideoRecording"
        binaryMessenger:binaryMessenger
        codec:CameraInterfaceGetCodec()];
    if (api) {
      NSCAssert([api respondsToSelector:@selector(pauseVideoRecordingWithError:)], @"CameraInterface api (%@) doesn't respond to @selector(pauseVideoRecordingWithError:)", api);
      [channel setMessageHandler:^(id _Nullable message, FlutterReply callback) {
        FlutterError *error;
        [api pauseVideoRecordingWithError:&error];
        callback(wrapResult(nil, error));
      }];
    } else {
      [channel setMessageHandler:nil];
    }
  }
  {
    FlutterBasicMessageChannel *channel =
      [[FlutterBasicMessageChannel alloc]
        initWithName:@"dev.flutter.pigeon.CameraInterface.resumeVideoRecording"
        binaryMessenger:binaryMessenger
        codec:CameraInterfaceGetCodec()];
    if (api) {
      NSCAssert([api respondsToSelector:@selector(resumeVideoRecordingWithError:)], @"CameraInterface api (%@) doesn't respond to @selector(resumeVideoRecordingWithError:)", api);
      [channel setMessageHandler:^(id _Nullable message, FlutterReply callback) {
        FlutterError *error;
        [api resumeVideoRecordingWithError:&error];
        callback(wrapResult(nil, error));
      }];
    } else {
      [channel setMessageHandler:nil];
    }
  }
  {
    FlutterBasicMessageChannel *channel =
      [[FlutterBasicMessageChannel alloc]
        initWithName:@"dev.flutter.pigeon.CameraInterface.receivedImageFromStream"
        binaryMessenger:binaryMessenger
        codec:CameraInterfaceGetCodec()];
    if (api) {
      NSCAssert([api respondsToSelector:@selector(receivedImageFromStreamWithError:)], @"CameraInterface api (%@) doesn't respond to @selector(receivedImageFromStreamWithError:)", api);
      [channel setMessageHandler:^(id _Nullable message, FlutterReply callback) {
        FlutterError *error;
        [api receivedImageFromStreamWithError:&error];
        callback(wrapResult(nil, error));
      }];
    } else {
      [channel setMessageHandler:nil];
    }
  }
  {
    FlutterBasicMessageChannel *channel =
      [[FlutterBasicMessageChannel alloc]
        initWithName:@"dev.flutter.pigeon.CameraInterface.stopRecordingVideo"
        binaryMessenger:binaryMessenger
        codec:CameraInterfaceGetCodec()];
    if (api) {
      NSCAssert([api respondsToSelector:@selector(stopRecordingVideoWithCompletion:)], @"CameraInterface api (%@) doesn't respond to @selector(stopRecordingVideoWithCompletion:)", api);
      [channel setMessageHandler:^(id _Nullable message, FlutterReply callback) {
        [api stopRecordingVideoWithCompletion:^(NSNumber *_Nullable output, FlutterError *_Nullable error) {
          callback(wrapResult(output, error));
        }];
      }];
    } else {
      [channel setMessageHandler:nil];
    }
  }
  {
    FlutterBasicMessageChannel *channel =
      [[FlutterBasicMessageChannel alloc]
        initWithName:@"dev.flutter.pigeon.CameraInterface.getFrontSensors"
        binaryMessenger:binaryMessenger
        codec:CameraInterfaceGetCodec()];
    if (api) {
      NSCAssert([api respondsToSelector:@selector(getFrontSensorsWithError:)], @"CameraInterface api (%@) doesn't respond to @selector(getFrontSensorsWithError:)", api);
      [channel setMessageHandler:^(id _Nullable message, FlutterReply callback) {
        FlutterError *error;
        NSArray<PigeonSensorTypeDevice *> *output = [api getFrontSensorsWithError:&error];
        callback(wrapResult(output, error));
      }];
    } else {
      [channel setMessageHandler:nil];
    }
  }
  {
    FlutterBasicMessageChannel *channel =
      [[FlutterBasicMessageChannel alloc]
        initWithName:@"dev.flutter.pigeon.CameraInterface.getBackSensors"
        binaryMessenger:binaryMessenger
        codec:CameraInterfaceGetCodec()];
    if (api) {
      NSCAssert([api respondsToSelector:@selector(getBackSensorsWithError:)], @"CameraInterface api (%@) doesn't respond to @selector(getBackSensorsWithError:)", api);
      [channel setMessageHandler:^(id _Nullable message, FlutterReply callback) {
        FlutterError *error;
        NSArray<PigeonSensorTypeDevice *> *output = [api getBackSensorsWithError:&error];
        callback(wrapResult(output, error));
      }];
    } else {
      [channel setMessageHandler:nil];
    }
  }
  {
    FlutterBasicMessageChannel *channel =
      [[FlutterBasicMessageChannel alloc]
        initWithName:@"dev.flutter.pigeon.CameraInterface.start"
        binaryMessenger:binaryMessenger
        codec:CameraInterfaceGetCodec()];
    if (api) {
      NSCAssert([api respondsToSelector:@selector(startWithError:)], @"CameraInterface api (%@) doesn't respond to @selector(startWithError:)", api);
      [channel setMessageHandler:^(id _Nullable message, FlutterReply callback) {
        FlutterError *error;
        NSNumber *output = [api startWithError:&error];
        callback(wrapResult(output, error));
      }];
    } else {
      [channel setMessageHandler:nil];
    }
  }
  {
    FlutterBasicMessageChannel *channel =
      [[FlutterBasicMessageChannel alloc]
        initWithName:@"dev.flutter.pigeon.CameraInterface.stop"
        binaryMessenger:binaryMessenger
        codec:CameraInterfaceGetCodec()];
    if (api) {
      NSCAssert([api respondsToSelector:@selector(stopWithError:)], @"CameraInterface api (%@) doesn't respond to @selector(stopWithError:)", api);
      [channel setMessageHandler:^(id _Nullable message, FlutterReply callback) {
        FlutterError *error;
        NSNumber *output = [api stopWithError:&error];
        callback(wrapResult(output, error));
      }];
    } else {
      [channel setMessageHandler:nil];
    }
  }
  {
    FlutterBasicMessageChannel *channel =
      [[FlutterBasicMessageChannel alloc]
        initWithName:@"dev.flutter.pigeon.CameraInterface.setFlashMode"
        binaryMessenger:binaryMessenger
        codec:CameraInterfaceGetCodec()];
    if (api) {
      NSCAssert([api respondsToSelector:@selector(setFlashModeMode:error:)], @"CameraInterface api (%@) doesn't respond to @selector(setFlashModeMode:error:)", api);
      [channel setMessageHandler:^(id _Nullable message, FlutterReply callback) {
        NSArray *args = message;
        NSString *arg_mode = GetNullableObjectAtIndex(args, 0);
        FlutterError *error;
        [api setFlashModeMode:arg_mode error:&error];
        callback(wrapResult(nil, error));
      }];
    } else {
      [channel setMessageHandler:nil];
    }
  }
  {
    FlutterBasicMessageChannel *channel =
      [[FlutterBasicMessageChannel alloc]
        initWithName:@"dev.flutter.pigeon.CameraInterface.handleAutoFocus"
        binaryMessenger:binaryMessenger
        codec:CameraInterfaceGetCodec()];
    if (api) {
      NSCAssert([api respondsToSelector:@selector(handleAutoFocusWithError:)], @"CameraInterface api (%@) doesn't respond to @selector(handleAutoFocusWithError:)", api);
      [channel setMessageHandler:^(id _Nullable message, FlutterReply callback) {
        FlutterError *error;
        [api handleAutoFocusWithError:&error];
        callback(wrapResult(nil, error));
      }];
    } else {
      [channel setMessageHandler:nil];
    }
  }
  /// Starts auto focus on a point at ([x], [y]).
  ///
  /// On Android, you can control after how much time you want to switch back
  /// to passive focus mode with [androidFocusSettings].
  {
    FlutterBasicMessageChannel *channel =
      [[FlutterBasicMessageChannel alloc]
        initWithName:@"dev.flutter.pigeon.CameraInterface.focusOnPoint"
        binaryMessenger:binaryMessenger
        codec:CameraInterfaceGetCodec()];
    if (api) {
      NSCAssert([api respondsToSelector:@selector(focusOnPointPreviewSize:x:y:androidFocusSettings:error:)], @"CameraInterface api (%@) doesn't respond to @selector(focusOnPointPreviewSize:x:y:androidFocusSettings:error:)", api);
      [channel setMessageHandler:^(id _Nullable message, FlutterReply callback) {
        NSArray *args = message;
        PreviewSize *arg_previewSize = GetNullableObjectAtIndex(args, 0);
        NSNumber *arg_x = GetNullableObjectAtIndex(args, 1);
        NSNumber *arg_y = GetNullableObjectAtIndex(args, 2);
        AndroidFocusSettings *arg_androidFocusSettings = GetNullableObjectAtIndex(args, 3);
        FlutterError *error;
        [api focusOnPointPreviewSize:arg_previewSize x:arg_x y:arg_y androidFocusSettings:arg_androidFocusSettings error:&error];
        callback(wrapResult(nil, error));
      }];
    } else {
      [channel setMessageHandler:nil];
    }
  }
  {
    FlutterBasicMessageChannel *channel =
      [[FlutterBasicMessageChannel alloc]
        initWithName:@"dev.flutter.pigeon.CameraInterface.setZoom"
        binaryMessenger:binaryMessenger
        codec:CameraInterfaceGetCodec()];
    if (api) {
      NSCAssert([api respondsToSelector:@selector(setZoomZoom:error:)], @"CameraInterface api (%@) doesn't respond to @selector(setZoomZoom:error:)", api);
      [channel setMessageHandler:^(id _Nullable message, FlutterReply callback) {
        NSArray *args = message;
        NSNumber *arg_zoom = GetNullableObjectAtIndex(args, 0);
        FlutterError *error;
        [api setZoomZoom:arg_zoom error:&error];
        callback(wrapResult(nil, error));
      }];
    } else {
      [channel setMessageHandler:nil];
    }
  }
  {
    FlutterBasicMessageChannel *channel =
      [[FlutterBasicMessageChannel alloc]
        initWithName:@"dev.flutter.pigeon.CameraInterface.setMirrorFrontCamera"
        binaryMessenger:binaryMessenger
        codec:CameraInterfaceGetCodec()];
    if (api) {
      NSCAssert([api respondsToSelector:@selector(setMirrorFrontCameraMirror:error:)], @"CameraInterface api (%@) doesn't respond to @selector(setMirrorFrontCameraMirror:error:)", api);
      [channel setMessageHandler:^(id _Nullable message, FlutterReply callback) {
        NSArray *args = message;
        NSNumber *arg_mirror = GetNullableObjectAtIndex(args, 0);
        FlutterError *error;
        [api setMirrorFrontCameraMirror:arg_mirror error:&error];
        callback(wrapResult(nil, error));
      }];
    } else {
      [channel setMessageHandler:nil];
    }
  }
  {
    FlutterBasicMessageChannel *channel =
      [[FlutterBasicMessageChannel alloc]
        initWithName:@"dev.flutter.pigeon.CameraInterface.setSensor"
        binaryMessenger:binaryMessenger
        codec:CameraInterfaceGetCodec()];
    if (api) {
      NSCAssert([api respondsToSelector:@selector(setSensorSensors:error:)], @"CameraInterface api (%@) doesn't respond to @selector(setSensorSensors:error:)", api);
      [channel setMessageHandler:^(id _Nullable message, FlutterReply callback) {
        NSArray *args = message;
        NSArray<PigeonSensor *> *arg_sensors = GetNullableObjectAtIndex(args, 0);
        FlutterError *error;
        [api setSensorSensors:arg_sensors error:&error];
        callback(wrapResult(nil, error));
      }];
    } else {
      [channel setMessageHandler:nil];
    }
  }
  {
    FlutterBasicMessageChannel *channel =
      [[FlutterBasicMessageChannel alloc]
        initWithName:@"dev.flutter.pigeon.CameraInterface.setCorrection"
        binaryMessenger:binaryMessenger
        codec:CameraInterfaceGetCodec()];
    if (api) {
      NSCAssert([api respondsToSelector:@selector(setCorrectionBrightness:error:)], @"CameraInterface api (%@) doesn't respond to @selector(setCorrectionBrightness:error:)", api);
      [channel setMessageHandler:^(id _Nullable message, FlutterReply callback) {
        NSArray *args = message;
        NSNumber *arg_brightness = GetNullableObjectAtIndex(args, 0);
        FlutterError *error;
        [api setCorrectionBrightness:arg_brightness error:&error];
        callback(wrapResult(nil, error));
      }];
    } else {
      [channel setMessageHandler:nil];
    }
  }
  {
    FlutterBasicMessageChannel *channel =
      [[FlutterBasicMessageChannel alloc]
        initWithName:@"dev.flutter.pigeon.CameraInterface.getMinZoom"
        binaryMessenger:binaryMessenger
        codec:CameraInterfaceGetCodec()];
    if (api) {
      NSCAssert([api respondsToSelector:@selector(getMinZoomWithError:)], @"CameraInterface api (%@) doesn't respond to @selector(getMinZoomWithError:)", api);
      [channel setMessageHandler:^(id _Nullable message, FlutterReply callback) {
        FlutterError *error;
        NSNumber *output = [api getMinZoomWithError:&error];
        callback(wrapResult(output, error));
      }];
    } else {
      [channel setMessageHandler:nil];
    }
  }
  {
    FlutterBasicMessageChannel *channel =
      [[FlutterBasicMessageChannel alloc]
        initWithName:@"dev.flutter.pigeon.CameraInterface.getMaxZoom"
        binaryMessenger:binaryMessenger
        codec:CameraInterfaceGetCodec()];
    if (api) {
      NSCAssert([api respondsToSelector:@selector(getMaxZoomWithError:)], @"CameraInterface api (%@) doesn't respond to @selector(getMaxZoomWithError:)", api);
      [channel setMessageHandler:^(id _Nullable message, FlutterReply callback) {
        FlutterError *error;
        NSNumber *output = [api getMaxZoomWithError:&error];
        callback(wrapResult(output, error));
      }];
    } else {
      [channel setMessageHandler:nil];
    }
  }
  {
    FlutterBasicMessageChannel *channel =
      [[FlutterBasicMessageChannel alloc]
        initWithName:@"dev.flutter.pigeon.CameraInterface.setCaptureMode"
        binaryMessenger:binaryMessenger
        codec:CameraInterfaceGetCodec()];
    if (api) {
      NSCAssert([api respondsToSelector:@selector(setCaptureModeMode:error:)], @"CameraInterface api (%@) doesn't respond to @selector(setCaptureModeMode:error:)", api);
      [channel setMessageHandler:^(id _Nullable message, FlutterReply callback) {
        NSArray *args = message;
        NSString *arg_mode = GetNullableObjectAtIndex(args, 0);
        FlutterError *error;
        [api setCaptureModeMode:arg_mode error:&error];
        callback(wrapResult(nil, error));
      }];
    } else {
      [channel setMessageHandler:nil];
    }
  }
  {
    FlutterBasicMessageChannel *channel =
      [[FlutterBasicMessageChannel alloc]
        initWithName:@"dev.flutter.pigeon.CameraInterface.setRecordingAudioMode"
        binaryMessenger:binaryMessenger
        codec:CameraInterfaceGetCodec()];
    if (api) {
      NSCAssert([api respondsToSelector:@selector(setRecordingAudioModeEnableAudio:completion:)], @"CameraInterface api (%@) doesn't respond to @selector(setRecordingAudioModeEnableAudio:completion:)", api);
      [channel setMessageHandler:^(id _Nullable message, FlutterReply callback) {
        NSArray *args = message;
        NSNumber *arg_enableAudio = GetNullableObjectAtIndex(args, 0);
        [api setRecordingAudioModeEnableAudio:arg_enableAudio completion:^(NSNumber *_Nullable output, FlutterError *_Nullable error) {
          callback(wrapResult(output, error));
        }];
      }];
    } else {
      [channel setMessageHandler:nil];
    }
  }
  {
    FlutterBasicMessageChannel *channel =
      [[FlutterBasicMessageChannel alloc]
        initWithName:@"dev.flutter.pigeon.CameraInterface.availableSizes"
        binaryMessenger:binaryMessenger
        codec:CameraInterfaceGetCodec()];
    if (api) {
      NSCAssert([api respondsToSelector:@selector(availableSizesWithError:)], @"CameraInterface api (%@) doesn't respond to @selector(availableSizesWithError:)", api);
      [channel setMessageHandler:^(id _Nullable message, FlutterReply callback) {
        FlutterError *error;
        NSArray<PreviewSize *> *output = [api availableSizesWithError:&error];
        callback(wrapResult(output, error));
      }];
    } else {
      [channel setMessageHandler:nil];
    }
  }
  {
    FlutterBasicMessageChannel *channel =
      [[FlutterBasicMessageChannel alloc]
        initWithName:@"dev.flutter.pigeon.CameraInterface.refresh"
        binaryMessenger:binaryMessenger
        codec:CameraInterfaceGetCodec()];
    if (api) {
      NSCAssert([api respondsToSelector:@selector(refreshWithError:)], @"CameraInterface api (%@) doesn't respond to @selector(refreshWithError:)", api);
      [channel setMessageHandler:^(id _Nullable message, FlutterReply callback) {
        FlutterError *error;
        [api refreshWithError:&error];
        callback(wrapResult(nil, error));
      }];
    } else {
      [channel setMessageHandler:nil];
    }
  }
  {
    FlutterBasicMessageChannel *channel =
      [[FlutterBasicMessageChannel alloc]
        initWithName:@"dev.flutter.pigeon.CameraInterface.getEffectivPreviewSize"
        binaryMessenger:binaryMessenger
        codec:CameraInterfaceGetCodec()];
    if (api) {
      NSCAssert([api respondsToSelector:@selector(getEffectivPreviewSizeIndex:error:)], @"CameraInterface api (%@) doesn't respond to @selector(getEffectivPreviewSizeIndex:error:)", api);
      [channel setMessageHandler:^(id _Nullable message, FlutterReply callback) {
        NSArray *args = message;
        NSNumber *arg_index = GetNullableObjectAtIndex(args, 0);
        FlutterError *error;
        PreviewSize *output = [api getEffectivPreviewSizeIndex:arg_index error:&error];
        callback(wrapResult(output, error));
      }];
    } else {
      [channel setMessageHandler:nil];
    }
  }
  {
    FlutterBasicMessageChannel *channel =
      [[FlutterBasicMessageChannel alloc]
        initWithName:@"dev.flutter.pigeon.CameraInterface.setPhotoSize"
        binaryMessenger:binaryMessenger
        codec:CameraInterfaceGetCodec()];
    if (api) {
      NSCAssert([api respondsToSelector:@selector(setPhotoSizeSize:error:)], @"CameraInterface api (%@) doesn't respond to @selector(setPhotoSizeSize:error:)", api);
      [channel setMessageHandler:^(id _Nullable message, FlutterReply callback) {
        NSArray *args = message;
        PreviewSize *arg_size = GetNullableObjectAtIndex(args, 0);
        FlutterError *error;
        [api setPhotoSizeSize:arg_size error:&error];
        callback(wrapResult(nil, error));
      }];
    } else {
      [channel setMessageHandler:nil];
    }
  }
  {
    FlutterBasicMessageChannel *channel =
      [[FlutterBasicMessageChannel alloc]
        initWithName:@"dev.flutter.pigeon.CameraInterface.setPreviewSize"
        binaryMessenger:binaryMessenger
        codec:CameraInterfaceGetCodec()];
    if (api) {
      NSCAssert([api respondsToSelector:@selector(setPreviewSizeSize:error:)], @"CameraInterface api (%@) doesn't respond to @selector(setPreviewSizeSize:error:)", api);
      [channel setMessageHandler:^(id _Nullable message, FlutterReply callback) {
        NSArray *args = message;
        PreviewSize *arg_size = GetNullableObjectAtIndex(args, 0);
        FlutterError *error;
        [api setPreviewSizeSize:arg_size error:&error];
        callback(wrapResult(nil, error));
      }];
    } else {
      [channel setMessageHandler:nil];
    }
  }
  {
    FlutterBasicMessageChannel *channel =
      [[FlutterBasicMessageChannel alloc]
        initWithName:@"dev.flutter.pigeon.CameraInterface.setAspectRatio"
        binaryMessenger:binaryMessenger
        codec:CameraInterfaceGetCodec()];
    if (api) {
      NSCAssert([api respondsToSelector:@selector(setAspectRatioAspectRatio:error:)], @"CameraInterface api (%@) doesn't respond to @selector(setAspectRatioAspectRatio:error:)", api);
      [channel setMessageHandler:^(id _Nullable message, FlutterReply callback) {
        NSArray *args = message;
        NSString *arg_aspectRatio = GetNullableObjectAtIndex(args, 0);
        FlutterError *error;
        [api setAspectRatioAspectRatio:arg_aspectRatio error:&error];
        callback(wrapResult(nil, error));
      }];
    } else {
      [channel setMessageHandler:nil];
    }
  }
  {
    FlutterBasicMessageChannel *channel =
      [[FlutterBasicMessageChannel alloc]
        initWithName:@"dev.flutter.pigeon.CameraInterface.setupImageAnalysisStream"
        binaryMessenger:binaryMessenger
        codec:CameraInterfaceGetCodec()];
    if (api) {
      NSCAssert([api respondsToSelector:@selector(setupImageAnalysisStreamFormat:width:maxFramesPerSecond:autoStart:error:)], @"CameraInterface api (%@) doesn't respond to @selector(setupImageAnalysisStreamFormat:width:maxFramesPerSecond:autoStart:error:)", api);
      [channel setMessageHandler:^(id _Nullable message, FlutterReply callback) {
        NSArray *args = message;
        NSString *arg_format = GetNullableObjectAtIndex(args, 0);
        NSNumber *arg_width = GetNullableObjectAtIndex(args, 1);
        NSNumber *arg_maxFramesPerSecond = GetNullableObjectAtIndex(args, 2);
        NSNumber *arg_autoStart = GetNullableObjectAtIndex(args, 3);
        FlutterError *error;
        [api setupImageAnalysisStreamFormat:arg_format width:arg_width maxFramesPerSecond:arg_maxFramesPerSecond autoStart:arg_autoStart error:&error];
        callback(wrapResult(nil, error));
      }];
    } else {
      [channel setMessageHandler:nil];
    }
  }
  {
    FlutterBasicMessageChannel *channel =
      [[FlutterBasicMessageChannel alloc]
        initWithName:@"dev.flutter.pigeon.CameraInterface.setExifPreferences"
        binaryMessenger:binaryMessenger
        codec:CameraInterfaceGetCodec()];
    if (api) {
      NSCAssert([api respondsToSelector:@selector(setExifPreferencesExifPreferences:completion:)], @"CameraInterface api (%@) doesn't respond to @selector(setExifPreferencesExifPreferences:completion:)", api);
      [channel setMessageHandler:^(id _Nullable message, FlutterReply callback) {
        NSArray *args = message;
        ExifPreferences *arg_exifPreferences = GetNullableObjectAtIndex(args, 0);
        [api setExifPreferencesExifPreferences:arg_exifPreferences completion:^(NSNumber *_Nullable output, FlutterError *_Nullable error) {
          callback(wrapResult(output, error));
        }];
      }];
    } else {
      [channel setMessageHandler:nil];
    }
  }
  {
    FlutterBasicMessageChannel *channel =
      [[FlutterBasicMessageChannel alloc]
        initWithName:@"dev.flutter.pigeon.CameraInterface.startAnalysis"
        binaryMessenger:binaryMessenger
        codec:CameraInterfaceGetCodec()];
    if (api) {
      NSCAssert([api respondsToSelector:@selector(startAnalysisWithError:)], @"CameraInterface api (%@) doesn't respond to @selector(startAnalysisWithError:)", api);
      [channel setMessageHandler:^(id _Nullable message, FlutterReply callback) {
        FlutterError *error;
        [api startAnalysisWithError:&error];
        callback(wrapResult(nil, error));
      }];
    } else {
      [channel setMessageHandler:nil];
    }
  }
  {
    FlutterBasicMessageChannel *channel =
      [[FlutterBasicMessageChannel alloc]
        initWithName:@"dev.flutter.pigeon.CameraInterface.stopAnalysis"
        binaryMessenger:binaryMessenger
        codec:CameraInterfaceGetCodec()];
    if (api) {
      NSCAssert([api respondsToSelector:@selector(stopAnalysisWithError:)], @"CameraInterface api (%@) doesn't respond to @selector(stopAnalysisWithError:)", api);
      [channel setMessageHandler:^(id _Nullable message, FlutterReply callback) {
        FlutterError *error;
        [api stopAnalysisWithError:&error];
        callback(wrapResult(nil, error));
      }];
    } else {
      [channel setMessageHandler:nil];
    }
  }
  {
    FlutterBasicMessageChannel *channel =
      [[FlutterBasicMessageChannel alloc]
        initWithName:@"dev.flutter.pigeon.CameraInterface.setFilter"
        binaryMessenger:binaryMessenger
        codec:CameraInterfaceGetCodec()];
    if (api) {
      NSCAssert([api respondsToSelector:@selector(setFilterMatrix:error:)], @"CameraInterface api (%@) doesn't respond to @selector(setFilterMatrix:error:)", api);
      [channel setMessageHandler:^(id _Nullable message, FlutterReply callback) {
        NSArray *args = message;
        NSArray<NSNumber *> *arg_matrix = GetNullableObjectAtIndex(args, 0);
        FlutterError *error;
        [api setFilterMatrix:arg_matrix error:&error];
        callback(wrapResult(nil, error));
      }];
    } else {
      [channel setMessageHandler:nil];
    }
  }
  {
    FlutterBasicMessageChannel *channel =
      [[FlutterBasicMessageChannel alloc]
        initWithName:@"dev.flutter.pigeon.CameraInterface.isVideoRecordingAndImageAnalysisSupported"
        binaryMessenger:binaryMessenger
        codec:CameraInterfaceGetCodec()];
    if (api) {
      NSCAssert([api respondsToSelector:@selector(isVideoRecordingAndImageAnalysisSupportedSensor:completion:)], @"CameraInterface api (%@) doesn't respond to @selector(isVideoRecordingAndImageAnalysisSupportedSensor:completion:)", api);
      [channel setMessageHandler:^(id _Nullable message, FlutterReply callback) {
        NSArray *args = message;
        PigeonSensorPosition arg_sensor = [GetNullableObjectAtIndex(args, 0) integerValue];
        [api isVideoRecordingAndImageAnalysisSupportedSensor:arg_sensor completion:^(NSNumber *_Nullable output, FlutterError *_Nullable error) {
          callback(wrapResult(output, error));
        }];
      }];
    } else {
      [channel setMessageHandler:nil];
    }
  }
  {
    FlutterBasicMessageChannel *channel =
      [[FlutterBasicMessageChannel alloc]
        initWithName:@"dev.flutter.pigeon.CameraInterface.isMultiCamSupported"
        binaryMessenger:binaryMessenger
        codec:CameraInterfaceGetCodec()];
    if (api) {
      NSCAssert([api respondsToSelector:@selector(isMultiCamSupportedWithError:)], @"CameraInterface api (%@) doesn't respond to @selector(isMultiCamSupportedWithError:)", api);
      [channel setMessageHandler:^(id _Nullable message, FlutterReply callback) {
        FlutterError *error;
        NSNumber *output = [api isMultiCamSupportedWithError:&error];
        callback(wrapResult(output, error));
      }];
    } else {
      [channel setMessageHandler:nil];
    }
  }
}



================================================
FILE: ios/camerawesome/Sources/camerawesome/Utils/CameraQualities.m
================================================
//
//  CameraQualities.m
//  camerawesome
//
//  Created by Dimitri Dessus on 24/07/2020.
//

#import "CameraQualities.h"

// TODO: rework how qualities are working to be more easy
@implementation CameraQualities

+ (AVCaptureSessionPreset)selectVideoCapturePreset:(CGSize)size session:(AVCaptureSession *)session device:(AVCaptureDevice *)device {
  if (!CGSizeEqualToSize(CGSizeZero, size)) {
    AVCaptureSessionPreset bestPreset = [CameraQualities selectPresetForSize:size session:session];
    if ([session canSetSessionPreset:bestPreset]) {
      return bestPreset;
    }
  }
  
  return [self computeBestPresetWithSession:session device:device];
}

+ (NSString *)selectVideoCapturePreset:(AVCaptureSession *)session device:(AVCaptureDevice *)device {
  return [self computeBestPresetWithSession:session device:device];
}

+ (CGSize)getSizeForPreset:(NSString *)preset {
  if (preset == AVCaptureSessionPreset3840x2160) {
    return CGSizeMake(3840, 2160);
  } else if (preset == AVCaptureSessionPreset1920x1080) {
    return CGSizeMake(1920, 1080);
  } else if (preset == AVCaptureSessionPreset1280x720) {
    return CGSizeMake(1280, 720);
  } else if (preset == AVCaptureSessionPreset640x480) {
    return CGSizeMake(640, 480);
  } else if (preset == AVCaptureSessionPreset352x288) {
    return CGSizeMake(352, 288);
  } else {
    // Default to HD
    return CGSizeMake(1280, 720);
  }
}

+ (AVCaptureSessionPreset)computeBestPresetWithSession:(AVCaptureSession *)session device:(AVCaptureDevice *)device {
  NSArray *qualities = [CameraQualities captureFormatsForDevice:device];
  
  for (NSDictionary *quality in qualities) {
    CGSize qualitySize = CGSizeMake([quality[@"width"] floatValue], [quality[@"height"] floatValue]);
    AVCaptureSessionPreset currentPreset = [CameraQualities selectPresetForSize:qualitySize session:session];
    
    if ([session canSetSessionPreset:currentPreset]) {
      return currentPreset;
    }
  }
  
  // Default to HD
  return AVCaptureSessionPreset1280x720;
}

+ (NSString *)selectPresetForSize:(CGSize)size session:(AVCaptureSession *)session {
  if (size.width >= 2160 || size.height >= 3840) {
    if (@available(iOS 9.0, *)) {
      // we don't know the exact size, so we check if it can apply
      // if not, apply basic 1920x1080
      if ([session canSetSessionPreset:AVCaptureSessionPreset3840x2160]) {
        return AVCaptureSessionPreset3840x2160;
      } else {
        return AVCaptureSessionPreset1920x1080;
      }
      return AVCaptureSessionPreset3840x2160;
    } else {
      return AVCaptureSessionPreset1920x1080;
    }
  } else if (size.width == 1080 && size.height == 1920) {
    return AVCaptureSessionPreset1920x1080;
  } else if (size.width == 720 && size.height == 1280) {
    return AVCaptureSessionPreset1280x720;
  } else if (size.width == 480 && size.height == 640) {
    return AVCaptureSessionPreset640x480;
  } else if (size.width == 288 && size.height == 352) {
    return AVCaptureSessionPreset352x288;
  } else {
    // Default to HD
    return AVCaptureSessionPreset1280x720;
  }
}

+ (NSArray *)captureFormatsForDevice:(AVCaptureDevice *)device  {
  NSMutableArray *qualities = [[NSMutableArray alloc] init];
  NSArray<AVCaptureDeviceFormat *>* formats = [device formats];
  for(int i = 0; i < formats.count; i++) {
    AVCaptureDeviceFormat *format = formats[i];
    [qualities addObject:
       [PreviewSize makeWithWidth:[NSNumber numberWithDouble:CMVideoFormatDescriptionGetDimensions(format.formatDescription).width] height:[NSNumber numberWithDouble:CMVideoFormatDescriptionGetDimensions(format.formatDescription).height]]
    ];
  }
  return qualities;
}

@end



================================================
FILE: ios/camerawesome/Sources/camerawesome/Utils/AspectRatio/AspectRatioUtils.m
================================================
//
//  AspectRatioUtils.m
//  camerawesome
//
//  Created by Dimitri Dessus on 29/03/2023.
//

#import "AspectRatioUtils.h"

@implementation AspectRatioUtils

+ (AspectRatio)convertAspectRatio:(NSString *)aspectRatioStr {
  AspectRatio aspectRatioMode;
  if ([aspectRatioStr isEqualToString:@"RATIO_4_3"]) {
    aspectRatioMode = Ratio4_3;
  } else if ([aspectRatioStr isEqualToString:@"RATIO_16_9"]) {
    aspectRatioMode = Ratio16_9;
  } else {
    aspectRatioMode = Ratio1_1;
  }
  return aspectRatioMode;
}

@end



================================================
FILE: ios/camerawesome/Sources/camerawesome/Utils/CaptureMode/CaptureModeUtils.m
================================================
//
//  CaptureModeUtils.m
//  camerawesome
//
//  Created by Dimitri Dessus on 10/05/2023.
//

#import "CaptureModeUtils.h"

@implementation CaptureModeUtils

+ (CaptureModes)captureModeFromCaptureModeType:(NSString *)captureModeType {
  if ([captureModeType isEqualToString:@"PHOTO"]) {
    return Photo;
  } else if ([captureModeType isEqualToString:@"VIDEO"]) {
    return Video;
  } else if ([captureModeType isEqualToString:@"PREVIEW"]) {
    return Preview;
  } else {
    return AnalysisOnly;
  }
}

@end



================================================
FILE: ios/camerawesome/Sources/camerawesome/Utils/FlashMode/FlashModeUtils.m
================================================
//
//  FlashModeUtils.m
//  camerawesome
//
//  Created by Dimitri Dessus on 29/03/2023.
//

#import "FlashModeUtils.h"

@implementation FlashModeUtils

+ (CameraFlashMode)flashFromString:(NSString *)mode {
  CameraFlashMode flash;
  
  if ([mode isEqualToString:@"NONE"]) {
    flash = None;
  } else if ([mode isEqualToString:@"ON"]) {
    flash = On;
  } else if ([mode isEqualToString:@"AUTO"]) {
    flash = Auto;
  } else if ([mode isEqualToString:@"ALWAYS"]) {
    flash = Always;
  } else {
    flash = None;
  }
  
  return flash;
}

@end



================================================
FILE: ios/camerawesome/Sources/camerawesome/Utils/Sensor/SensorUtils.m
================================================
//
//  SensorUtils.m
//  camerawesome
//
//  Created by Dimitri Dessus on 30/03/2023.
//

#import "SensorUtils.h"

@implementation SensorUtils

+ (PigeonSensorType)sensorTypeFromDeviceType:(AVCaptureDeviceType)type {
  if (type == AVCaptureDeviceTypeBuiltInTelephotoCamera) {
    return PigeonSensorTypeTelephoto;
  } else if (type == AVCaptureDeviceTypeBuiltInUltraWideCamera) {
    return PigeonSensorTypeUltraWideAngle;
  } else if (type == AVCaptureDeviceTypeBuiltInTrueDepthCamera) {
    return PigeonSensorTypeTrueDepth;
  } else if (type == AVCaptureDeviceTypeBuiltInWideAngleCamera) {
    return PigeonSensorTypeWideAngle;
  } else {
    return PigeonSensorTypeUnknown;
  }
}

+ (AVCaptureDeviceType)deviceTypeFromSensorType:(PigeonSensorType)sensorType {
  if (sensorType == PigeonSensorTypeTelephoto) {
    return AVCaptureDeviceTypeBuiltInTelephotoCamera;
  } else if (sensorType == PigeonSensorTypeUltraWideAngle) {
    return AVCaptureDeviceTypeBuiltInUltraWideCamera;
  } else if (sensorType == PigeonSensorTypeTrueDepth) {
    return AVCaptureDeviceTypeBuiltInTrueDepthCamera;
  } else if (sensorType == PigeonSensorTypeWideAngle) {
    return AVCaptureDeviceTypeBuiltInWideAngleCamera;
  } else {
    return AVCaptureDeviceTypeBuiltInWideAngleCamera;
  }
}

@end



================================================
FILE: lib/camerawesome_plugin.dart
================================================
import 'dart:async';
import 'dart:io';

import 'package:camerawesome/camerawesome_plugin.dart';
import 'package:camerawesome/pigeon.dart';
import 'package:camerawesome/src/logger.dart';
import 'package:camerawesome/src/orchestrator/adapters/pigeon_sensor_adapter.dart';
import 'package:camerawesome/src/orchestrator/models/camera_physical_button.dart';
import 'package:collection/collection.dart';
import 'package:flutter/services.dart';

export 'src/camera_characteristics/camera_characteristics.dart';
export 'src/orchestrator/analysis/analysis_controller.dart';
export 'src/orchestrator/models/models.dart';
export 'src/orchestrator/models/sensor_type.dart';
export 'src/orchestrator/models/sensors.dart';
export 'src/orchestrator/states/states.dart';
export 'src/widgets/camera_awesome_builder.dart';
export 'src/orchestrator/analysis/analysis_to_image.dart';
export 'src/orchestrator/models/analysis/analysis_canvas.dart';

// filters
export 'src/orchestrator/models/filters/awesome_filters.dart';

// built in widgets
export 'src/widgets/widgets.dart';

// ignore: public_member_api_docs
enum CameraRunningState { starting, started, stopping, stopped }

/// Don't use this class directly. Instead, use [CameraAwesomeBuilder].
class CamerawesomePlugin {
  static const EventChannel _orientationChannel =
      EventChannel('camerawesome/orientation');

  static const EventChannel _permissionsChannel =
      EventChannel('camerawesome/permissions');

  static const EventChannel _imagesChannel =
      EventChannel('camerawesome/images');

  static const EventChannel _physicalButtonChannel =
      EventChannel('camerawesome/physical_button');

  static Stream<CameraOrientations>? _orientationStream;

  static Stream<CameraPhysicalButton>? _physicalButtonStream;

  static Stream<bool>? _permissionsStream;

  static Stream<Map<String, dynamic>>? _imagesStream;

  static CameraRunningState currentState = CameraRunningState.stopped;

  /// Set it to true to print dart logs from camerawesome
  static bool printLogs = false;

  static Future<bool?> checkiOSPermissions(
      List<String?> permissionsName) async {
    final permissions =
        await CameraInterface().checkPermissions(permissionsName);
    return permissions.isEmpty;
  }

  static Future<bool> start() async {
    if (currentState == CameraRunningState.started ||
        currentState == CameraRunningState.starting) {
      return true;
    }
    currentState = CameraRunningState.starting;
    bool res = await CameraInterface().start();
    if (res) currentState = CameraRunningState.started;
    return res;
  }

  static Future<bool> stop() async {
    if (currentState == CameraRunningState.stopped ||
        currentState == CameraRunningState.stopping) {
      return true;
    }
    _orientationStream = null;
    currentState = CameraRunningState.stopping;
    bool res;
    try {
      res = await CameraInterface().stop();
    } catch (e) {
      return false;
    }
    currentState = CameraRunningState.stopped;
    return res;
  }

  static Stream<CameraOrientations>? getNativeOrientation() {
    _orientationStream ??= _orientationChannel
        .receiveBroadcastStream('orientationChannel')
        .transform(StreamTransformer<dynamic, CameraOrientations>.fromHandlers(
            handleData: (data, sink) {
      CameraOrientations? newOrientation;
      switch (data) {
        case 'LANDSCAPE_LEFT':
          newOrientation = CameraOrientations.landscape_left;
          break;
        case 'LANDSCAPE_RIGHT':
          newOrientation = CameraOrientations.landscape_right;
          break;
        case 'PORTRAIT_UP':
          newOrientation = CameraOrientations.portrait_up;
          break;
        case 'PORTRAIT_DOWN':
          newOrientation = CameraOrientations.portrait_down;
          break;
        default:
      }
      sink.add(newOrientation!);
    }));
    return _orientationStream;
  }

  static Stream<CameraPhysicalButton>? listenPhysicalButton() {
    _physicalButtonStream ??= _physicalButtonChannel
        .receiveBroadcastStream('physicalButtonChannel')
        .transform(
            StreamTransformer<dynamic, CameraPhysicalButton>.fromHandlers(
                handleData: (data, sink) {
      CameraPhysicalButton? physicalButton;
      switch (data) {
        case 'VOLUME_UP':
          physicalButton = CameraPhysicalButton.volume_up;
          break;
        case 'VOLUME_DOWN':
          physicalButton = CameraPhysicalButton.volume_down;
          break;
        default:
      }
      sink.add(physicalButton!);
    }));
    return _physicalButtonStream;
  }

  static Stream<bool>? listenPermissionResult() {
    _permissionsStream ??= _permissionsChannel
        .receiveBroadcastStream('permissionsChannel')
        .transform(StreamTransformer<dynamic, bool>.fromHandlers(
            handleData: (data, sink) {
      sink.add(data);
    }));
    return _permissionsStream;
  }

  static Future<void> setupAnalysis({
    int width = 0,
    double? maxFramesPerSecond,
    required InputAnalysisImageFormat format,
    required bool autoStart,
  }) async {
    return CameraInterface().setupImageAnalysisStream(
      format.name,
      width,
      maxFramesPerSecond,
      autoStart,
    );
  }

  static Stream<Map<String, dynamic>>? listenCameraImages() {
    _imagesStream ??=
        _imagesChannel.receiveBroadcastStream('imagesChannel').transform(
      StreamTransformer<dynamic, Map<String, dynamic>>.fromHandlers(
        handleData: (data, sink) {
          sink.add(Map<String, dynamic>.from(data));
        },
      ),
    );
    return _imagesStream;
  }

  static Future receivedImageFromStream() {
    return CameraInterface().receivedImageFromStream();
  }

  static Future<bool?> init(
    SensorConfig sensorConfig,
    bool enableImageStream,
    bool enablePhysicalButton, {
    CaptureMode captureMode = CaptureMode.photo,
    required ExifPreferences exifPreferences,
    required VideoOptions? videoOptions,
    required bool mirrorFrontCamera,
  }) async {
    return CameraInterface()
        .setupCamera(
          sensorConfig.sensors.map((e) {
            return e.toPigeon();
          }).toList(),
          sensorConfig.aspectRatio.name.toUpperCase(),
          sensorConfig.zoom,
          mirrorFrontCamera,
          enablePhysicalButton,
          sensorConfig.flashMode.name.toUpperCase(),
          captureMode.name.toUpperCase(),
          enableImageStream,
          exifPreferences,
          videoOptions,
        )
        .then((value) => true);
  }

  static Future<List<Size>> getSizes() async {
    final availableSizes = await CameraInterface().availableSizes();
    return availableSizes
        .whereType<PreviewSize>()
        .map((e) => Size(e.width, e.height))
        .toList();
  }

  static Future<num?> getPreviewTexture(final int cameraPosition) {
    return CameraInterface().getPreviewTextureId(cameraPosition);
  }

  static Future<void> setPreviewSize(int width, int height) {
    return CameraInterface().setPreviewSize(
        PreviewSize(width: width.toDouble(), height: height.toDouble()));
  }

  static Future<void> refresh() {
    return CameraInterface().refresh();
  }

  /// android has a limits on preview size and fallback to 1920x1080 if preview is too big
  /// So to prevent having different ratio we get the real preview Size directly from nativ side
  static Future<PreviewSize> getEffectivPreviewSize(int index) async {
    final ps = await CameraInterface().getEffectivPreviewSize(index);
    if (ps != null) {
      return PreviewSize(width: ps.width, height: ps.height);
    } else {
      // TODO Should not be null?
      return PreviewSize(width: 0, height: 0);
    }
  }

  /// you can set a different size for preview and for photo
  /// for iOS, when taking a photo, best quality is automatically used
  static Future<void> setPhotoSize(int width, int height) {
    return CameraInterface().setPhotoSize(
      PreviewSize(
        width: width.toDouble(),
        height: height.toDouble(),
      ),
    );
  }

  static Future<bool> takePhoto(CaptureRequest captureRequest) async {
    final request = captureRequest.when(
      single: (single) => {
        single.sensor.toPigeon(): single.file?.path,
      },
      multiple: (multiple) => multiple.fileBySensor.map((key, value) {
        return MapEntry(key.toPigeon(), value?.path);
      }),
    );

    return CameraInterface().takePhoto(
      request.keys.toList(),
      request.values.toList(),
    );
  }

  static Future<void> recordVideo(CaptureRequest request) {
    final pathBySensor = request.when(
      single: (single) => {
        single.sensor.toPigeon(): single.file?.path,
      },
      multiple: (multiple) => multiple.fileBySensor
          .map((key, value) => MapEntry(key.toPigeon(), value?.path)),
    );
    if (Platform.isAndroid) {
      return CameraInterface().recordVideo(
        pathBySensor.keys.toList(),
        pathBySensor.values.toList(),
      );
    } else {
      return CameraInterface().recordVideo(
        pathBySensor.keys.toList(),
        pathBySensor.values.toList(),
      );
    }
  }

  static pauseVideoRecording() {
    CameraInterface().pauseVideoRecording();
  }

  static resumeVideoRecording() {
    return CameraInterface().resumeVideoRecording();
  }

  static stopRecordingVideo() {
    return CameraInterface().stopRecordingVideo();
  }

  /// Switch flash mode from Android / iOS
  static Future<void> setFlashMode(FlashMode flashMode) {
    return CameraInterface().setFlashMode(flashMode.name.toUpperCase());
  }

  static startAutoFocus() {
    return CameraInterface().handleAutoFocus();
  }

  /// Start auto focus on a specific [position] with a given [previewSize].
  ///
  /// On Android, you can set [androidFocusSettings].
  /// It contains a parameter [AndroidFocusSettings.autoCancelDurationInMillis].
  /// It is the time in milliseconds after which the auto focus will be canceled.
  /// Passive focus will resume after that duration.
  ///
  /// If that duration is equals to or less than 0, auto focus is never
  /// cancelled and passive focus will not resume. After this, if you want to
  /// focus on an other point, you'll have to call again [focusOnPoint].
  static Future<void> focusOnPoint({
    required PreviewSize previewSize,
    required Offset position,
    required AndroidFocusSettings? androidFocusSettings,
  }) {
    return CameraInterface().focusOnPoint(
      previewSize,
      position.dx,
      position.dy,
      androidFocusSettings,
    );
  }

  /// calls zoom from Android / iOS --
  static Future<void> setZoom(num zoom) {
    return CameraInterface().setZoom(zoom.toDouble());
  }

  /// switch camera sensor between [Sensors.back] and [Sensors.front]
  /// on iOS, you can specify the deviceId if you have multiple cameras
  /// call [getSensors] to get the list of available cameras
  static Future<void> setSensor(List<Sensor?> sensors) {
    return CameraInterface().setSensor(
      sensors.map((e) {
        return PigeonSensor(
          position: e?.position?.name != null
              ? PigeonSensorPosition.values.byName(e!.position!.name)
              : PigeonSensorPosition.unknown,
          deviceId: e?.deviceId,
          type: e?.type?.name != null
              ? PigeonSensorType.values.byName(e!.type!.name)
              : PigeonSensorType.unknown,
        );
      }).toList(),
    );
  }

  /// change capture mode between [CaptureMode.photo] and [CaptureMode.video]
  static Future<void> setCaptureMode(CaptureMode captureMode) {
    return CameraInterface().setCaptureMode(captureMode.name.toUpperCase());
  }

  /// enable audio mode recording or not
  static Future<void> setAudioMode(bool enableAudio) {
    return CameraInterface().setRecordingAudioMode(enableAudio);
  }

  /// set exif preferences when a photo is saved
  ///
  /// The GPS value can be null on Android if:
  /// - Location is disabled on the phone
  /// - ExifPreferences.saveGPSLocation is false
  /// - Permission ACCESS_FINE_LOCATION has not been granted
  static Future<bool> setExifPreferences(ExifPreferences savedExifData) {
    return CameraInterface().setExifPreferences(savedExifData);
  }

  /// set brightness manually with range [0,1]
  static Future<void> setBrightness(double brightness) {
    if (brightness < 0 || brightness > 1) {
      throw "Value must be between [0,1]";
    }
    return CameraInterface().setCorrection(brightness);
  }

  /// returns the max zoom available on device
  static Future<double?> getMaxZoom() {
    return CameraInterface().getMaxZoom();
  }

  /// returns the min zoom available on device
  static Future<double?> getMinZoom() {
    return CameraInterface().getMinZoom();
  }

  static Future<bool> isMultiCamSupported() {
    return CameraInterface().isMultiCamSupported();
  }

  /// Change aspect ratio when a photo is taken
  static Future<void> setAspectRatio(String ratio) {
    return CameraInterface().setAspectRatio(ratio.toUpperCase());
  }

  // TODO: implement it on Android
  /// Returns the list of available sensors on device.
  ///
  /// The list contains the back and front sensors
  /// with their name, type, uid, iso and flash availability
  ///
  /// Only available on iOS for now
  static Future<SensorDeviceData> getSensors() async {
    if (Platform.isAndroid) {
      return Future.value(SensorDeviceData());
    } else {
      // Can't use getter with pigeon, so we have to map the data manually...
      final frontSensors = await CameraInterface().getFrontSensors();
      final backSensors = await CameraInterface().getBackSensors();

      final frontSensorsData = frontSensors
          .map(
            (data) => SensorTypeDevice(
              flashAvailable: data!.flashAvailable,
              iso: data.iso,
              name: data.name,
              uid: data.uid,
              sensorType: SensorType.values.firstWhere(
                (element) => element.name == data.sensorType.name,
              ),
            ),
          )
          .toList();
      final backSensorsData = backSensors
          .map(
            (data) => SensorTypeDevice(
              flashAvailable: data!.flashAvailable,
              iso: data.iso,
              name: data.name,
              uid: data.uid,
              sensorType: SensorType.values.firstWhere(
                (element) => element.name == data.sensorType.name,
              ),
            ),
          )
          .toList();

      return SensorDeviceData(
        ultraWideAngle: backSensorsData
            .where(
              (element) => element.sensorType == SensorType.ultraWideAngle,
            )
            .toList()
            .firstOrNull,
        telephoto: backSensorsData
            .where(
              (element) => element.sensorType == SensorType.telephoto,
            )
            .toList()
            .firstOrNull,
        wideAngle: backSensorsData
            .where(
              (element) => element.sensorType == SensorType.wideAngle,
            )
            .toList()
            .firstOrNull,
        trueDepth: frontSensorsData
            .where(
              (element) => element.sensorType == SensorType.trueDepth,
            )
            .toList()
            .firstOrNull,
      );
    }
  }

  // ---------------------------------------------------
  // UTILITY METHODS
  // ---------------------------------------------------
  static Future<List<CamerAwesomePermission>?> checkAndRequestPermissions(
    bool saveGpsLocation, {
    bool checkMicrophonePermissions = true,
    bool checkCameraPermissions = true,
  }) async {
    try {
      if (Platform.isAndroid) {
        return CameraInterface()
            .requestPermissions(saveGpsLocation)
            .then((givenPermissions) {
          return givenPermissions
              .map((e) => CamerAwesomePermission.values
                  .firstWhere((element) => element.name == e))
              .toList();
        });
      } else if (Platform.isIOS) {
        // TODO iOS Return only permissions that were given

        List<String> permissions = [];
        if (checkMicrophonePermissions) {
          permissions.add("microphone");
        }
        if (checkCameraPermissions) {
          permissions.add("camera");
        }

        return CamerawesomePlugin.checkiOSPermissions(permissions)
            .then((givenPermissions) => CamerAwesomePermission.values);
      }
    } catch (e) {
      printLog("failed to check permissions here...");
      // ignore: avoid_print
      print(e);
    }
    return Future.value([]);
  }

  static Future<void> startAnalysis() {
    return CameraInterface().startAnalysis();
  }

  static Future<void> stopAnalysis() {
    return CameraInterface().stopAnalysis();
  }

  static Future<void> setFilter(AwesomeFilter filter) {
    return CameraInterface().setFilter(filter.matrix);
  }

  static Future<void> setMirrorFrontCamera(bool mirrorFrontCamera) {
    return CameraInterface().setMirrorFrontCamera(mirrorFrontCamera);
  }
}



================================================
FILE: lib/pigeon.dart
================================================
// Autogenerated from Pigeon (v9.2.5), do not edit directly.
// See also: https://pub.dev/packages/pigeon
// ignore_for_file: public_member_api_docs, non_constant_identifier_names, avoid_as, unused_import, unnecessary_parenthesis, prefer_null_aware_operators, omit_local_variable_types, unused_shown_name, unnecessary_import

import 'dart:async';
import 'dart:typed_data' show Float64List, Int32List, Int64List, Uint8List;

import 'package:flutter/foundation.dart' show ReadBuffer, WriteBuffer;
import 'package:flutter/services.dart';

enum PigeonSensorPosition {
  back,
  front,
  unknown,
}

/// Video recording quality, from [sd] to [uhd], with [highest] and [lowest] to
/// let the device choose the best/worst quality available.
/// [highest] is the default quality.
///
/// Qualities are defined like this:
/// [sd] < [hd] < [fhd] < [uhd]
enum VideoRecordingQuality {
  lowest,
  sd,
  hd,
  fhd,
  uhd,
  highest,
}

/// If the specified [VideoRecordingQuality] is not available on the device,
/// the [VideoRecordingQuality] will fallback to [higher] or [lower] quality.
/// [higher] is the default fallback strategy.
enum QualityFallbackStrategy {
  higher,
  lower,
}

enum CupertinoFileType {
  quickTimeMovie,
  mpeg4,
  appleM4V,
  type3GPP,
  type3GPP2,
}

enum CupertinoCodecType {
  h264,
  hevc,
  hevcWithAlpha,
  jpeg,
  appleProRes4444,
  appleProRes422,
  appleProRes422HQ,
  appleProRes422LT,
  appleProRes422Proxy,
}

enum PigeonSensorType {
  /// A built-in wide-angle camera.
  ///
  /// The wide angle sensor is the default sensor for iOS
  wideAngle,

  /// A built-in camera with a shorter focal length than that of the wide-angle camera.
  ultraWideAngle,

  /// A built-in camera device with a longer focal length than the wide-angle camera.
  telephoto,

  /// A device that consists of two cameras, one Infrared and one YUV.
  ///
  /// iOS only
  trueDepth,
  unknown,
}

enum CamerAwesomePermission {
  storage,
  camera,
  location,
  recordAudio,
}

enum AnalysisImageFormat {
  yuv_420,
  bgra8888,
  jpeg,
  nv21,
  unknown,
}

enum AnalysisRotation {
  rotation0deg,
  rotation90deg,
  rotation180deg,
  rotation270deg,
}

class PreviewSize {
  PreviewSize({
    required this.width,
    required this.height,
  });

  double width;

  double height;

  Object encode() {
    return <Object?>[
      width,
      height,
    ];
  }

  static PreviewSize decode(Object result) {
    result as List<Object?>;
    return PreviewSize(
      width: result[0]! as double,
      height: result[1]! as double,
    );
  }

  Size toSize() => Size(width, height);

  /// Returns a new [PreviewSize] with [width] and [height] inverted.
  /// Useful when the preview size is given in portrait mode but the camera
  /// is in landscape mode.
  /// Ex : for tablets, the preview size is given in landscape mode but the device is in portrait mode.
  inverted() => PreviewSize(width: height, height: width);
}

class ExifPreferences {
  ExifPreferences({
    required this.saveGPSLocation,
  });

  bool saveGPSLocation;

  Object encode() {
    return <Object?>[
      saveGPSLocation,
    ];
  }

  static ExifPreferences decode(Object result) {
    result as List<Object?>;
    return ExifPreferences(
      saveGPSLocation: result[0]! as bool,
    );
  }
}

class PigeonSensor {
  PigeonSensor({
    required this.position,
    required this.type,
    this.deviceId,
  });

  PigeonSensorPosition position;

  PigeonSensorType type;

  String? deviceId;

  Object encode() {
    return <Object?>[
      position.index,
      type.index,
      deviceId,
    ];
  }

  static PigeonSensor decode(Object result) {
    result as List<Object?>;
    return PigeonSensor(
      position: PigeonSensorPosition.values[result[0]! as int],
      type: PigeonSensorType.values[result[1]! as int],
      deviceId: result[2] as String?,
    );
  }
}

/// Video recording options. Some of them are specific to each platform.
class VideoOptions {
  VideoOptions({
    required this.enableAudio,
    this.quality,
    this.android,
    this.ios,
  });

  /// Enable audio while video recording
  bool enableAudio;

  /// The quality of the video recording, defaults to [VideoRecordingQuality.highest].
  VideoRecordingQuality? quality;

  AndroidVideoOptions? android;

  CupertinoVideoOptions? ios;

  Object encode() {
    return <Object?>[
      enableAudio,
      quality?.index,
      android?.encode(),
      ios?.encode(),
    ];
  }

  static VideoOptions decode(Object result) {
    result as List<Object?>;
    return VideoOptions(
      enableAudio: result[0]! as bool,
      quality: result[1] != null
          ? VideoRecordingQuality.values[result[1]! as int]
          : null,
      android: result[2] != null
          ? AndroidVideoOptions.decode(result[2]! as List<Object?>)
          : null,
      ios: result[3] != null
          ? CupertinoVideoOptions.decode(result[3]! as List<Object?>)
          : null,
    );
  }
}

class AndroidVideoOptions {
  AndroidVideoOptions({
    this.bitrate,
    this.fallbackStrategy,
  });

  /// The bitrate of the video recording. Only set it if a custom bitrate is
  /// desired.
  int? bitrate;

  QualityFallbackStrategy? fallbackStrategy;

  Object encode() {
    return <Object?>[
      bitrate,
      fallbackStrategy?.index,
    ];
  }

  static AndroidVideoOptions decode(Object result) {
    result as List<Object?>;
    return AndroidVideoOptions(
      bitrate: result[0] as int?,
      fallbackStrategy: result[1] != null
          ? QualityFallbackStrategy.values[result[1]! as int]
          : null,
    );
  }
}

class CupertinoVideoOptions {
  CupertinoVideoOptions({
    this.fileType,
    this.codec,
    this.fps,
  });

  /// Specify video file type, defaults to [AVFileTypeQuickTimeMovie].
  CupertinoFileType? fileType;

  /// Specify video codec, defaults to [AVVideoCodecTypeH264].
  CupertinoCodecType? codec;

  /// Specify video fps, defaults to [30].
  int? fps;

  Object encode() {
    return <Object?>[
      fileType?.index,
      codec?.index,
      fps,
    ];
  }

  static CupertinoVideoOptions decode(Object result) {
    result as List<Object?>;
    return CupertinoVideoOptions(
      fileType: result[0] != null
          ? CupertinoFileType.values[result[0]! as int]
          : null,
      codec: result[1] != null
          ? CupertinoCodecType.values[result[1]! as int]
          : null,
      fps: result[2] as int?,
    );
  }
}

class PigeonSensorTypeDevice {
  PigeonSensorTypeDevice({
    required this.sensorType,
    required this.name,
    required this.iso,
    required this.flashAvailable,
    required this.uid,
  });

  PigeonSensorType sensorType;

  /// A localized device name for display in the user interface.
  String name;

  /// The current exposure ISO value.
  double iso;

  /// A Boolean value that indicates whether the flash is currently available for use.
  bool flashAvailable;

  /// An identifier that uniquely identifies the device.
  String uid;

  Object encode() {
    return <Object?>[
      sensorType.index,
      name,
      iso,
      flashAvailable,
      uid,
    ];
  }

  static PigeonSensorTypeDevice decode(Object result) {
    result as List<Object?>;
    return PigeonSensorTypeDevice(
      sensorType: PigeonSensorType.values[result[0]! as int],
      name: result[1]! as String,
      iso: result[2]! as double,
      flashAvailable: result[3]! as bool,
      uid: result[4]! as String,
    );
  }
}

class AndroidFocusSettings {
  AndroidFocusSettings({
    required this.autoCancelDurationInMillis,
  });

  /// The auto focus will be canceled after the given [autoCancelDurationInMillis].
  /// If [autoCancelDurationInMillis] is equals to 0 (or less), the auto focus
  /// will **not** be canceled. A manual `focusOnPoint` call will be needed to
  /// focus on an other point.
  /// Minimal duration of [autoCancelDurationInMillis] is 1000 ms. If set
  /// between 0 (exclusive) and 1000 (exclusive), it will be raised to 1000.
  int autoCancelDurationInMillis;

  Object encode() {
    return <Object?>[
      autoCancelDurationInMillis,
    ];
  }

  static AndroidFocusSettings decode(Object result) {
    result as List<Object?>;
    return AndroidFocusSettings(
      autoCancelDurationInMillis: result[0]! as int,
    );
  }
}

class PlaneWrapper {
  PlaneWrapper({
    required this.bytes,
    required this.bytesPerRow,
    this.bytesPerPixel,
    this.width,
    this.height,
  });

  Uint8List bytes;

  int bytesPerRow;

  int? bytesPerPixel;

  int? width;

  int? height;

  Object encode() {
    return <Object?>[
      bytes,
      bytesPerRow,
      bytesPerPixel,
      width,
      height,
    ];
  }

  static PlaneWrapper decode(Object result) {
    result as List<Object?>;
    return PlaneWrapper(
      bytes: result[0]! as Uint8List,
      bytesPerRow: result[1]! as int,
      bytesPerPixel: result[2] as int?,
      width: result[3] as int?,
      height: result[4] as int?,
    );
  }
}

class CropRectWrapper {
  CropRectWrapper({
    required this.left,
    required this.top,
    required this.width,
    required this.height,
  });

  int left;

  int top;

  int width;

  int height;

  Object encode() {
    return <Object?>[
      left,
      top,
      width,
      height,
    ];
  }

  static CropRectWrapper decode(Object result) {
    result as List<Object?>;
    return CropRectWrapper(
      left: result[0]! as int,
      top: result[1]! as int,
      width: result[2]! as int,
      height: result[3]! as int,
    );
  }
}

class AnalysisImageWrapper {
  AnalysisImageWrapper({
    required this.format,
    this.bytes,
    required this.width,
    required this.height,
    this.planes,
    this.cropRect,
    this.rotation,
  });

  AnalysisImageFormat format;

  Uint8List? bytes;

  int width;

  int height;

  List<PlaneWrapper?>? planes;

  CropRectWrapper? cropRect;

  AnalysisRotation? rotation;

  Object encode() {
    return <Object?>[
      format.index,
      bytes,
      width,
      height,
      planes,
      cropRect?.encode(),
      rotation?.index,
    ];
  }

  static AnalysisImageWrapper decode(Object result) {
    result as List<Object?>;
    return AnalysisImageWrapper(
      format: AnalysisImageFormat.values[result[0]! as int],
      bytes: result[1] as Uint8List?,
      width: result[2]! as int,
      height: result[3]! as int,
      planes: (result[4] as List<Object?>?)?.cast<PlaneWrapper?>(),
      cropRect: result[5] != null
          ? CropRectWrapper.decode(result[5]! as List<Object?>)
          : null,
      rotation:
          result[6] != null ? AnalysisRotation.values[result[6]! as int] : null,
    );
  }
}

class _AnalysisImageUtilsCodec extends StandardMessageCodec {
  const _AnalysisImageUtilsCodec();

  @override
  void writeValue(WriteBuffer buffer, Object? value) {
    if (value is AnalysisImageWrapper) {
      buffer.putUint8(128);
      writeValue(buffer, value.encode());
    } else if (value is CropRectWrapper) {
      buffer.putUint8(129);
      writeValue(buffer, value.encode());
    } else if (value is PlaneWrapper) {
      buffer.putUint8(130);
      writeValue(buffer, value.encode());
    } else {
      super.writeValue(buffer, value);
    }
  }

  @override
  Object? readValueOfType(int type, ReadBuffer buffer) {
    switch (type) {
      case 128:
        return AnalysisImageWrapper.decode(readValue(buffer)!);
      case 129:
        return CropRectWrapper.decode(readValue(buffer)!);
      case 130:
        return PlaneWrapper.decode(readValue(buffer)!);
      default:
        return super.readValueOfType(type, buffer);
    }
  }
}

class AnalysisImageUtils {
  /// Constructor for [AnalysisImageUtils].  The [binaryMessenger] named argument is
  /// available for dependency injection.  If it is left null, the default
  /// BinaryMessenger will be used which routes to the host platform.
  AnalysisImageUtils({BinaryMessenger? binaryMessenger})
      : _binaryMessenger = binaryMessenger;
  final BinaryMessenger? _binaryMessenger;

  static const MessageCodec<Object?> codec = _AnalysisImageUtilsCodec();

  Future<AnalysisImageWrapper> nv21toJpeg(
      AnalysisImageWrapper arg_nv21Image, int arg_jpegQuality) async {
    final BasicMessageChannel<Object?> channel = BasicMessageChannel<Object?>(
        'dev.flutter.pigeon.AnalysisImageUtils.nv21toJpeg', codec,
        binaryMessenger: _binaryMessenger);
    final List<Object?>? replyList = await channel
        .send(<Object?>[arg_nv21Image, arg_jpegQuality]) as List<Object?>?;
    if (replyList == null) {
      throw PlatformException(
        code: 'channel-error',
        message: 'Unable to establish connection on channel.',
      );
    } else if (replyList.length > 1) {
      throw PlatformException(
        code: replyList[0]! as String,
        message: replyList[1] as String?,
        details: replyList[2],
      );
    } else if (replyList[0] == null) {
      throw PlatformException(
        code: 'null-error',
        message: 'Host platform returned null value for non-null return value.',
      );
    } else {
      return (replyList[0] as AnalysisImageWrapper?)!;
    }
  }

  Future<AnalysisImageWrapper> yuv420toJpeg(
      AnalysisImageWrapper arg_yuvImage, int arg_jpegQuality) async {
    final BasicMessageChannel<Object?> channel = BasicMessageChannel<Object?>(
        'dev.flutter.pigeon.AnalysisImageUtils.yuv420toJpeg', codec,
        binaryMessenger: _binaryMessenger);
    final List<Object?>? replyList = await channel
        .send(<Object?>[arg_yuvImage, arg_jpegQuality]) as List<Object?>?;
    if (replyList == null) {
      throw PlatformException(
        code: 'channel-error',
        message: 'Unable to establish connection on channel.',
      );
    } else if (replyList.length > 1) {
      throw PlatformException(
        code: replyList[0]! as String,
        message: replyList[1] as String?,
        details: replyList[2],
      );
    } else if (replyList[0] == null) {
      throw PlatformException(
        code: 'null-error',
        message: 'Host platform returned null value for non-null return value.',
      );
    } else {
      return (replyList[0] as AnalysisImageWrapper?)!;
    }
  }

  Future<AnalysisImageWrapper> yuv420toNv21(
      AnalysisImageWrapper arg_yuvImage) async {
    final BasicMessageChannel<Object?> channel = BasicMessageChannel<Object?>(
        'dev.flutter.pigeon.AnalysisImageUtils.yuv420toNv21', codec,
        binaryMessenger: _binaryMessenger);
    final List<Object?>? replyList =
        await channel.send(<Object?>[arg_yuvImage]) as List<Object?>?;
    if (replyList == null) {
      throw PlatformException(
        code: 'channel-error',
        message: 'Unable to establish connection on channel.',
      );
    } else if (replyList.length > 1) {
      throw PlatformException(
        code: replyList[0]! as String,
        message: replyList[1] as String?,
        details: replyList[2],
      );
    } else if (replyList[0] == null) {
      throw PlatformException(
        code: 'null-error',
        message: 'Host platform returned null value for non-null return value.',
      );
    } else {
      return (replyList[0] as AnalysisImageWrapper?)!;
    }
  }

  Future<AnalysisImageWrapper> bgra8888toJpeg(
      AnalysisImageWrapper arg_bgra8888image, int arg_jpegQuality) async {
    final BasicMessageChannel<Object?> channel = BasicMessageChannel<Object?>(
        'dev.flutter.pigeon.AnalysisImageUtils.bgra8888toJpeg', codec,
        binaryMessenger: _binaryMessenger);
    final List<Object?>? replyList = await channel
        .send(<Object?>[arg_bgra8888image, arg_jpegQuality]) as List<Object?>?;
    if (replyList == null) {
      throw PlatformException(
        code: 'channel-error',
        message: 'Unable to establish connection on channel.',
      );
    } else if (replyList.length > 1) {
      throw PlatformException(
        code: replyList[0]! as String,
        message: replyList[1] as String?,
        details: replyList[2],
      );
    } else if (replyList[0] == null) {
      throw PlatformException(
        code: 'null-error',
        message: 'Host platform returned null value for non-null return value.',
      );
    } else {
      return (replyList[0] as AnalysisImageWrapper?)!;
    }
  }
}

class _CameraInterfaceCodec extends StandardMessageCodec {
  const _CameraInterfaceCodec();

  @override
  void writeValue(WriteBuffer buffer, Object? value) {
    if (value is AndroidFocusSettings) {
      buffer.putUint8(128);
      writeValue(buffer, value.encode());
    } else if (value is AndroidVideoOptions) {
      buffer.putUint8(129);
      writeValue(buffer, value.encode());
    } else if (value is CupertinoVideoOptions) {
      buffer.putUint8(130);
      writeValue(buffer, value.encode());
    } else if (value is ExifPreferences) {
      buffer.putUint8(131);
      writeValue(buffer, value.encode());
    } else if (value is PigeonSensor) {
      buffer.putUint8(132);
      writeValue(buffer, value.encode());
    } else if (value is PigeonSensorTypeDevice) {
      buffer.putUint8(133);
      writeValue(buffer, value.encode());
    } else if (value is PreviewSize) {
      buffer.putUint8(134);
      writeValue(buffer, value.encode());
    } else if (value is PreviewSize) {
      buffer.putUint8(135);
      writeValue(buffer, value.encode());
    } else if (value is VideoOptions) {
      buffer.putUint8(136);
      writeValue(buffer, value.encode());
    } else {
      super.writeValue(buffer, value);
    }
  }

  @override
  Object? readValueOfType(int type, ReadBuffer buffer) {
    switch (type) {
      case 128:
        return AndroidFocusSettings.decode(readValue(buffer)!);
      case 129:
        return AndroidVideoOptions.decode(readValue(buffer)!);
      case 130:
        return CupertinoVideoOptions.decode(readValue(buffer)!);
      case 131:
        return ExifPreferences.decode(readValue(buffer)!);
      case 132:
        return PigeonSensor.decode(readValue(buffer)!);
      case 133:
        return PigeonSensorTypeDevice.decode(readValue(buffer)!);
      case 134:
        return PreviewSize.decode(readValue(buffer)!);
      case 135:
        return PreviewSize.decode(readValue(buffer)!);
      case 136:
        return VideoOptions.decode(readValue(buffer)!);
      default:
        return super.readValueOfType(type, buffer);
    }
  }
}

class CameraInterface {
  /// Constructor for [CameraInterface].  The [binaryMessenger] named argument is
  /// available for dependency injection.  If it is left null, the default
  /// BinaryMessenger will be used which routes to the host platform.
  CameraInterface({BinaryMessenger? binaryMessenger})
      : _binaryMessenger = binaryMessenger;
  final BinaryMessenger? _binaryMessenger;

  static const MessageCodec<Object?> codec = _CameraInterfaceCodec();

  Future<bool> setupCamera(
      List<PigeonSensor?> arg_sensors,
      String arg_aspectRatio,
      double arg_zoom,
      bool arg_mirrorFrontCamera,
      bool arg_enablePhysicalButton,
      String arg_flashMode,
      String arg_captureMode,
      bool arg_enableImageStream,
      ExifPreferences arg_exifPreferences,
      VideoOptions? arg_videoOptions) async {
    final BasicMessageChannel<Object?> channel = BasicMessageChannel<Object?>(
        'dev.flutter.pigeon.CameraInterface.setupCamera', codec,
        binaryMessenger: _binaryMessenger);
    final List<Object?>? replyList = await channel.send(<Object?>[
      arg_sensors,
      arg_aspectRatio,
      arg_zoom,
      arg_mirrorFrontCamera,
      arg_enablePhysicalButton,
      arg_flashMode,
      arg_captureMode,
      arg_enableImageStream,
      arg_exifPreferences,
      arg_videoOptions
    ]) as List<Object?>?;
    if (replyList == null) {
      throw PlatformException(
        code: 'channel-error',
        message: 'Unable to establish connection on channel.',
      );
    } else if (replyList.length > 1) {
      throw PlatformException(
        code: replyList[0]! as String,
        message: replyList[1] as String?,
        details: replyList[2],
      );
    } else if (replyList[0] == null) {
      throw PlatformException(
        code: 'null-error',
        message: 'Host platform returned null value for non-null return value.',
      );
    } else {
      return (replyList[0] as bool?)!;
    }
  }

  Future<List<String?>> checkPermissions(List<String?> arg_permissions) async {
    final BasicMessageChannel<Object?> channel = BasicMessageChannel<Object?>(
        'dev.flutter.pigeon.CameraInterface.checkPermissions', codec,
        binaryMessenger: _binaryMessenger);
    final List<Object?>? replyList =
        await channel.send(<Object?>[arg_permissions]) as List<Object?>?;
    if (replyList == null) {
      throw PlatformException(
        code: 'channel-error',
        message: 'Unable to establish connection on channel.',
      );
    } else if (replyList.length > 1) {
      throw PlatformException(
        code: replyList[0]! as String,
        message: replyList[1] as String?,
        details: replyList[2],
      );
    } else if (replyList[0] == null) {
      throw PlatformException(
        code: 'null-error',
        message: 'Host platform returned null value for non-null return value.',
      );
    } else {
      return (replyList[0] as List<Object?>?)!.cast<String?>();
    }
  }

  /// Returns given [CamerAwesomePermission] list (as String). Location permission might be
  /// refused but the app should still be able to run.
  Future<List<String?>> requestPermissions(bool arg_saveGpsLocation) async {
    final BasicMessageChannel<Object?> channel = BasicMessageChannel<Object?>(
        'dev.flutter.pigeon.CameraInterface.requestPermissions', codec,
        binaryMessenger: _binaryMessenger);
    final List<Object?>? replyList =
        await channel.send(<Object?>[arg_saveGpsLocation]) as List<Object?>?;
    if (replyList == null) {
      throw PlatformException(
        code: 'channel-error',
        message: 'Unable to establish connection on channel.',
      );
    } else if (replyList.length > 1) {
      throw PlatformException(
        code: replyList[0]! as String,
        message: replyList[1] as String?,
        details: replyList[2],
      );
    } else if (replyList[0] == null) {
      throw PlatformException(
        code: 'null-error',
        message: 'Host platform returned null value for non-null return value.',
      );
    } else {
      return (replyList[0] as List<Object?>?)!.cast<String?>();
    }
  }

  Future<int> getPreviewTextureId(int arg_cameraPosition) async {
    final BasicMessageChannel<Object?> channel = BasicMessageChannel<Object?>(
        'dev.flutter.pigeon.CameraInterface.getPreviewTextureId', codec,
        binaryMessenger: _binaryMessenger);
    final List<Object?>? replyList =
        await channel.send(<Object?>[arg_cameraPosition]) as List<Object?>?;
    if (replyList == null) {
      throw PlatformException(
        code: 'channel-error',
        message: 'Unable to establish connection on channel.',
      );
    } else if (replyList.length > 1) {
      throw PlatformException(
        code: replyList[0]! as String,
        message: replyList[1] as String?,
        details: replyList[2],
      );
    } else if (replyList[0] == null) {
      throw PlatformException(
        code: 'null-error',
        message: 'Host platform returned null value for non-null return value.',
      );
    } else {
      return (replyList[0] as int?)!;
    }
  }

  Future<bool> takePhoto(
      List<PigeonSensor?> arg_sensors, List<String?> arg_paths) async {
    final BasicMessageChannel<Object?> channel = BasicMessageChannel<Object?>(
        'dev.flutter.pigeon.CameraInterface.takePhoto', codec,
        binaryMessenger: _binaryMessenger);
    final List<Object?>? replyList =
        await channel.send(<Object?>[arg_sensors, arg_paths]) as List<Object?>?;
    if (replyList == null) {
      throw PlatformException(
        code: 'channel-error',
        message: 'Unable to establish connection on channel.',
      );
    } else if (replyList.length > 1) {
      throw PlatformException(
        code: replyList[0]! as String,
        message: replyList[1] as String?,
        details: replyList[2],
      );
    } else if (replyList[0] == null) {
      throw PlatformException(
        code: 'null-error',
        message: 'Host platform returned null value for non-null return value.',
      );
    } else {
      return (replyList[0] as bool?)!;
    }
  }

  Future<void> recordVideo(
      List<PigeonSensor?> arg_sensors, List<String?> arg_paths) async {
    final BasicMessageChannel<Object?> channel = BasicMessageChannel<Object?>(
        'dev.flutter.pigeon.CameraInterface.recordVideo', codec,
        binaryMessenger: _binaryMessenger);
    final List<Object?>? replyList =
        await channel.send(<Object?>[arg_sensors, arg_paths]) as List<Object?>?;
    if (replyList == null) {
      throw PlatformException(
        code: 'channel-error',
        message: 'Unable to establish connection on channel.',
      );
    } else if (replyList.length > 1) {
      throw PlatformException(
        code: replyList[0]! as String,
        message: replyList[1] as String?,
        details: replyList[2],
      );
    } else {
      return;
    }
  }

  Future<void> pauseVideoRecording() async {
    final BasicMessageChannel<Object?> channel = BasicMessageChannel<Object?>(
        'dev.flutter.pigeon.CameraInterface.pauseVideoRecording', codec,
        binaryMessenger: _binaryMessenger);
    final List<Object?>? replyList = await channel.send(null) as List<Object?>?;
    if (replyList == null) {
      throw PlatformException(
        code: 'channel-error',
        message: 'Unable to establish connection on channel.',
      );
    } else if (replyList.length > 1) {
      throw PlatformException(
        code: replyList[0]! as String,
        message: replyList[1] as String?,
        details: replyList[2],
      );
    } else {
      return;
    }
  }

  Future<void> resumeVideoRecording() async {
    final BasicMessageChannel<Object?> channel = BasicMessageChannel<Object?>(
        'dev.flutter.pigeon.CameraInterface.resumeVideoRecording', codec,
        binaryMessenger: _binaryMessenger);
    final List<Object?>? replyList = await channel.send(null) as List<Object?>?;
    if (replyList == null) {
      throw PlatformException(
        code: 'channel-error',
        message: 'Unable to establish connection on channel.',
      );
    } else if (replyList.length > 1) {
      throw PlatformException(
        code: replyList[0]! as String,
        message: replyList[1] as String?,
        details: replyList[2],
      );
    } else {
      return;
    }
  }

  Future<void> receivedImageFromStream() async {
    final BasicMessageChannel<Object?> channel = BasicMessageChannel<Object?>(
        'dev.flutter.pigeon.CameraInterface.receivedImageFromStream', codec,
        binaryMessenger: _binaryMessenger);
    final List<Object?>? replyList = await channel.send(null) as List<Object?>?;
    if (replyList == null) {
      throw PlatformException(
        code: 'channel-error',
        message: 'Unable to establish connection on channel.',
      );
    } else if (replyList.length > 1) {
      throw PlatformException(
        code: replyList[0]! as String,
        message: replyList[1] as String?,
        details: replyList[2],
      );
    } else {
      return;
    }
  }

  Future<bool> stopRecordingVideo() async {
    final BasicMessageChannel<Object?> channel = BasicMessageChannel<Object?>(
        'dev.flutter.pigeon.CameraInterface.stopRecordingVideo', codec,
        binaryMessenger: _binaryMessenger);
    final List<Object?>? replyList = await channel.send(null) as List<Object?>?;
    if (replyList == null) {
      throw PlatformException(
        code: 'channel-error',
        message: 'Unable to establish connection on channel.',
      );
    } else if (replyList.length > 1) {
      throw PlatformException(
        code: replyList[0]! as String,
        message: replyList[1] as String?,
        details: replyList[2],
      );
    } else if (replyList[0] == null) {
      throw PlatformException(
        code: 'null-error',
        message: 'Host platform returned null value for non-null return value.',
      );
    } else {
      return (replyList[0] as bool?)!;
    }
  }

  Future<List<PigeonSensorTypeDevice?>> getFrontSensors() async {
    final BasicMessageChannel<Object?> channel = BasicMessageChannel<Object?>(
        'dev.flutter.pigeon.CameraInterface.getFrontSensors', codec,
        binaryMessenger: _binaryMessenger);
    final List<Object?>? replyList = await channel.send(null) as List<Object?>?;
    if (replyList == null) {
      throw PlatformException(
        code: 'channel-error',
        message: 'Unable to establish connection on channel.',
      );
    } else if (replyList.length > 1) {
      throw PlatformException(
        code: replyList[0]! as String,
        message: replyList[1] as String?,
        details: replyList[2],
      );
    } else if (replyList[0] == null) {
      throw PlatformException(
        code: 'null-error',
        message: 'Host platform returned null value for non-null return value.',
      );
    } else {
      return (replyList[0] as List<Object?>?)!.cast<PigeonSensorTypeDevice?>();
    }
  }

  Future<List<PigeonSensorTypeDevice?>> getBackSensors() async {
    final BasicMessageChannel<Object?> channel = BasicMessageChannel<Object?>(
        'dev.flutter.pigeon.CameraInterface.getBackSensors', codec,
        binaryMessenger: _binaryMessenger);
    final List<Object?>? replyList = await channel.send(null) as List<Object?>?;
    if (replyList == null) {
      throw PlatformException(
        code: 'channel-error',
        message: 'Unable to establish connection on channel.',
      );
    } else if (replyList.length > 1) {
      throw PlatformException(
        code: replyList[0]! as String,
        message: replyList[1] as String?,
        details: replyList[2],
      );
    } else if (replyList[0] == null) {
      throw PlatformException(
        code: 'null-error',
        message: 'Host platform returned null value for non-null return value.',
      );
    } else {
      return (replyList[0] as List<Object?>?)!.cast<PigeonSensorTypeDevice?>();
    }
  }

  Future<bool> start() async {
    final BasicMessageChannel<Object?> channel = BasicMessageChannel<Object?>(
        'dev.flutter.pigeon.CameraInterface.start', codec,
        binaryMessenger: _binaryMessenger);
    final List<Object?>? replyList = await channel.send(null) as List<Object?>?;
    if (replyList == null) {
      throw PlatformException(
        code: 'channel-error',
        message: 'Unable to establish connection on channel.',
      );
    } else if (replyList.length > 1) {
      throw PlatformException(
        code: replyList[0]! as String,
        message: replyList[1] as String?,
        details: replyList[2],
      );
    } else if (replyList[0] == null) {
      throw PlatformException(
        code: 'null-error',
        message: 'Host platform returned null value for non-null return value.',
      );
    } else {
      return (replyList[0] as bool?)!;
    }
  }

  Future<bool> stop() async {
    final BasicMessageChannel<Object?> channel = BasicMessageChannel<Object?>(
        'dev.flutter.pigeon.CameraInterface.stop', codec,
        binaryMessenger: _binaryMessenger);
    final List<Object?>? replyList = await channel.send(null) as List<Object?>?;
    if (replyList == null) {
      throw PlatformException(
        code: 'channel-error',
        message: 'Unable to establish connection on channel.',
      );
    } else if (replyList.length > 1) {
      throw PlatformException(
        code: replyList[0]! as String,
        message: replyList[1] as String?,
        details: replyList[2],
      );
    } else if (replyList[0] == null) {
      throw PlatformException(
        code: 'null-error',
        message: 'Host platform returned null value for non-null return value.',
      );
    } else {
      return (replyList[0] as bool?)!;
    }
  }

  Future<void> setFlashMode(String arg_mode) async {
    final BasicMessageChannel<Object?> channel = BasicMessageChannel<Object?>(
        'dev.flutter.pigeon.CameraInterface.setFlashMode', codec,
        binaryMessenger: _binaryMessenger);
    final List<Object?>? replyList =
        await channel.send(<Object?>[arg_mode]) as List<Object?>?;
    if (replyList == null) {
      throw PlatformException(
        code: 'channel-error',
        message: 'Unable to establish connection on channel.',
      );
    } else if (replyList.length > 1) {
      throw PlatformException(
        code: replyList[0]! as String,
        message: replyList[1] as String?,
        details: replyList[2],
      );
    } else {
      return;
    }
  }

  Future<void> handleAutoFocus() async {
    final BasicMessageChannel<Object?> channel = BasicMessageChannel<Object?>(
        'dev.flutter.pigeon.CameraInterface.handleAutoFocus', codec,
        binaryMessenger: _binaryMessenger);
    final List<Object?>? replyList = await channel.send(null) as List<Object?>?;
    if (replyList == null) {
      throw PlatformException(
        code: 'channel-error',
        message: 'Unable to establish connection on channel.',
      );
    } else if (replyList.length > 1) {
      throw PlatformException(
        code: replyList[0]! as String,
        message: replyList[1] as String?,
        details: replyList[2],
      );
    } else {
      return;
    }
  }

  /// Starts auto focus on a point at ([x], [y]).
  ///
  /// On Android, you can control after how much time you want to switch back
  /// to passive focus mode with [androidFocusSettings].
  Future<void> focusOnPoint(PreviewSize arg_previewSize, double arg_x,
      double arg_y, AndroidFocusSettings? arg_androidFocusSettings) async {
    final BasicMessageChannel<Object?> channel = BasicMessageChannel<Object?>(
        'dev.flutter.pigeon.CameraInterface.focusOnPoint', codec,
        binaryMessenger: _binaryMessenger);
    final List<Object?>? replyList = await channel.send(
            <Object?>[arg_previewSize, arg_x, arg_y, arg_androidFocusSettings])
        as List<Object?>?;
    if (replyList == null) {
      throw PlatformException(
        code: 'channel-error',
        message: 'Unable to establish connection on channel.',
      );
    } else if (replyList.length > 1) {
      throw PlatformException(
        code: replyList[0]! as String,
        message: replyList[1] as String?,
        details: replyList[2],
      );
    } else {
      return;
    }
  }

  Future<void> setZoom(double arg_zoom) async {
    final BasicMessageChannel<Object?> channel = BasicMessageChannel<Object?>(
        'dev.flutter.pigeon.CameraInterface.setZoom', codec,
        binaryMessenger: _binaryMessenger);
    final List<Object?>? replyList =
        await channel.send(<Object?>[arg_zoom]) as List<Object?>?;
    if (replyList == null) {
      throw PlatformException(
        code: 'channel-error',
        message: 'Unable to establish connection on channel.',
      );
    } else if (replyList.length > 1) {
      throw PlatformException(
        code: replyList[0]! as String,
        message: replyList[1] as String?,
        details: replyList[2],
      );
    } else {
      return;
    }
  }

  Future<void> setMirrorFrontCamera(bool arg_mirror) async {
    final BasicMessageChannel<Object?> channel = BasicMessageChannel<Object?>(
        'dev.flutter.pigeon.CameraInterface.setMirrorFrontCamera', codec,
        binaryMessenger: _binaryMessenger);
    final List<Object?>? replyList =
        await channel.send(<Object?>[arg_mirror]) as List<Object?>?;
    if (replyList == null) {
      throw PlatformException(
        code: 'channel-error',
        message: 'Unable to establish connection on channel.',
      );
    } else if (replyList.length > 1) {
      throw PlatformException(
        code: replyList[0]! as String,
        message: replyList[1] as String?,
        details: replyList[2],
      );
    } else {
      return;
    }
  }

  Future<void> setSensor(List<PigeonSensor?> arg_sensors) async {
    final BasicMessageChannel<Object?> channel = BasicMessageChannel<Object?>(
        'dev.flutter.pigeon.CameraInterface.setSensor', codec,
        binaryMessenger: _binaryMessenger);
    final List<Object?>? replyList =
        await channel.send(<Object?>[arg_sensors]) as List<Object?>?;
    if (replyList == null) {
      throw PlatformException(
        code: 'channel-error',
        message: 'Unable to establish connection on channel.',
      );
    } else if (replyList.length > 1) {
      throw PlatformException(
        code: replyList[0]! as String,
        message: replyList[1] as String?,
        details: replyList[2],
      );
    } else {
      return;
    }
  }

  Future<void> setCorrection(double arg_brightness) async {
    final BasicMessageChannel<Object?> channel = BasicMessageChannel<Object?>(
        'dev.flutter.pigeon.CameraInterface.setCorrection', codec,
        binaryMessenger: _binaryMessenger);
    final List<Object?>? replyList =
        await channel.send(<Object?>[arg_brightness]) as List<Object?>?;
    if (replyList == null) {
      throw PlatformException(
        code: 'channel-error',
        message: 'Unable to establish connection on channel.',
      );
    } else if (replyList.length > 1) {
      throw PlatformException(
        code: replyList[0]! as String,
        message: replyList[1] as String?,
        details: replyList[2],
      );
    } else {
      return;
    }
  }

  Future<double> getMinZoom() async {
    final BasicMessageChannel<Object?> channel = BasicMessageChannel<Object?>(
        'dev.flutter.pigeon.CameraInterface.getMinZoom', codec,
        binaryMessenger: _binaryMessenger);
    final List<Object?>? replyList = await channel.send(null) as List<Object?>?;
    if (replyList == null) {
      throw PlatformException(
        code: 'channel-error',
        message: 'Unable to establish connection on channel.',
      );
    } else if (replyList.length > 1) {
      throw PlatformException(
        code: replyList[0]! as String,
        message: replyList[1] as String?,
        details: replyList[2],
      );
    } else if (replyList[0] == null) {
      throw PlatformException(
        code: 'null-error',
        message: 'Host platform returned null value for non-null return value.',
      );
    } else {
      return (replyList[0] as double?)!;
    }
  }

  Future<double> getMaxZoom() async {
    final BasicMessageChannel<Object?> channel = BasicMessageChannel<Object?>(
        'dev.flutter.pigeon.CameraInterface.getMaxZoom', codec,
        binaryMessenger: _binaryMessenger);
    final List<Object?>? replyList = await channel.send(null) as List<Object?>?;
    if (replyList == null) {
      throw PlatformException(
        code: 'channel-error',
        message: 'Unable to establish connection on channel.',
      );
    } else if (replyList.length > 1) {
      throw PlatformException(
        code: replyList[0]! as String,
        message: replyList[1] as String?,
        details: replyList[2],
      );
    } else if (replyList[0] == null) {
      throw PlatformException(
        code: 'null-error',
        message: 'Host platform returned null value for non-null return value.',
      );
    } else {
      return (replyList[0] as double?)!;
    }
  }

  Future<void> setCaptureMode(String arg_mode) async {
    final BasicMessageChannel<Object?> channel = BasicMessageChannel<Object?>(
        'dev.flutter.pigeon.CameraInterface.setCaptureMode', codec,
        binaryMessenger: _binaryMessenger);
    final List<Object?>? replyList =
        await channel.send(<Object?>[arg_mode]) as List<Object?>?;
    if (replyList == null) {
      throw PlatformException(
        code: 'channel-error',
        message: 'Unable to establish connection on channel.',
      );
    } else if (replyList.length > 1) {
      throw PlatformException(
        code: replyList[0]! as String,
        message: replyList[1] as String?,
        details: replyList[2],
      );
    } else {
      return;
    }
  }

  Future<bool> setRecordingAudioMode(bool arg_enableAudio) async {
    final BasicMessageChannel<Object?> channel = BasicMessageChannel<Object?>(
        'dev.flutter.pigeon.CameraInterface.setRecordingAudioMode', codec,
        binaryMessenger: _binaryMessenger);
    final List<Object?>? replyList =
        await channel.send(<Object?>[arg_enableAudio]) as List<Object?>?;
    if (replyList == null) {
      throw PlatformException(
        code: 'channel-error',
        message: 'Unable to establish connection on channel.',
      );
    } else if (replyList.length > 1) {
      throw PlatformException(
        code: replyList[0]! as String,
        message: replyList[1] as String?,
        details: replyList[2],
      );
    } else if (replyList[0] == null) {
      throw PlatformException(
        code: 'null-error',
        message: 'Host platform returned null value for non-null return value.',
      );
    } else {
      return (replyList[0] as bool?)!;
    }
  }

  Future<List<PreviewSize?>> availableSizes() async {
    final BasicMessageChannel<Object?> channel = BasicMessageChannel<Object?>(
        'dev.flutter.pigeon.CameraInterface.availableSizes', codec,
        binaryMessenger: _binaryMessenger);
    final List<Object?>? replyList = await channel.send(null) as List<Object?>?;
    if (replyList == null) {
      throw PlatformException(
        code: 'channel-error',
        message: 'Unable to establish connection on channel.',
      );
    } else if (replyList.length > 1) {
      throw PlatformException(
        code: replyList[0]! as String,
        message: replyList[1] as String?,
        details: replyList[2],
      );
    } else if (replyList[0] == null) {
      throw PlatformException(
        code: 'null-error',
        message: 'Host platform returned null value for non-null return value.',
      );
    } else {
      return (replyList[0] as List<Object?>?)!.cast<PreviewSize?>();
    }
  }

  Future<void> refresh() async {
    final BasicMessageChannel<Object?> channel = BasicMessageChannel<Object?>(
        'dev.flutter.pigeon.CameraInterface.refresh', codec,
        binaryMessenger: _binaryMessenger);
    final List<Object?>? replyList = await channel.send(null) as List<Object?>?;
    if (replyList == null) {
      throw PlatformException(
        code: 'channel-error',
        message: 'Unable to establish connection on channel.',
      );
    } else if (replyList.length > 1) {
      throw PlatformException(
        code: replyList[0]! as String,
        message: replyList[1] as String?,
        details: replyList[2],
      );
    } else {
      return;
    }
  }

  Future<PreviewSize?> getEffectivPreviewSize(int arg_index) async {
    final BasicMessageChannel<Object?> channel = BasicMessageChannel<Object?>(
        'dev.flutter.pigeon.CameraInterface.getEffectivPreviewSize', codec,
        binaryMessenger: _binaryMessenger);
    final List<Object?>? replyList =
        await channel.send(<Object?>[arg_index]) as List<Object?>?;
    if (replyList == null) {
      throw PlatformException(
        code: 'channel-error',
        message: 'Unable to establish connection on channel.',
      );
    } else if (replyList.length > 1) {
      throw PlatformException(
        code: replyList[0]! as String,
        message: replyList[1] as String?,
        details: replyList[2],
      );
    } else {
      return (replyList[0] as PreviewSize?);
    }
  }

  Future<void> setPhotoSize(PreviewSize arg_size) async {
    final BasicMessageChannel<Object?> channel = BasicMessageChannel<Object?>(
        'dev.flutter.pigeon.CameraInterface.setPhotoSize', codec,
        binaryMessenger: _binaryMessenger);
    final List<Object?>? replyList =
        await channel.send(<Object?>[arg_size]) as List<Object?>?;
    if (replyList == null) {
      throw PlatformException(
        code: 'channel-error',
        message: 'Unable to establish connection on channel.',
      );
    } else if (replyList.length > 1) {
      throw PlatformException(
        code: replyList[0]! as String,
        message: replyList[1] as String?,
        details: replyList[2],
      );
    } else {
      return;
    }
  }

  Future<void> setPreviewSize(PreviewSize arg_size) async {
    final BasicMessageChannel<Object?> channel = BasicMessageChannel<Object?>(
        'dev.flutter.pigeon.CameraInterface.setPreviewSize', codec,
        binaryMessenger: _binaryMessenger);
    final List<Object?>? replyList =
        await channel.send(<Object?>[arg_size]) as List<Object?>?;
    if (replyList == null) {
      throw PlatformException(
        code: 'channel-error',
        message: 'Unable to establish connection on channel.',
      );
    } else if (replyList.length > 1) {
      throw PlatformException(
        code: replyList[0]! as String,
        message: replyList[1] as String?,
        details: replyList[2],
      );
    } else {
      return;
    }
  }

  Future<void> setAspectRatio(String arg_aspectRatio) async {
    final BasicMessageChannel<Object?> channel = BasicMessageChannel<Object?>(
        'dev.flutter.pigeon.CameraInterface.setAspectRatio', codec,
        binaryMessenger: _binaryMessenger);
    final List<Object?>? replyList =
        await channel.send(<Object?>[arg_aspectRatio]) as List<Object?>?;
    if (replyList == null) {
      throw PlatformException(
        code: 'channel-error',
        message: 'Unable to establish connection on channel.',
      );
    } else if (replyList.length > 1) {
      throw PlatformException(
        code: replyList[0]! as String,
        message: replyList[1] as String?,
        details: replyList[2],
      );
    } else {
      return;
    }
  }

  Future<void> setupImageAnalysisStream(String arg_format, int arg_width,
      double? arg_maxFramesPerSecond, bool arg_autoStart) async {
    final BasicMessageChannel<Object?> channel = BasicMessageChannel<Object?>(
        'dev.flutter.pigeon.CameraInterface.setupImageAnalysisStream', codec,
        binaryMessenger: _binaryMessenger);
    final List<Object?>? replyList = await channel.send(<Object?>[
      arg_format,
      arg_width,
      arg_maxFramesPerSecond,
      arg_autoStart
    ]) as List<Object?>?;
    if (replyList == null) {
      throw PlatformException(
        code: 'channel-error',
        message: 'Unable to establish connection on channel.',
      );
    } else if (replyList.length > 1) {
      throw PlatformException(
        code: replyList[0]! as String,
        message: replyList[1] as String?,
        details: replyList[2],
      );
    } else {
      return;
    }
  }

  Future<bool> setExifPreferences(ExifPreferences arg_exifPreferences) async {
    final BasicMessageChannel<Object?> channel = BasicMessageChannel<Object?>(
        'dev.flutter.pigeon.CameraInterface.setExifPreferences', codec,
        binaryMessenger: _binaryMessenger);
    final List<Object?>? replyList =
        await channel.send(<Object?>[arg_exifPreferences]) as List<Object?>?;
    if (replyList == null) {
      throw PlatformException(
        code: 'channel-error',
        message: 'Unable to establish connection on channel.',
      );
    } else if (replyList.length > 1) {
      throw PlatformException(
        code: replyList[0]! as String,
        message: replyList[1] as String?,
        details: replyList[2],
      );
    } else if (replyList[0] == null) {
      throw PlatformException(
        code: 'null-error',
        message: 'Host platform returned null value for non-null return value.',
      );
    } else {
      return (replyList[0] as bool?)!;
    }
  }

  Future<void> startAnalysis() async {
    final BasicMessageChannel<Object?> channel = BasicMessageChannel<Object?>(
        'dev.flutter.pigeon.CameraInterface.startAnalysis', codec,
        binaryMessenger: _binaryMessenger);
    final List<Object?>? replyList = await channel.send(null) as List<Object?>?;
    if (replyList == null) {
      throw PlatformException(
        code: 'channel-error',
        message: 'Unable to establish connection on channel.',
      );
    } else if (replyList.length > 1) {
      throw PlatformException(
        code: replyList[0]! as String,
        message: replyList[1] as String?,
        details: replyList[2],
      );
    } else {
      return;
    }
  }

  Future<void> stopAnalysis() async {
    final BasicMessageChannel<Object?> channel = BasicMessageChannel<Object?>(
        'dev.flutter.pigeon.CameraInterface.stopAnalysis', codec,
        binaryMessenger: _binaryMessenger);
    final List<Object?>? replyList = await channel.send(null) as List<Object?>?;
    if (replyList == null) {
      throw PlatformException(
        code: 'channel-error',
        message: 'Unable to establish connection on channel.',
      );
    } else if (replyList.length > 1) {
      throw PlatformException(
        code: replyList[0]! as String,
        message: replyList[1] as String?,
        details: replyList[2],
      );
    } else {
      return;
    }
  }

  Future<void> setFilter(List<double?> arg_matrix) async {
    final BasicMessageChannel<Object?> channel = BasicMessageChannel<Object?>(
        'dev.flutter.pigeon.CameraInterface.setFilter', codec,
        binaryMessenger: _binaryMessenger);
    final List<Object?>? replyList =
        await channel.send(<Object?>[arg_matrix]) as List<Object?>?;
    if (replyList == null) {
      throw PlatformException(
        code: 'channel-error',
        message: 'Unable to establish connection on channel.',
      );
    } else if (replyList.length > 1) {
      throw PlatformException(
        code: replyList[0]! as String,
        message: replyList[1] as String?,
        details: replyList[2],
      );
    } else {
      return;
    }
  }

  Future<bool> isVideoRecordingAndImageAnalysisSupported(
      PigeonSensorPosition arg_sensor) async {
    final BasicMessageChannel<Object?> channel = BasicMessageChannel<Object?>(
        'dev.flutter.pigeon.CameraInterface.isVideoRecordingAndImageAnalysisSupported',
        codec,
        binaryMessenger: _binaryMessenger);
    final List<Object?>? replyList =
        await channel.send(<Object?>[arg_sensor.index]) as List<Object?>?;
    if (replyList == null) {
      throw PlatformException(
        code: 'channel-error',
        message: 'Unable to establish connection on channel.',
      );
    } else if (replyList.length > 1) {
      throw PlatformException(
        code: replyList[0]! as String,
        message: replyList[1] as String?,
        details: replyList[2],
      );
    } else if (replyList[0] == null) {
      throw PlatformException(
        code: 'null-error',
        message: 'Host platform returned null value for non-null return value.',
      );
    } else {
      return (replyList[0] as bool?)!;
    }
  }

  Future<bool> isMultiCamSupported() async {
    final BasicMessageChannel<Object?> channel = BasicMessageChannel<Object?>(
        'dev.flutter.pigeon.CameraInterface.isMultiCamSupported', codec,
        binaryMessenger: _binaryMessenger);
    final List<Object?>? replyList = await channel.send(null) as List<Object?>?;
    if (replyList == null) {
      throw PlatformException(
        code: 'channel-error',
        message: 'Unable to establish connection on channel.',
      );
    } else if (replyList.length > 1) {
      throw PlatformException(
        code: replyList[0]! as String,
        message: replyList[1] as String?,
        details: replyList[2],
      );
    } else if (replyList[0] == null) {
      throw PlatformException(
        code: 'null-error',
        message: 'Host platform returned null value for non-null return value.',
      );
    } else {
      return (replyList[0] as bool?)!;
    }
  }
}



================================================
FILE: lib/src/logger.dart
================================================
import 'package:camerawesome/camerawesome_plugin.dart';
import 'package:flutter/foundation.dart';

/// Print logs if [CamerawesomePlugins.printLogs] is true, otherwise stays quiet
printLog(String text) {
  // TODO Add Log levels (verbose/warning/error?) + native logs printing config?
  if (CamerawesomePlugin.printLogs) {
    debugPrint(text);
  }
}



================================================
FILE: lib/src/camera_characteristics/camera_characteristics.dart
================================================
import 'package:camerawesome/pigeon.dart';
import 'package:camerawesome/src/orchestrator/models/sensors.dart';

class CameraCharacteristics {
  const CameraCharacteristics._();

  static Future<bool> isVideoRecordingAndImageAnalysisSupported(
    SensorPosition sensor,
  ) {
    return CameraInterface().isVideoRecordingAndImageAnalysisSupported(
        PigeonSensorPosition.values.byName(sensor.name));
  }
}



================================================
FILE: lib/src/orchestrator/camera_context.dart
================================================
// ignore_for_file: close_sinks

import 'dart:async';
import 'dart:io';
import 'dart:ui';

import 'package:camerawesome/camerawesome_plugin.dart';
import 'package:camerawesome/pigeon.dart';
import 'package:rxdart/rxdart.dart';

/// This class handle the current state of the camera
/// - [PhotoCameraState]
/// - [VideoCameraState]
class CameraContext {
  /// Listen current state from child widgets
  late final BehaviorSubject<CameraState> stateController;

  late final BehaviorSubject<AwesomeFilter> filterController;

  late final BehaviorSubject<bool> filterSelectorOpened;

  /// on media capturing stream controller
  late final BehaviorSubject<MediaCapture?> mediaCaptureController;

  BehaviorSubject<SensorConfig> sensorConfigController;

  /// implement this to have a callback after CameraAwesome asked for permissions
  final OnPermissionsResult? onPermissionsResult;

  final CaptureMode initialCaptureMode;

  /// Configuration holding path builders for taking pictures and recording
  /// videos. May be null if in [CaptureMode.analysisOnly] or [CaptureMode.preview].
  final SaveConfig? saveConfig;

  final bool enablePhysicalButton;

  /// allows to create dynamic analysis using the current preview
  /// Image analysis controller. You may use it to start or stop image analysis.
  final AnalysisController? analysisController;

  /// List of available filters
  final List<AwesomeFilter>? availableFilters;

  /// Preferences concerning Exif (photos metadata)
  ExifPreferences exifPreferences;

  Stream<AwesomeFilter> get filter$ => filterController.stream;

  Stream<bool> get filterSelectorOpened$ => filterSelectorOpened.stream;

  Stream<CameraState> get state$ => stateController.stream;

  Stream<MediaCapture?> get captureState$ => mediaCaptureController.stream;

  MediaCapture? get captureState => mediaCaptureController.stream.value;

  CameraState get state => stateController.value;

  /// The config associated with a [Sensors].
  /// [back] sensor frequently has flash while [front] does not for instance.
  ValueStream<SensorConfig> get sensorConfig$ => sensorConfigController.stream;

  CameraContext._({
    required this.initialCaptureMode,
    required this.sensorConfigController,
    required this.analysisController,
    required this.saveConfig,
    required this.exifPreferences,
    required this.filterController,
    required this.enablePhysicalButton,
    required this.availableFilters,
    this.onPermissionsResult,
  }) {
    var preparingState = PreparingCameraState(
      this,
      initialCaptureMode,
    );
    stateController = BehaviorSubject.seeded(preparingState);
    filterSelectorOpened = BehaviorSubject.seeded(false);
    mediaCaptureController = BehaviorSubject.seeded(null);
  }

  CameraContext.create(
    SensorConfig sensorConfig, {
    required CaptureMode initialCaptureMode,
    OnPermissionsResult? onPermissionsResult,
    required SaveConfig? saveConfig,
    OnImageForAnalysis? onImageForAnalysis,
    AnalysisConfig? analysisConfig,
    required ExifPreferences exifPreferences,
    required AwesomeFilter filter,
    required bool enablePhysicalButton,
    List<AwesomeFilter>? availableFilters,
  }) : this._(
          initialCaptureMode: initialCaptureMode,
          sensorConfigController: BehaviorSubject.seeded(sensorConfig),
          filterController: BehaviorSubject.seeded(filter),
          enablePhysicalButton: enablePhysicalButton,
          onPermissionsResult: onPermissionsResult,
          saveConfig: saveConfig,
          analysisController: onImageForAnalysis != null
              ? AnalysisController.fromPlugin(
                  onImageListener: onImageForAnalysis,
                  conf: analysisConfig,
                )
              : null,
          exifPreferences: exifPreferences,
          availableFilters: availableFilters,
        );

  changeState(CameraState newState) async {
    final currentZoom = state.sensorConfig.zoom;
    state.dispose();

    if (state.captureMode != newState.captureMode) {
      // This should not be done multiple times for the same CaptureMode or it
      // generates problems (especially when recording a video)
      await CamerawesomePlugin.setCaptureMode(newState.captureMode!);
    }
    if (!stateController.isClosed) {
      stateController.add(newState);
    }

    // Reset filter when changing state
    // Currently camerAwesome does not support filter on video
    if (newState is VideoCameraState) {
      filterController.add(AwesomeFilter.None);
      filterSelectorOpened.add(false);
    }
    newState.sensorConfig.setZoom(currentZoom);
  }

  Future<void> toggleFilterSelector() async {
    filterSelectorOpened.add(!filterSelectorOpened.value);
  }

  Future<void> setFilter(AwesomeFilter newFilter) async {
    await CamerawesomePlugin.setFilter(newFilter);
    filterController.add(newFilter);
  }

  Future<void> setSensorConfig(SensorConfig newConfig) async {
    sensorConfigController.sink.add(newConfig);
    if (sensorConfigController.hasValue &&
        !identical(newConfig, sensorConfigController.value)) {
      sensorConfigController.value.dispose();
    }
    await CamerawesomePlugin.setSensor(
      newConfig.sensors,
    );
  }

  SensorConfig get sensorConfig {
    return sensorConfigController.value;
  }

  bool get imageAnalysisEnabled => analysisController?.enabled == true;

  dispose() {
    sensorConfig.dispose();
    sensorConfigController.close();
    mediaCaptureController.close();
    stateController.close();
    analysisController?.stop();
    state.dispose();
    CamerawesomePlugin.stop();
  }

  /// Global focus
  void focus() {
    CamerawesomePlugin.startAutoFocus();
  }

  /// Start auto focus on a specific [flutterPosition].
  /// [pixelPreviewSize] can be retrieved from [CameraState.previewSize].
  /// [flutterPreviewSize] is the size of the preview widget. You can retrieve
  /// it from the builders of [CameraAwesomeBuilder].
  ///
  /// Use [androidFocusSettings] for additional Android focus settings (auto
  /// focus timeout before going back to passive mode).
  Future<void> focusOnPoint({
    required Offset flutterPosition,
    required PreviewSize pixelPreviewSize,
    required PreviewSize flutterPreviewSize,
    AndroidFocusSettings? androidFocusSettings,
  }) async {
    if (Platform.isIOS) {
      final xPercentage = flutterPosition.dx / flutterPreviewSize.width;
      final yPercentage = flutterPosition.dy / flutterPreviewSize.height;

      return CamerawesomePlugin.focusOnPoint(
        position: Offset(xPercentage, yPercentage),
        previewSize: pixelPreviewSize,
        androidFocusSettings: null,
      );
    } else {
      final ratio = pixelPreviewSize.height / flutterPreviewSize.height;
      // Transform flutter position to pixel position
      Offset pixelPosition = flutterPosition.scale(ratio, ratio);
      return CamerawesomePlugin.focusOnPoint(
        position: pixelPosition,
        previewSize: pixelPreviewSize,
        androidFocusSettings: androidFocusSettings ??
            AndroidFocusSettings(autoCancelDurationInMillis: 5000),
      );
    }
  }

  Future<PreviewSize> previewSize(int index) {
    return CamerawesomePlugin.getEffectivPreviewSize(index);
  }

  Future<SensorDeviceData> getSensors() {
    return CamerawesomePlugin.getSensors();
  }

  Future<int?> previewTextureId(int cameraPosition) {
    return CamerawesomePlugin.getPreviewTexture(cameraPosition)
        .then(((value) => value?.toInt()));
  }
}



================================================
FILE: lib/src/orchestrator/adapters/pigeon_sensor_adapter.dart
================================================
import 'package:camerawesome/pigeon.dart';
import 'package:camerawesome/src/orchestrator/models/sensors.dart';

extension PigeonSensorAdapter on Sensor {
  PigeonSensor toPigeon() {
    return PigeonSensor(
      position: position?.name != null
          ? PigeonSensorPosition.values.byName(position!.name)
          : PigeonSensorPosition.unknown,
      deviceId: deviceId,
      type: type?.name != null
          ? PigeonSensorType.values.byName(type!.name)
          : PigeonSensorType.unknown,
    );
  }
}



================================================
FILE: lib/src/orchestrator/analysis/analysis_controller.dart
================================================
import 'dart:async';
import 'dart:io';

import 'package:camerawesome/camerawesome_plugin.dart';
import 'package:camerawesome/src/logger.dart';

class AnalysisController {
  final OnImageForAnalysis? onImageListener;

  final Stream<Map<String, dynamic>>? _images$;

  final AnalysisConfig conf;

  StreamSubscription? imageSubscription;

  bool _analysisEnabled;

  AnalysisController._({
    required Stream<Map<String, dynamic>>? images$,
    required this.conf,
    this.onImageListener,
    required bool analysisEnabled,
  })  : _images$ = images$,
        _analysisEnabled = analysisEnabled;

  factory AnalysisController.fromPlugin({
    OnImageForAnalysis? onImageListener,
    required AnalysisConfig? conf,
  }) =>
      AnalysisController._(
        onImageListener: onImageListener,
        conf: conf ?? AnalysisConfig(),
        images$: CamerawesomePlugin.listenCameraImages(),
        analysisEnabled: conf?.autoStart ?? true,
      );

  Future<void> setup() async {
    if (onImageListener == null) {
      printLog("...AnalysisController off, no onImageListener");
      return;
    }
    if (imageSubscription != null) {
      printLog('AnalysisController controller already started');
      return;
    }

    if (Platform.isIOS) {
      await CamerawesomePlugin.setupAnalysis(
        format: conf.cupertinoOptions.outputFormat,
        // TODO Can't set width on iOS
        width: 0,
        maxFramesPerSecond: conf.maxFramesPerSecond,
        autoStart: conf.autoStart,
      );
    } else {
      await CamerawesomePlugin.setupAnalysis(
        format: conf.androidOptions.outputFormat,
        width: conf.androidOptions.width,
        maxFramesPerSecond: conf.maxFramesPerSecond,
        autoStart: conf.autoStart,
      );
    }

    if (conf.autoStart) {
      await start();
    }
    printLog("...AnalysisController setup");
  }

  get enabled => onImageListener != null && _analysisEnabled;

  // this should not return a bool but just throw an exception if something goes wrong
  Future<bool> start() async {
    if (onImageListener == null || imageSubscription != null) {
      return false;
    }
    await CamerawesomePlugin.startAnalysis();
    imageSubscription = _images$?.listen((event) async {
      await onImageListener!(AnalysisImage.from(event));
      await CamerawesomePlugin.receivedImageFromStream();
    });
    _analysisEnabled = true;
    printLog("...AnalysisController started");
    return _analysisEnabled;
  }

  Future<void> stop() async {
    if (onImageListener == null || imageSubscription == null) {
      return;
    }
    _analysisEnabled = false;
    await CamerawesomePlugin.stopAnalysis();
    imageSubscription?.cancel();
    imageSubscription = null;
  }
}



================================================
FILE: lib/src/orchestrator/analysis/analysis_to_image.dart
================================================
import 'dart:io';

import 'package:camerawesome/camerawesome_plugin.dart';
import 'package:flutter/material.dart';

class AnalysisPreview {
  final Size nativePreviewSize;
  final Size previewSize;
  final Offset offset;
  final double scale;
  final Sensor? sensor;

  AnalysisPreview({
    required this.nativePreviewSize,
    required this.previewSize,
    required this.offset,
    required this.scale,
    required this.sensor,
  });

  factory AnalysisPreview.hidden() => AnalysisPreview(
        nativePreviewSize: Size.zero,
        previewSize: Size.zero,
        offset: Offset.zero,
        scale: 1,
        sensor: null,
      );

  Offset convertPoint(Offset point) {
    return Offset(point.dx * scale, point.dy * scale)
        .translate(offset.dx, offset.dy);
  }

  /// this method is used to convert a point from an image to the preview
  /// according to the current preview size and the image size
  /// also in case of Android, it will flip the point if required
  Offset convertFromImage(
    Offset point,
    AnalysisImage img, {
    bool? flipXY,
  }) {
    num imageDiffX;
    num imageDiffY;
    final shouldflipXY = flipXY ?? img.flipXY();
    if (Platform.isIOS) {
      imageDiffX = img.size.width - img.croppedSize.width;
      imageDiffY = img.size.height - img.croppedSize.height;
    } else {
      // Width and height are inverted on Android
      imageDiffX = img.size.height - img.croppedSize.width;
      imageDiffY = img.size.width - img.croppedSize.height;
    }
    var offset = (Offset(
              (shouldflipXY ? point.dy : point.dx).toDouble() -
                  (imageDiffX / 2),
              (shouldflipXY ? point.dx : point.dy).toDouble() -
                  (imageDiffY / 2),
            ) *
            scale)
        .translate(
      // If screenSize is bigger than croppedSize, move the element to half the difference
      (previewSize.width - (img.croppedSize.width * scale)) / 2,
      (previewSize.height - (img.croppedSize.height * scale)) / 2,
    );
    return offset;
  }

  Rect get rect => Rect.fromCenter(
        center: previewSize.center(Offset.zero),
        width: previewSize.width,
        height: previewSize.height,
      );

  bool get isBackCamera => sensor?.position == SensorPosition.back;
}



================================================
FILE: lib/src/orchestrator/exceptions/camera_states_exceptions.dart
================================================
// You called an action you are not supposed to call while camera is loading
class CameraNotReadyException implements Exception {
  final String? message;
  CameraNotReadyException({this.message});

  @override
  String toString() {
    return '''
      CamerAwesome is not ready yet. 
      ==============================================================
      You must call start when current state is PreparingCameraState
      --------------------------------------------------------------
      additional informations: $message
    ''';
  }
}

/// from [PreparingCameraState] you must provide a valid next capture mode
class NoValidCaptureModeException implements Exception {}



================================================
FILE: lib/src/orchestrator/file/builder/capture_request_builder.dart
================================================
import 'package:camerawesome/src/orchestrator/file/builder/capture_request_builder_stub.dart'
    if (dart.library.io) 'capture_request_builder_io.dart'
    if (dart.library.html) 'capture_request_builder_web.dart';
import 'package:camerawesome/src/orchestrator/models/capture_modes.dart';
import 'package:camerawesome/src/orchestrator/models/capture_request.dart';
import 'package:camerawesome/src/orchestrator/models/sensors.dart';

abstract class BaseCaptureRequestBuilder {
  Future<CaptureRequest> build({
    required CaptureMode captureMode,
    required List<Sensor> sensors,
  });
}

class AwesomeCaptureRequestBuilder {
  final CaptureRequestBuilderImpl _fileBuilder;

  AwesomeCaptureRequestBuilder() : _fileBuilder = CaptureRequestBuilderImpl();

  Future<CaptureRequest> build({
    required CaptureMode captureMode,
    required List<Sensor> sensors,
  }) {
    return _fileBuilder.build(captureMode: captureMode, sensors: sensors);
  }
}



================================================
FILE: lib/src/orchestrator/file/builder/capture_request_builder_io.dart
================================================
import 'dart:io';

import 'package:camerawesome/src/orchestrator/file/builder/capture_request_builder.dart';
import 'package:camerawesome/src/orchestrator/models/capture_modes.dart';
import 'package:camerawesome/src/orchestrator/models/capture_request.dart';
import 'package:camerawesome/src/orchestrator/models/sensors.dart';
import 'package:path_provider/path_provider.dart';

class CaptureRequestBuilderImpl extends BaseCaptureRequestBuilder {
  Future<String> newFile(
    CaptureMode captureMode, {
    Sensor? sensor,
  }) async {
    final Directory extDir = await getTemporaryDirectory();
    final testDir =
        await Directory('${extDir.path}/camerawesome').create(recursive: true);
    final String fileExtension =
        captureMode == CaptureMode.photo ? 'jpg' : 'mp4';
    String extension = "";
    if (sensor != null) {
      if (sensor.position != null) {
        extension = "_${sensor.position!.name}";
      }
      if (sensor.type != null) {
        extension = "${extension}_${sensor.type}";
      }
      if (sensor.deviceId != null) {
        extension = "${extension}_${sensor.deviceId}";
      }
    }
    final String filePath =
        '${testDir.path}/${DateTime.now().microsecondsSinceEpoch}$extension.$fileExtension';
    return filePath;
  }

  @override
  Future<CaptureRequest> build({
    required CaptureMode captureMode,
    required List<Sensor> sensors,
  }) async {
    if (sensors.length == 1) {
      return SingleCaptureRequest(await newFile(captureMode), sensors.first);
    } else {
      return MultipleCaptureRequest({
        for (var sensor in sensors)
          sensor: await newFile(captureMode, sensor: sensor),
      });
    }
  }
}



================================================
FILE: lib/src/orchestrator/file/builder/capture_request_builder_stub.dart
================================================
import 'package:camerawesome/src/orchestrator/file/builder/capture_request_builder.dart';
import 'package:camerawesome/src/orchestrator/models/capture_modes.dart';
import 'package:camerawesome/src/orchestrator/models/capture_request.dart';
import 'package:camerawesome/src/orchestrator/models/sensors.dart';

class CaptureRequestBuilderImpl extends BaseCaptureRequestBuilder {
  @override
  Future<CaptureRequest> build({
    required CaptureMode captureMode,
    required List<Sensor> sensors,
  }) {
    throw "Stub method";
  }
}



================================================
FILE: lib/src/orchestrator/file/builder/capture_request_builder_web.dart
================================================
import 'package:camerawesome/src/orchestrator/file/builder/capture_request_builder.dart';
import 'package:camerawesome/src/orchestrator/models/capture_modes.dart';
import 'package:camerawesome/src/orchestrator/models/capture_request.dart';
import 'package:camerawesome/src/orchestrator/models/sensors.dart';

class CaptureRequestBuilderImpl extends BaseCaptureRequestBuilder {
  @override
  Future<CaptureRequest> build({
    required CaptureMode captureMode,
    required List<Sensor> sensors,
  }) async {
    if (sensors.length == 1) {
      return SingleCaptureRequest(null, sensors.first);
    } else {
      return MultipleCaptureRequest({for (var sensor in sensors) sensor: null});
    }
  }
}



================================================
FILE: lib/src/orchestrator/file/content/file_content.dart
================================================
import 'dart:typed_data';

import 'package:camerawesome/src/orchestrator/file/content/file_content_stub.dart'
    if (dart.library.io) 'file_content_io.dart'
    if (dart.library.html) 'file_content_web.dart';
import 'package:cross_file/cross_file.dart';

abstract class BaseFileContent {
  Future<Uint8List?> read(XFile file);

  Future<XFile?> write(XFile file, Uint8List bytes);
}

class FileContent {
  final FileContentImpl _fileBuilder;

  FileContent() : _fileBuilder = FileContentImpl();

  Future<Uint8List?> read(XFile file) {
    return _fileBuilder.read(file);
  }

  Future<XFile?> write(XFile file, Uint8List bytes) {
    return _fileBuilder.write(file, bytes);
  }
}



================================================
FILE: lib/src/orchestrator/file/content/file_content_io.dart
================================================
import 'dart:io';
import 'dart:typed_data';

import 'package:camerawesome/src/orchestrator/file/content/file_content.dart';
import 'package:cross_file/cross_file.dart';

class FileContentImpl extends BaseFileContent {
  @override
  Future<Uint8List?> read(XFile file) {
    return File(file.path).readAsBytes();
  }

  @override
  Future<XFile?> write(XFile file, Uint8List bytes) async {
    await File(file.path).writeAsBytes(bytes);
    return file;
  }
}



================================================
FILE: lib/src/orchestrator/file/content/file_content_stub.dart
================================================
import 'dart:typed_data';

import 'package:camerawesome/src/orchestrator/file/content/file_content.dart';
import 'package:cross_file/cross_file.dart';

class FileContentImpl extends BaseFileContent {
  @override
  Future<Uint8List?> read(XFile file) {
    throw Exception("Stub method");
  }

  @override
  Future<XFile?> write(XFile file, Uint8List bytes) {
    throw Exception("Stub method");
  }
}



================================================
FILE: lib/src/orchestrator/file/content/file_content_web.dart
================================================
import 'dart:typed_data';

import 'package:camerawesome/src/orchestrator/file/content/file_content.dart';
import 'package:cross_file/cross_file.dart';

class FileContentImpl extends BaseFileContent {
  @override
  Future<Uint8List?> read(XFile file) {
    return file.readAsBytes();
  }

  @override
  Future<XFile?> write(XFile file, Uint8List bytes) async {
    return XFile(file.path, bytes: bytes);
  }
}



================================================
FILE: lib/src/orchestrator/models/camera_flashes.dart
================================================
enum FlashMode {
  none,
  on,
  auto,
  always,
}



================================================
FILE: lib/src/orchestrator/models/camera_orientations.dart
================================================
// ignore_for_file: constant_identifier_names
enum CameraOrientations {
  landscape_left,
  landscape_right,
  portrait_up,
  portrait_down,
}



================================================
FILE: lib/src/orchestrator/models/camera_physical_button.dart
================================================
// ignore_for_file: constant_identifier_names
enum CameraPhysicalButton {
  volume_up,
  volume_down,
}



================================================
FILE: lib/src/orchestrator/models/capture_modes.dart
================================================
import 'package:camerawesome/camerawesome_plugin.dart';
import 'package:camerawesome/src/orchestrator/camera_context.dart';

enum CaptureMode {
  photo,
  video,
  preview,
  // ignore: constant_identifier_names
  analysis_only;

  CameraState toCameraState(CameraContext cameraContext) {
    if (this == CaptureMode.photo) {
      return PhotoCameraState.from(cameraContext);
    } else if (this == CaptureMode.video) {
      return VideoCameraState.from(cameraContext);
    } else if (this == CaptureMode.preview) {
      return PreviewCameraState(cameraContext: cameraContext);
    } else if (this == CaptureMode.analysis_only) {
      return AnalysisCameraState(cameraContext: cameraContext);
    }
    throw "State not recognized";
  }
}



================================================
FILE: lib/src/orchestrator/models/capture_request.dart
================================================
import 'package:camerawesome/src/orchestrator/models/sensors.dart';
import 'package:cross_file/cross_file.dart';

abstract class CaptureRequest {
  const CaptureRequest();

  T when<T>({
    T Function(SingleCaptureRequest)? single,
    T Function(MultipleCaptureRequest)? multiple,
  }) {
    if (this is SingleCaptureRequest) {
      return single!(this as SingleCaptureRequest);
    } else if (this is MultipleCaptureRequest) {
      return multiple!(this as MultipleCaptureRequest);
    } else {
      throw Exception("Unknown CaptureResult type");
    }
  }

  String? get path;
}

class SingleCaptureRequest extends CaptureRequest {
  final XFile? file;
  final Sensor sensor;

  SingleCaptureRequest(String? filePath, this.sensor)
      : file = filePath == null ? null : XFile(filePath);

  @override
  String? get path => file?.path;
}

class MultipleCaptureRequest extends CaptureRequest {
  final Map<Sensor, XFile?> fileBySensor;

  MultipleCaptureRequest(Map<Sensor, String?> filePathBySensor)
      : fileBySensor = {
          for (final sensor in filePathBySensor.keys)
            sensor: filePathBySensor[sensor] != null
                ? XFile(filePathBySensor[sensor]!)
                : null,
        };

  @override
  String? get path =>
      fileBySensor.values.firstWhere((element) => element != null)?.path;
}



================================================
FILE: lib/src/orchestrator/models/media_capture.dart
================================================
import 'package:camerawesome/camerawesome_plugin.dart';
import 'package:flutter/foundation.dart';

enum MediaCaptureStatus {
  capturing,
  success,
  failure,
}

enum VideoState {
  started,
  paused,
  resumed,
  stopped,
  error,
}

class MediaCapture {
  final Exception? exception;
  final CaptureRequest captureRequest;
  final MediaCaptureStatus status;
  final VideoState? videoState;

  MediaCapture.capturing({
    this.exception,
    required this.captureRequest,
    this.videoState,
  }) : status = MediaCaptureStatus.capturing;

  MediaCapture.success({
    this.exception,
    required this.captureRequest,
  })  : status = MediaCaptureStatus.success,
        videoState = VideoState.stopped;

  MediaCapture.failure({
    this.exception,
    required this.captureRequest,
  })  : status = MediaCaptureStatus.failure,
        videoState = VideoState.error;

  /// Returns true if the capture has either a mimeType with "image" inside or
  /// if the file path ends with "jpg".
  /// On Web, returns always true
  bool get isPicture => kIsWeb
      ? true
      : captureRequest.when(
          single: (singleCaptureRequest) =>
              singleCaptureRequest.file?.path.endsWith("jpg") == true ||
              singleCaptureRequest.file?.mimeType?.contains("image") == true,
          multiple: (multipleCaptureRequest) =>
              multipleCaptureRequest.fileBySensor.values.first?.path
                      .endsWith("jpg") ==
                  true ||
              multipleCaptureRequest.fileBySensor.values.first?.mimeType
                      ?.contains("image") ==
                  true,
        );

  bool get isVideo => !isPicture;

  bool get isRecordingVideo =>
      isVideo && status == MediaCaptureStatus.capturing;
}



================================================
FILE: lib/src/orchestrator/models/models.dart
================================================
export 'analysis/analysis_image.dart';
export 'camera_flashes.dart';
export 'camera_orientations.dart';
export 'capture_modes.dart';
export 'capture_request.dart';
export 'media_capture.dart';
export 'sensor_data.dart';
export 'sensors.dart';
export 'save_config.dart';
export 'sensor_config.dart';
export 'permission_utils.dart';
export 'filters/awesome_filter.dart';
export 'analysis/analysis.dart';



================================================
FILE: lib/src/orchestrator/models/permission_utils.dart
================================================
import 'package:camerawesome/pigeon.dart';

extension PermissionsUtils on CamerAwesomePermission {
  static List<CamerAwesomePermission> get needed => [
        CamerAwesomePermission.camera,
        //CamerAwesomePermission.storage,
      ];
}

extension PermissionsMatcher on List<CamerAwesomePermission> {
  bool hasRequiredPermissions() {
    for (var p in PermissionsUtils.needed) {
      if (!contains(p)) {
        return false;
      }
    }
    return true;
  }
}



================================================
FILE: lib/src/orchestrator/models/save_config.dart
================================================
import 'package:camerawesome/camerawesome_plugin.dart';
import 'package:camerawesome/pigeon.dart';
import 'package:camerawesome/src/orchestrator/file/builder/capture_request_builder.dart';

typedef CaptureRequestBuilder = Future<CaptureRequest> Function(
    List<Sensor> sensors);

class SaveConfig {
  final CaptureRequestBuilder? photoPathBuilder;
  final CaptureRequestBuilder? videoPathBuilder;
  final List<CaptureMode> captureModes;
  final CaptureMode initialCaptureMode;
  final VideoOptions? videoOptions;
  final bool mirrorFrontCamera;

  /// Choose if you want to persist user location in image metadata or not
  final ExifPreferences? exifPreferences;

  SaveConfig._({
    this.photoPathBuilder,
    this.videoPathBuilder,
    required this.captureModes,
    required this.initialCaptureMode,
    this.videoOptions,
    this.exifPreferences,
    required this.mirrorFrontCamera,
  });

  /// You only want to take photos
  SaveConfig.photo({
    CaptureRequestBuilder? pathBuilder,
    ExifPreferences? exifPreferences,
    bool mirrorFrontCamera = false,
  }) : this._(
          photoPathBuilder: pathBuilder ??
              (sensors) => AwesomeCaptureRequestBuilder()
                  .build(captureMode: CaptureMode.photo, sensors: sensors),
          captureModes: [CaptureMode.photo],
          initialCaptureMode: CaptureMode.photo,
          exifPreferences: exifPreferences,
          mirrorFrontCamera: mirrorFrontCamera,
        );

  /// You only want to take videos
  SaveConfig.video({
    CaptureRequestBuilder? pathBuilder,
    VideoOptions? videoOptions,
    bool mirrorFrontCamera = false,
  }) : this._(
          videoPathBuilder: pathBuilder ??
              (sensors) => AwesomeCaptureRequestBuilder()
                  .build(captureMode: CaptureMode.video, sensors: sensors),
          captureModes: [CaptureMode.video],
          initialCaptureMode: CaptureMode.video,
          videoOptions: videoOptions,
          mirrorFrontCamera: mirrorFrontCamera,
        );

  /// You want to be able to take both photos and videos
  SaveConfig.photoAndVideo({
    CaptureRequestBuilder? photoPathBuilder,
    CaptureRequestBuilder? videoPathBuilder,
    CaptureMode initialCaptureMode = CaptureMode.photo,
    VideoOptions? videoOptions,
    ExifPreferences? exifPreferences,
    bool mirrorFrontCamera = false,
  }) : this._(
          photoPathBuilder: photoPathBuilder ??
              (sensors) => AwesomeCaptureRequestBuilder()
                  .build(captureMode: CaptureMode.photo, sensors: sensors),
          videoPathBuilder: videoPathBuilder ??
              (sensors) => AwesomeCaptureRequestBuilder()
                  .build(captureMode: CaptureMode.video, sensors: sensors),
          captureModes: [CaptureMode.photo, CaptureMode.video],
          initialCaptureMode: initialCaptureMode,
          videoOptions: videoOptions,
          exifPreferences: exifPreferences,
          mirrorFrontCamera: mirrorFrontCamera,
        );
}



================================================
FILE: lib/src/orchestrator/models/sensor_config.dart
================================================
// ignore_for_file: close_sinks

import 'dart:async';

import 'package:camerawesome/camerawesome_plugin.dart';
import 'package:rxdart/rxdart.dart';

// TODO find a way to explain that this sensorconfig is not bound anymore (user changed sensor for example)
class SensorConfig {
  late BehaviorSubject<FlashMode> _flashModeController;

  late BehaviorSubject<SensorType> _sensorTypeController;

  late Stream<FlashMode> flashMode$;

  late Stream<SensorType> sensorType$;

  late BehaviorSubject<CameraAspectRatios> _aspectRatioController;

  late Stream<CameraAspectRatios> aspectRatio$;

  /// Zoom from native side. Must be between 0.0 and 1.0
  late Stream<double> zoom$;

  /// [back] or [front] camera
  final List<Sensor> sensors;

  // /// choose your photo size from the [selectDefaultSize] method
  // late Stream<Size?> previewSize;

  /// set brightness correction manually range [0,1] (optional)
  late Stream<double>? brightness$;

  late BehaviorSubject<double> _zoomController;

  /// Use this stream to debounce brightness events
  final BehaviorSubject<double> _brightnessController =
      BehaviorSubject<double>();
  StreamSubscription? _brightnessSubscription;

  SensorConfig.single({
    Sensor? sensor,
    FlashMode flashMode = FlashMode.none,
    double zoom = 0.0,
    CameraAspectRatios aspectRatio = CameraAspectRatios.ratio_4_3,
  }) : this._(
          sensors: [sensor ?? Sensor.position(SensorPosition.back)],
          flash: flashMode,
          currentZoom: zoom,
          aspectRatio: aspectRatio,
        );

  SensorConfig.multiple({
    required List<Sensor> sensors,
    FlashMode flashMode = FlashMode.none,
    double zoom = 0.0,
    CameraAspectRatios aspectRatio = CameraAspectRatios.ratio_4_3,
  }) : this._(
          sensors: sensors,
          flash: flashMode,
          currentZoom: zoom,
          aspectRatio: aspectRatio,
        );

  SensorConfig._({
    required this.sensors,
    FlashMode flash = FlashMode.none,
    CameraAspectRatios aspectRatio = CameraAspectRatios.ratio_4_3,

    /// Zoom must be between 0.0 (no zoom) and 1.0 (max zoom)
    double currentZoom = 0.0,
  }) {
    _flashModeController = BehaviorSubject<FlashMode>.seeded(flash);
    flashMode$ = _flashModeController.stream;

    _sensorTypeController = BehaviorSubject<SensorType>.seeded(
        sensors.first.type ?? SensorType.wideAngle);
    sensorType$ = _sensorTypeController.stream;

    _zoomController = BehaviorSubject<double>.seeded(currentZoom);
    zoom$ = _zoomController.stream;

    _aspectRatioController = BehaviorSubject.seeded(aspectRatio);
    aspectRatio$ = _aspectRatioController.stream;

    _brightnessSubscription = _brightnessController.stream
        .debounceTime(const Duration(milliseconds: 500))
        .listen((value) => CamerawesomePlugin.setBrightness(value));
  }

  Future<void> setZoom(double zoom) async {
    if (zoom < 0 || zoom > 1) {
      throw "Zoom value must be between 0 and 1";
    }
    await CamerawesomePlugin.setZoom(zoom);
    if (!_zoomController.isClosed) {
      _zoomController.sink.add(zoom);
    }
  }

  /// Returns the current zoom without stream
  double get zoom => _zoomController.value;

  /// Set manually the [FlashMode] between
  /// [FlashMode.none] no flash
  /// [FlashMode.on] always flashing when taking photo
  /// [FlashMode.auto] let the camera decide if it should use flash or not
  /// [FlashMode.always] flash light stays open
  Future<void> setFlashMode(FlashMode flashMode) async {
    await CamerawesomePlugin.setFlashMode(flashMode);
    _flashModeController.sink.add(flashMode);
  }

  /// Returns the current flash mode without stream
  FlashMode get flashMode => _flashModeController.value;

  /// Switch the flash according to the previous state
  void switchCameraFlash() {
    final FlashMode newFlashMode;
    switch (flashMode) {
      case FlashMode.none:
        newFlashMode = FlashMode.auto;
        break;
      case FlashMode.on:
        newFlashMode = FlashMode.always;
        break;
      case FlashMode.auto:
        newFlashMode = FlashMode.on;
        break;
      case FlashMode.always:
        newFlashMode = FlashMode.none;
        break;
    }
    setFlashMode(newFlashMode);
  }

  /// switch the camera preview / photo / video aspect ratio
  /// [CameraAspectRatios.ratio_16_9]
  /// [CameraAspectRatios.ratio_4_3]
  /// [CameraAspectRatios.ratio_1_1]
  Future<void> switchCameraRatio() async {
    if (aspectRatio == CameraAspectRatios.ratio_16_9) {
      setAspectRatio(CameraAspectRatios.ratio_4_3);
    } else if (aspectRatio == CameraAspectRatios.ratio_4_3) {
      setAspectRatio(CameraAspectRatios.ratio_1_1);
    } else {
      setAspectRatio(CameraAspectRatios.ratio_16_9);
    }
  }

  /// Change the current [CameraAspectRatios] one of
  /// [CameraAspectRatios.ratio_16_9]
  /// [CameraAspectRatios.ratio_4_3]
  /// [CameraAspectRatios.ratio_1_1]
  Future<void> setAspectRatio(CameraAspectRatios ratio) async {
    await CamerawesomePlugin.setAspectRatio(ratio.name);
    _aspectRatioController.add(ratio);
  }

  /// Returns the current camera aspect ratio without stream
  CameraAspectRatios get aspectRatio => _aspectRatioController.value;

  /// set brightness correction manually range [0,1] (optionnal)
  setBrightness(double brightness) {
    if (brightness < 0 || brightness > 1) {
      throw "Brightness value must be between 0 and 1";
    }
    // The stream will debounce before actually setting the brightness
    _brightnessController.sink.add(brightness);
  }

  /// Returns the current brightness without stream
  double get brightness => _brightnessController.value;

  void dispose() {
    _brightnessSubscription?.cancel();
    _brightnessController.close();
    _sensorTypeController.close();
    _zoomController.close();
    _flashModeController.close();
    _aspectRatioController.close();
  }
}



================================================
FILE: lib/src/orchestrator/models/sensor_data.dart
================================================
/// used to expose Brightness level
class SensorData {
  double value;

  SensorData(this.value);
}



================================================
FILE: lib/src/orchestrator/models/sensor_type.dart
================================================
enum SensorType {
  /// A built-in wide-angle camera.
  ///
  /// The wide angle sensor is the default sensor for iOS
  wideAngle,

  /// A built-in camera with a shorter focal length than that of the wide-angle camera.
  ultraWideAngle,

  /// A built-in camera device with a longer focal length than the wide-angle camera.
  telephoto,

  /// A device that consists of two cameras, one Infrared and one YUV.
  ///
  /// iOS only
  trueDepth,

  unknown;

  SensorType get defaultSensorType => SensorType.wideAngle;
}

class SensorTypeDevice {
  final SensorType sensorType;

  /// A localized device name for display in the user interface.
  final String name;

  /// The current exposure ISO value.
  final num iso;

  /// A Boolean value that indicates whether the flash is currently available for use.
  final bool flashAvailable;

  /// An identifier that uniquely identifies the device.
  final String uid;

  SensorTypeDevice({
    required this.sensorType,
    required this.name,
    required this.iso,
    required this.flashAvailable,
    required this.uid,
  });
}

// TODO: instead of storing SensorTypeDevice values,
// this would be useful when CameraX will support multiple sensors.
// store them in a list of SensorTypeDevice.
// ex:
// List<SensorTypeDevice> wideAngle;
// List<SensorTypeDevice> ultraWideAngle;

class SensorDeviceData {
  /// A built-in wide-angle camera.
  ///
  /// The wide angle sensor is the default sensor for iOS
  SensorTypeDevice? wideAngle;

  /// A built-in camera with a shorter focal length than that of the wide-angle camera.
  SensorTypeDevice? ultraWideAngle;

  /// A built-in camera device with a longer focal length than the wide-angle camera.
  SensorTypeDevice? telephoto;

  /// A device that consists of two cameras, one Infrared and one YUV.
  ///
  /// iOS only
  SensorTypeDevice? trueDepth;

  SensorDeviceData({
    this.wideAngle,
    this.ultraWideAngle,
    this.telephoto,
    this.trueDepth,
  });

  List<SensorTypeDevice> get availableSensors {
    return [
      wideAngle,
      ultraWideAngle,
      telephoto,
      trueDepth,
    ].where((element) => element != null).cast<SensorTypeDevice>().toList();
  }

  int get availableBackSensors => [
        wideAngle,
        ultraWideAngle,
        telephoto,
      ].where((element) => element != null).length;

  int get availableFrontSensors => [
        trueDepth,
      ].where((element) => element != null).length;
}



================================================
FILE: lib/src/orchestrator/models/sensors.dart
================================================
import 'package:camerawesome/camerawesome_plugin.dart';

enum CameraAspectRatios {
  ratio_16_9,
  ratio_4_3,
  ratio_1_1; // only for iOS

  CameraAspectRatios get defaultRatio => CameraAspectRatios.ratio_4_3;
}

enum SensorPosition {
  front,
  back,
}

class Sensor {
  SensorPosition? position;
  SensorType? type;
  String? deviceId;

  Sensor._({
    this.position,
    this.type,
    this.deviceId,
  });

  Sensor.position(SensorPosition position)
      : this._(
          position: position,
        );

  Sensor.type(SensorType type)
      : this._(
          type: type,
        );

  Sensor.id(String deviceId)
      : this._(
          deviceId: deviceId,
        );
}



================================================
FILE: lib/src/orchestrator/models/video_options.dart
================================================
enum CupertinoVideoCodec {
  /// The H.264 video codec.
  h264,

  /// The HEVC video codec.
  hevc,

  /// The HEVC video codec that supports an alpha channel.
  hevcWithAlpha,

  /// The JPEG video codec.
  jpeg,

  /// The Apple ProRes 4444 video codec.
  appleProRes4444,

  /// The Apple ProRes 422 video codec.
  appleProRes422,

  /// The Apple ProRes 422 HQ video codec.
  appleProRes422HQ,

  /// The Apple ProRes 422 LT video codec.
  appleProRes422LT,

  /// The Apple ProRes 422 Proxy video codec.
  appleProRes422Proxy,
}

enum CupertinoFileType {
  /// The UTI for the QuickTime movie file format.
  ///
  /// Files are identified with the .mov and .qt extensions.
  quickTimeMovie,

  /// The UTI for the MPEG-4 file format.
  ///
  /// Files are identified with the .mp4 extension.
  mpeg4,

  /// The UTI for the iTunes video file format.
  ///
  /// Files are identified with the .m4v extension.
  appleM4V,

  /// The UTI for the 3GPP file format.
  ///
  /// Files are identified with the .3gp, .3gpp, and .sdv extensions.
  type3GPP,

  /// The UTI for the 3GPP2 file format.
  ///
  /// Files are identified with the .3g2, .3gp2 extensions.
  type3GPP2,
}

class CupertinoVideoOptions {
  /// The video codec to use when recording a video.
  CupertinoVideoCodec codec;

  /// The file type to use when recording a video.
  ///
  /// **WARNING:** Be sure to use the correct file type extension for the video!
  CupertinoFileType fileType;

  int? fps;

  CupertinoVideoOptions({
    this.codec = CupertinoVideoCodec.h264,
    this.fileType = CupertinoFileType.quickTimeMovie,
    this.fps,
  });

  Map<String, dynamic> toMap() {
    return {
      'codec': codec.name,
      'fileType': fileType.name,
      'fps': fps,
    };
  }
}



================================================
FILE: lib/src/orchestrator/models/analysis/analysis.dart
================================================
export 'analysis_config.dart';
export 'analysis_image.dart';
export 'image_plane.dart';
export 'input_analysis.dart';



================================================
FILE: lib/src/orchestrator/models/analysis/analysis_canvas.dart
================================================
import 'dart:math';

import 'package:flutter/material.dart';

class CanvasTransformation {
  final Point? scale;
  final Point? translate;

  const CanvasTransformation({
    this.scale,
    this.translate,
  });
}

extension CanvasTransformationExt on Canvas {
  void applyTransformation(
    CanvasTransformation transformation,
    Size canvasSize,
  ) {
    if (transformation.scale != null) {
      scale(transformation.scale!.x.toDouble(),
          transformation.scale!.y.toDouble());
    }
    if (transformation.translate != null) {
      translate(
        transformation.translate!.x.toDouble() * canvasSize.width,
        transformation.translate!.y.toDouble() * canvasSize.height,
      );
    }
  }
}

extension PointExt on Point {
  Offset toOffset() => Offset(x.toDouble(), y.toDouble());
}



================================================
FILE: lib/src/orchestrator/models/analysis/analysis_config.dart
================================================
import 'package:camerawesome/camerawesome_plugin.dart';

class AnalysisConfig {
  /// This is used to improve performance on low performance devices.
  /// It will skip frames if the camera is producing more than the specified.
  ///
  /// For exemple, if the camera is producing 30fps and you set this to 10, it will skip 20 frames.
  ///
  /// Default is null (disabled).
  final double? maxFramesPerSecond;

  /// When set to true, image analysis starts automatically. Otherwise, you will
  /// have to start it manually using [AnalysisController.start()]
  final bool autoStart;

  /// Android specific options for image analysis.
  final AndroidAnalysisOptions androidOptions;

  /// iOS specific options for image analysis.
  final CupertinoAnalysisOptions cupertinoOptions;

  AnalysisConfig({
    this.maxFramesPerSecond,
    this.autoStart = true,
    this.androidOptions = const AndroidAnalysisOptions.nv21(width: 500),
    this.cupertinoOptions = const CupertinoAnalysisOptions.bgra8888(),
  });
}

class AndroidAnalysisOptions {
  /// Image analysis format.
  /// Recommended format for image analysis on Android is nv21.
  final InputAnalysisImageFormat outputFormat;

  /// `Target width of you image analysis. CamerAwesome will try to find the
  /// closest resolution to this [width].
  /// The smaller the image, the faster the analysis will be. 500 is often enough
  /// to detect barcodes or faces for example.
  final int width;

  const AndroidAnalysisOptions._({
    this.outputFormat = InputAnalysisImageFormat.nv21,
    this.width = 500,
  });

  const AndroidAnalysisOptions.nv21({
    required int width,
  }) : this._(
          width: width,
          outputFormat: InputAnalysisImageFormat.nv21,
        );

  const AndroidAnalysisOptions.yuv420({
    required int width,
  }) : this._(
          width: width,
          outputFormat: InputAnalysisImageFormat.yuv_420,
        );

  const AndroidAnalysisOptions.bgra8888({
    required int width,
  }) : this._(
          width: width,
          outputFormat: InputAnalysisImageFormat.bgra8888,
        );

  const AndroidAnalysisOptions.jpeg({
    required int width,
  }) : this._(width: width, outputFormat: InputAnalysisImageFormat.jpeg);
}

class CupertinoAnalysisOptions {
  /// Image analysis format.
  /// Recommended format for image analysis on iOS is bgra8888.
  final InputAnalysisImageFormat outputFormat;

  const CupertinoAnalysisOptions._({
    required this.outputFormat,
  });

  const CupertinoAnalysisOptions.bgra8888()
      : this._(outputFormat: InputAnalysisImageFormat.bgra8888);
}



================================================
FILE: lib/src/orchestrator/models/analysis/analysis_image.dart
================================================
import 'dart:io';
import 'dart:math';
import 'dart:ui';

import 'package:camerawesome/camerawesome_plugin.dart';
import 'package:camerawesome/pigeon.dart';
import 'package:flutter/foundation.dart';

part 'analysis_image_ext.dart';

abstract class AnalysisImage {
  /// The height of the image in pixels.
  final int height;

  /// The width of the image in pixels.
  final int width;

  /// The format of the image.
  final InputAnalysisImageFormat format;

  /// The rotation of the image.
  final InputAnalysisImageRotation rotation;

  /// Full size of the analysis image.
  Size get size => Size(width.toDouble(), height.toDouble());

  /// Cropped size of the analysis image corresponding to what is seen on the preview.
  Size get croppedSize;

  const AnalysisImage({
    required this.height,
    required this.width,
    required this.format,
    required this.rotation,
  });

  factory AnalysisImage.from(Map<String, dynamic> map) {
    final format = inputAnalysisImageFormatParser(map["format"]);
    if (format == InputAnalysisImageFormat.nv21) {
      return Nv21Image.from(map);
    } else if (format == InputAnalysisImageFormat.bgra8888) {
      return Bgra8888Image.from(map);
    } else if (format == InputAnalysisImageFormat.yuv_420) {
      return Yuv420Image.from(map);
    } else if (format == InputAnalysisImageFormat.jpeg) {
      return JpegImage.from(map);
    } else {
      throw "Unsupported AnalysisImage format: $format";
    }
  }

  /// Helper function to decide what to do depending on the AnalysisImage format with type safe checks.
  T? when<T>({
    T Function(Nv21Image image)? nv21,
    T Function(Bgra8888Image image)? bgra8888,
    T Function(JpegImage image)? jpeg,
    T Function(Yuv420Image image)? yuv420,
  }) {
    if (this is Nv21Image) {
      return nv21?.call(this as Nv21Image);
    } else if (this is Bgra8888Image) {
      return bgra8888?.call(this as Bgra8888Image);
    } else if (this is JpegImage) {
      return jpeg?.call(this as JpegImage);
    } else if (this is Yuv420Image) {
      return yuv420?.call(this as Yuv420Image);
    } else {
      throw "Unsupported AnalysisImage format: $format";
    }
  }

  // Symmetry for Android since native image analysis is not mirrored but preview is
  // if true when drawing the image on the preview, flip x and y
  bool flipXY() {
    if (Platform.isAndroid) {
      switch (rotation) {
        case InputAnalysisImageRotation.rotation0deg:
        case InputAnalysisImageRotation.rotation180deg:
          return true;
        case InputAnalysisImageRotation.rotation90deg:
        default:
          return false;
      }
    }
    return false;
  }

  // Symmetry for Android since native image analysis is not mirrored but preview is
  // It also handles device rotation
  CanvasTransformation? getCanvasTransformation(
    AnalysisPreview preview,
  ) {
    if (!Platform.isAndroid) {
      return null;
    }

    return switch ((rotation, preview.sensor?.position)) {
      (InputAnalysisImageRotation.rotation0deg, SensorPosition.back) =>
        const CanvasTransformation(
          scale: Point(-1, 1),
          translate: Point(-1, 0),
        ),
      (InputAnalysisImageRotation.rotation180deg, SensorPosition.back) =>
        const CanvasTransformation(
          scale: Point(1, -1),
          translate: Point(0, -1),
        ),
      (InputAnalysisImageRotation.rotation90deg, SensorPosition.back) => null,
      // const CanvasTransformation(
      //   scale: Point(1, -1),
      //   translate: Point(0, -1),
      // ),
      (InputAnalysisImageRotation.rotation0deg, _) ||
      (_, SensorPosition.back) =>
        const CanvasTransformation(
          scale: Point(-1, -1),
          translate: Point(-1, -1),
        ),
      (_, _) => const CanvasTransformation(
          scale: Point(-1, 1),
          translate: Point(-1, 0),
        ),
    };
  }
}

class Bgra8888Image extends AnalysisImage {
  final List<ImagePlane> planes;

  const Bgra8888Image({
    required super.height,
    required super.width,
    required this.planes,
    required super.format,
    required super.rotation,
  });

  Bgra8888Image.from(Map<String, dynamic> map)
      : this(
          height: map["height"],
          width: map["width"],
          planes: (map["planes"] as List<dynamic>)
              .map((e) => ImagePlane.from(Map<String, dynamic>.from(e)))
              .toList(),
          rotation: InputAnalysisImageRotation.values.byName(map["rotation"]),
          format: inputAnalysisImageFormatParser(map["format"]),
        );

  @override
  Size get croppedSize => Size(width.toDouble(), height.toDouble());

  Uint8List get bytes {
    final allBytes = WriteBuffer();
    for (final plane in planes) {
      allBytes.putUint8List(plane.bytes);
    }
    return allBytes.done().buffer.asUint8List();
  }
}

class Nv21Image extends AnalysisImage {
  final List<ImagePlane> planes;
  final Uint8List bytes;
  final Rect cropRect;

  const Nv21Image({
    required this.bytes,
    required this.cropRect,
    required super.height,
    required super.width,
    required this.planes,
    required super.format,
    required super.rotation,
  });

  Nv21Image.from(Map<String, dynamic> map)
      : this(
          bytes: map["nv21Image"],
          cropRect: Rect.fromLTRB(
            map["cropRect"]["left"].toDouble(),
            map["cropRect"]["top"].toDouble(),
            map["cropRect"]["right"].toDouble(),
            map["cropRect"]["bottom"].toDouble(),
          ),
          height: map["height"],
          width: map["width"],
          planes: (map["planes"] as List<dynamic>)
              .map((e) => ImagePlane.from(Map<String, dynamic>.from(e)))
              .toList(),
          rotation: InputAnalysisImageRotation.values.byName(map["rotation"]),
          format: inputAnalysisImageFormatParser(map["format"]),
        );

  @override
  Size get croppedSize => Size(
        // TODO Width and height of cropRect are inverted
        cropRect.size.height,
        cropRect.size.width,
      );
}

class Yuv420Image extends AnalysisImage {
  final List<ImagePlane> planes;
  final Rect cropRect;

  const Yuv420Image({
    required super.height,
    required super.width,
    required this.cropRect,
    required this.planes,
    required super.format,
    required super.rotation,
  });

  Yuv420Image.from(Map<String, dynamic> map)
      : this(
          cropRect: Rect.fromLTRB(
            map["cropRect"]["left"].toDouble(),
            map["cropRect"]["top"].toDouble(),
            map["cropRect"]["right"].toDouble(),
            map["cropRect"]["bottom"].toDouble(),
          ),
          height: map["height"],
          width: map["width"],
          planes: (map["planes"] as List<dynamic>)
              .map((e) => ImagePlane.from(Map<String, dynamic>.from(e)))
              .toList(),
          rotation: InputAnalysisImageRotation.values.byName(map["rotation"]),
          format: inputAnalysisImageFormatParser(map["format"]),
        );

  @override
  Size get croppedSize => Size(
        // TODO Width and height of cropRect are inverted
        cropRect.size.height,
        cropRect.size.width,
      );
}

class JpegImage extends AnalysisImage {
  final Uint8List bytes;
  final Rect? cropRect;

  const JpegImage({
    required this.bytes,
    required super.height,
    required super.width,
    required super.format,
    required super.rotation,
    required this.cropRect,
  });

  JpegImage.from(Map<String, dynamic> map)
      : this(
          bytes: map["jpegImage"],
          height: map["height"],
          width: map["width"],
          rotation: InputAnalysisImageRotation.values.byName(map["rotation"]),
          format: inputAnalysisImageFormatParser(map["format"]),
          cropRect: map["cropRect"] != null
              ? Rect.fromLTRB(
                  map["cropRect"]["left"].toDouble(),
                  map["cropRect"]["top"].toDouble(),
                  map["cropRect"]["right"].toDouble(),
                  map["cropRect"]["bottom"].toDouble(),
                )
              : null,
        );

  @override
  Size get croppedSize => cropRect != null
      ? Size(
          // TODO Width and height of cropRect are inverted
          cropRect!.size.height,
          cropRect!.size.width,
        )
      : Size(width.toDouble(), height.toDouble());
}



================================================
FILE: lib/src/orchestrator/models/analysis/analysis_image_ext.dart
================================================
part of 'analysis_image.dart';

extension Nv21Converter on Nv21Image {
  /// Converts the image to a [JpegImage], helping when one wants to display it
  /// or make JPEG treatment on it.
  Future<JpegImage> toJpeg({
    int quality = 100,
  }) async {
    final jpegQuality = quality.clamp(0, 100);
    final wrappedImage = await AnalysisImageUtils().nv21toJpeg(
      wrapped(),
      jpegQuality,
    );
    return wrappedImage.unwrap() as JpegImage;
  }
}

extension Yuv420Converter on Yuv420Image {
  /// Converts the image to a [JpegImage], helping when one wants to display it
  /// or make JPEG treatment on it.
  Future<JpegImage> toJpeg({
    int quality = 100,
  }) async {
    final jpegQuality = quality.clamp(0, 100);
    final wrappedImage = await AnalysisImageUtils().yuv420toJpeg(
      wrapped(),
      jpegQuality,
    );
    return wrappedImage.unwrap() as JpegImage;
  }

  /// Converts the image to a [Nv21Image].
  Future<Nv21Image> toNv21() async {
    final wrappedImage = await AnalysisImageUtils().yuv420toNv21(
      wrapped(),
    );
    return wrappedImage.unwrap() as Nv21Image;
  }
}

extension Bgra8888Converter on Bgra8888Image {
  Future<JpegImage> toJpeg({
    int quality = 100,
  }) async {
    // TODO Not implemented on the native side
    final jpegQuality = quality.clamp(0, 100);
    final wrappedValue = wrapped();
    final wrappedImage = await AnalysisImageUtils().bgra8888toJpeg(
      wrappedValue,
      jpegQuality,
    );
    return wrappedImage.unwrap() as JpegImage;
  }
}

/// Wraps (converts) the [AnalysisImage] to a [AnalysisImageWrapper].
/// [AnalysisImageWrapper] is generated by pigeon and should not be handled by
/// end-user.
extension AnalysisWrapper on AnalysisImage {
  AnalysisImageWrapper wrapped() {
    return AnalysisImageWrapper(
      height: height,
      width: width,
      format: AnalysisImageFormat.values.byName(format.name),
      bytes: when<Uint8List?>(
        nv21: (image) => image.bytes,
        bgra8888: (image) => image.bytes,
        jpeg: (image) => image.bytes,
        yuv420: (image) => null,
      ),
      planes: when<List<PlaneWrapper>?>(
        nv21: (image) => image.planes.map((e) => e.wrapped()).toList(),
        bgra8888: (image) => image.planes.map((e) => e.wrapped()).toList(),
        jpeg: (image) => null,
        yuv420: (image) => image.planes.map((e) => e.wrapped()).toList(),
      ),
      cropRect: when<CropRectWrapper?>(
        nv21: (image) => CropRectWrapper(
          left: image.cropRect.left.toInt(),
          top: image.cropRect.top.toInt(),
          width: image.cropRect.width.toInt(),
          height: image.cropRect.height.toInt(),
        ),
        bgra8888: (image) => null,
        jpeg: (image) => null,
        yuv420: (image) => CropRectWrapper(
          left: image.cropRect.left.toInt(),
          top: image.cropRect.top.toInt(),
          width: image.cropRect.width.toInt(),
          height: image.cropRect.height.toInt(),
        ),
      ),
      rotation: AnalysisRotation.values.byName(rotation.name),
    );
  }
}

/// Unwraps (converts) the [AnalysisImageWrapper] to a [AnalysisImage].
/// [AnalysisImageWrapper] is generated by pigeon and should not be handled by
/// end-user.
extension AnalysisUnwrapper on AnalysisImageWrapper {
  AnalysisImage unwrap() {
    switch (format) {
      case AnalysisImageFormat.yuv_420:
        return Yuv420Image(
          height: height,
          width: width,
          cropRect: Rect.fromLTWH(
            cropRect!.left.toDouble(),
            cropRect!.top.toDouble(),
            cropRect!.width.toDouble(),
            cropRect!.height.toDouble(),
          ),
          planes: planes!.map((p) => p!.unwrap()).toList(),
          format: InputAnalysisImageFormat.values.byName(format.name),
          rotation: InputAnalysisImageRotation.values.byName(rotation!.name),
        );
      case AnalysisImageFormat.bgra8888:
        return Bgra8888Image(
          height: height,
          width: width,
          planes: planes!.map((p) => p!.unwrap()).toList(),
          format: InputAnalysisImageFormat.values.byName(format.name),
          rotation: InputAnalysisImageRotation.values.byName(rotation!.name),
        );
      case AnalysisImageFormat.jpeg:
        return JpegImage(
          height: height,
          width: width,
          bytes: bytes!,
          cropRect: cropRect != null
              ? Rect.fromLTWH(
                  cropRect!.left.toDouble(),
                  cropRect!.top.toDouble(),
                  cropRect!.width.toDouble(),
                  cropRect!.height.toDouble(),
                )
              : null,
          format: InputAnalysisImageFormat.values.byName(format.name),
          rotation: InputAnalysisImageRotation.values.byName(rotation!.name),
        );
      case AnalysisImageFormat.nv21:
        return Nv21Image(
          height: height,
          width: width,
          bytes: bytes!,
          planes: planes!.map((p) => p!.unwrap()).toList(),
          cropRect: Rect.fromLTWH(
            cropRect!.left.toDouble(),
            cropRect!.top.toDouble(),
            cropRect!.width.toDouble(),
            cropRect!.height.toDouble(),
          ),
          format: InputAnalysisImageFormat.values.byName(format.name),
          rotation: InputAnalysisImageRotation.values.byName(rotation!.name),
        );
      case AnalysisImageFormat.unknown:
        throw "Unhandled format: $format";
    }
  }
}

/// Wraps (converts) the [ImagePlane] to a [PlaneWrapper].
/// [PlaneWrapper] is generated by pigeon and should not be handled by end-user.
extension PlaneWrap on ImagePlane {
  PlaneWrapper wrapped() {
    return PlaneWrapper(
      bytes: bytes,
      bytesPerRow: bytesPerRow,
      bytesPerPixel: bytesPerPixel,
      width: width,
      height: height,
    );
  }
}

/// Unwraps (converts) the [PlaneWrapper] to a [ImagePlane].
/// [PlaneWrapper] is generated by pigeon and should not be handled by end-user.
extension PlaneUnwrap on PlaneWrapper {
  ImagePlane unwrap() {
    return ImagePlane(
      width: width,
      height: height,
      bytes: bytes,
      bytesPerRow: bytesPerRow,
      bytesPerPixel: bytesPerPixel,
    );
  }
}



================================================
FILE: lib/src/orchestrator/models/analysis/image_plane.dart
================================================
import 'dart:typed_data';

class ImagePlane {
  // TODO: Android is now broken as I need to change args for iOS
  Uint8List bytes;
  int bytesPerRow;
  int? bytesPerPixel;
  int? height;
  int? width;

  ImagePlane({
    required this.bytes,
    required this.bytesPerRow,
    required this.bytesPerPixel,
    required this.height,
    required this.width,
  });

  factory ImagePlane.from(Map<String, dynamic> map) {
    return ImagePlane(
      bytes: map["bytes"],
      bytesPerRow: map["bytesPerRow"] ?? map["rowStride"],
      bytesPerPixel: map["pixelStride"],
      height: map["height"],
      width: map["width"],
    );
  }
}



================================================
FILE: lib/src/orchestrator/models/analysis/input_analysis.dart
================================================
enum InputAnalysisImageFormat { yuv_420, bgra8888, jpeg, nv21, unknown }

enum InputAnalysisImageRotation {
  rotation0deg,
  rotation90deg,
  rotation180deg,
  rotation270deg
}

InputAnalysisImageFormat inputAnalysisImageFormatParser(String value) {
  switch (value) {
    case 'yuv420':
    case 'yuv_420_888': // android.graphics.ImageFormat.YUV_420_888
      return InputAnalysisImageFormat.yuv_420;
    case 'bgra8888':
      return InputAnalysisImageFormat.bgra8888;
    case 'jpeg': // android.graphics.ImageFormat.JPEG
      return InputAnalysisImageFormat.jpeg;
    case 'nv21': // android.graphics.ImageFormat.nv21
      return InputAnalysisImageFormat.nv21;
    case 'rgba_8888':
      return InputAnalysisImageFormat.bgra8888;
  }
  return InputAnalysisImageFormat.unknown;
}



================================================
FILE: lib/src/orchestrator/models/filters/awesome_filter.dart
================================================
// ignore_for_file: non_constant_identifier_names

import 'dart:ui';

import 'package:colorfilter_generator/addons.dart';
import 'package:colorfilter_generator/colorfilter_generator.dart';
import 'package:colorfilter_generator/presets.dart';
import 'package:camerawesome/src/photofilters/filters/filters.dart'
    as photofilters;
import 'package:camerawesome/src/photofilters/filters/preset_filters.dart'
    as preset_filters;

// TODO: colorfilter_generator can be removed from dependencies
// find a way to do it with photofilters only
class AwesomeFilter {
  final String _name;
  final photofilters.Filter _outputFilter;
  final List<double> matrix;

  AwesomeFilter({
    required String name,
    required photofilters.Filter outputFilter,
    required this.matrix,
  })  : _name = name,
        _outputFilter = outputFilter;

  ColorFilter get preview => ColorFilter.matrix(matrix);

  photofilters.Filter get output => _outputFilter;

  String get name => _name;

  String get id => _name.toUpperCase().replaceAll(' ', '_');

  static AwesomeFilter get None => AwesomeFilter(
        name: 'Original',
        outputFilter: preset_filters.NoFilter(),
        matrix: PresetFilters.none.matrix,
      );

  static AwesomeFilter get AddictiveBlue => AwesomeFilter(
        name: 'Addictive Blue',
        outputFilter: preset_filters.AddictiveBlueFilter(),
        matrix: PresetFilters.addictiveBlue.matrix,
      );

  static AwesomeFilter get AddictiveRed => AwesomeFilter(
        name: 'Addictive Red',
        outputFilter: preset_filters.AddictiveRedFilter(),
        matrix: PresetFilters.addictiveRed.matrix,
      );

  static AwesomeFilter get Aden => AwesomeFilter(
        name: 'Aden',
        outputFilter: preset_filters.AdenFilter(),
        matrix: ColorFilterGenerator(
          name: 'Aden',
          filters: [
            ColorFilterAddons.addictiveColor(48, 30, 45),
            ColorFilterAddons.saturation(-0.2),
          ],
        ).matrix,
      );

  static AwesomeFilter get Amaro => AwesomeFilter(
        name: 'Amaro',
        outputFilter: preset_filters.AmaroFilter(),
        matrix: PresetFilters.amaro.matrix,
      );

  static AwesomeFilter get Ashby => AwesomeFilter(
        name: 'Ashby',
        outputFilter: preset_filters.AshbyFilter(),
        matrix: ColorFilterGenerator(
          name: 'Ashby',
          filters: [
            ColorFilterAddons.addictiveColor(45, 30, 15),
            ColorFilterAddons.brightness(0.1),
          ],
        ).matrix,
      );

  static AwesomeFilter get Brannan => AwesomeFilter(
        name: 'Brannan',
        outputFilter: preset_filters.BrannanFilter(),
        matrix: ColorFilterGenerator(
          name: 'Brannan',
          filters: [
            ColorFilterAddons.contrast(0.23),
            ColorFilterAddons.addictiveColor(7, 7, 25),
          ],
        ).matrix,
      );

  static AwesomeFilter get Brooklyn => AwesomeFilter(
        name: 'Brooklyn',
        outputFilter: preset_filters.BrooklynFilter(),
        matrix: ColorFilterGenerator(
          name: 'Brooklyn',
          filters: [
            ColorFilterAddons.sepia(0.4),
            ColorFilterAddons.brightness(-0.1),
            ColorFilterAddons.addictiveColor(25, 30, 42),
          ],
        ).matrix,
      );

  // static AwesomeFilter get Charmes => AwesomeFilter(
  //   name: 'Charmes',
  //       outputFilter: PhotoFilters.CharmesFilter(),
  //       matrix: PresetFilters.charmes.matrix,
  //     );

  static AwesomeFilter get Clarendon => AwesomeFilter(
        name: 'Clarendon',
        outputFilter: preset_filters.ClarendonFilter(),
        matrix: PresetFilters.clarendon.matrix,
      );

  static AwesomeFilter get Crema => AwesomeFilter(
        name: 'Crema',
        outputFilter: preset_filters.CremaFilter(),
        matrix: PresetFilters.crema.matrix,
      );

  static AwesomeFilter get Dogpatch => AwesomeFilter(
        name: 'Dogpatch',
        outputFilter: preset_filters.DogpatchFilter(),
        matrix: PresetFilters.dogpatch.matrix,
      );

  // static AwesomeFilter get Earlybird => AwesomeFilter(
  //   name: 'Earlybird',
  //       outputFilter: PhotoFilters.EarlybirdFilter(),
  //       matrix: PresetFilters.earlybird.matrix,
  //     );

  // static AwesomeFilter get f1977 => AwesomeFilter(
  //   name: '1977',
  //       outputFilter: PhotoFilters.F1977Filter(),
  //       matrix: PresetFilters.f1977.matrix,
  //     );

  static AwesomeFilter get Gingham => AwesomeFilter(
        name: 'Gingham',
        outputFilter: preset_filters.GinghamFilter(),
        matrix: PresetFilters.gingham.matrix,
      );

  static AwesomeFilter get Ginza => AwesomeFilter(
        name: 'Ginza',
        outputFilter: preset_filters.GinzaFilter(),
        matrix: PresetFilters.ginza.matrix,
      );

  static AwesomeFilter get Hefe => AwesomeFilter(
        name: 'Hefe',
        outputFilter: preset_filters.HefeFilter(),
        matrix: PresetFilters.hefe.matrix,
      );

  // static AwesomeFilter get Helena => AwesomeFilter(
  //   name: 'Helena',
  //       outputFilter: PhotoFilters.HelenaFilter(),
  //       matrix: PresetFilters.helena.matrix,
  //     );

  static AwesomeFilter get Hudson => AwesomeFilter(
        name: 'Hudson',
        outputFilter: preset_filters.HudsonFilter(),
        matrix: PresetFilters.hudson.matrix,
      );

  static AwesomeFilter get Inkwell => AwesomeFilter(
        name: 'Inkwell',
        outputFilter: preset_filters.InkwellFilter(),
        matrix: PresetFilters.inkwell.matrix,
      );

  static AwesomeFilter get Juno => AwesomeFilter(
        name: 'Juno',
        outputFilter: preset_filters.JunoFilter(),
        matrix: PresetFilters.juno.matrix,
      );

  // static AwesomeFilter get Kelvin => AwesomeFilter(
  //   name: 'Kelvin',
  //       outputFilter: PhotoFilters.KelvinFilter(),
  //       matrix: PresetFilters.kelvin.matrix,
  //     );

  static AwesomeFilter get Lark => AwesomeFilter(
        name: 'Lark',
        outputFilter: preset_filters.LarkFilter(),
        matrix: PresetFilters.lark.matrix,
      );

  static AwesomeFilter get LoFi => AwesomeFilter(
        name: 'Lo-Fi',
        outputFilter: preset_filters.LoFiFilter(),
        matrix: PresetFilters.loFi.matrix,
      );

  static AwesomeFilter get Ludwig => AwesomeFilter(
        name: 'Ludwig',
        outputFilter: preset_filters.LudwigFilter(),
        matrix: PresetFilters.ludwig.matrix,
      );

  // static AwesomeFilter get Maven => AwesomeFilter(
  //   name: 'Maven',
  //       outputFilter: PhotoFilters.MavenFilter(),
  //       matrix: PresetFilters.maven.matrix,
  //     );

  // static AwesomeFilter get Mayfair => AwesomeFilter(
  //   name: 'Mayfair',
  //       outputFilter: PhotoFilters.MayfairFilter(),
  //       matrix: PresetFilters.mayfair.matrix,
  //     );

  static AwesomeFilter get Moon => AwesomeFilter(
        name: 'Moon',
        outputFilter: preset_filters.MoonFilter(),
        matrix: PresetFilters.moon.matrix,
      );

  // static AwesomeFilter get Nashville => AwesomeFilter(
  //   name: 'Nashville',
  //       outputFilter: PhotoFilters.NashvilleFilter(),
  //       matrix: PresetFilters.nashville.matrix,
  //     );

  static AwesomeFilter get Perpetua => AwesomeFilter(
        name: 'Perpetua',
        outputFilter: preset_filters.PerpetuaFilter(),
        matrix: PresetFilters.perpetua.matrix,
      );

  static AwesomeFilter get Reyes => AwesomeFilter(
        name: 'Reyes',
        outputFilter: preset_filters.ReyesFilter(),
        matrix: PresetFilters.reyes.matrix,
      );

  // static AwesomeFilter get Rise => AwesomeFilter(
  //   name: 'Rise',
  //       outputFilter: PhotoFilters.RiseFilter(),
  //       matrix: PresetFilters.rise.matrix,
  //     );

  static AwesomeFilter get Sierra => AwesomeFilter(
        name: 'Sierra',
        outputFilter: preset_filters.SierraFilter(),
        matrix: PresetFilters.sierra.matrix,
      );

  // static AwesomeFilter get Skyline => AwesomeFilter(
  //   name: 'Skyline',
  //       outputFilter: PhotoFilters.SkylineFilter(),
  //       matrix: PresetFilters.skyline.matrix,
  //     );

  static AwesomeFilter get Slumber => AwesomeFilter(
        name: 'Slumber',
        outputFilter: preset_filters.SlumberFilter(),
        matrix: PresetFilters.slumber.matrix,
      );

  static AwesomeFilter get Stinson => AwesomeFilter(
        name: 'Stinson',
        outputFilter: preset_filters.StinsonFilter(),
        matrix: PresetFilters.stinson.matrix,
      );

  static AwesomeFilter get Sutro => AwesomeFilter(
        name: 'Sutro',
        outputFilter: preset_filters.SutroFilter(),
        matrix: PresetFilters.sutro.matrix,
      );

  // static AwesomeFilter get Toaster => AwesomeFilter(
  //   name: 'Toaster',
  //       outputFilter: PhotoFilters.ToasterFilter(),
  //       matrix: PresetFilters.toaster.matrix,
  //     );

  // static AwesomeFilter get Valencia => AwesomeFilter(
  //   name: 'Valencia',
  //       outputFilter: PhotoFilters.ValenciaFilter(),
  //       matrix: PresetFilters.valencia.matrix,
  //     );

  // static AwesomeFilter get Vesper => AwesomeFilter(
  //       name: 'Vesper',
  //       outputFilter: PhotoFilters.VesperFilter(),
  //       matrix: PresetFilters.vesper.matrix,
  //     );

  static AwesomeFilter get Walden => AwesomeFilter(
      name: 'Walden',
      outputFilter: preset_filters.WaldenFilter(),
      matrix: ColorFilterGenerator(
        name: "Walden",
        filters: [
          ColorFilterAddons.brightness(0.1),
          ColorFilterAddons.addictiveColor(45, 45, 0),
        ],
      ).matrix);

  static AwesomeFilter get Willow => AwesomeFilter(
        name: 'Willow',
        outputFilter: preset_filters.WillowFilter(),
        matrix: PresetFilters.willow.matrix,
      );

  static AwesomeFilter get XProII => AwesomeFilter(
        name: 'X-Pro II',
        outputFilter: preset_filters.XProIIFilter(),
        matrix: ColorFilterGenerator(
          name: "X-Pro II",
          filters: [
            ColorFilterAddons.addictiveColor(30, 30, 0),
            ColorFilterAddons.saturation(0.2),
            ColorFilterAddons.contrast(0.2),
            ColorFilterAddons.hue(0.03),
            ColorFilterAddons.brightness(0.04),
          ],
        ).matrix,
      );
}



================================================
FILE: lib/src/orchestrator/models/filters/awesome_filters.dart
================================================
import 'package:camerawesome/src/orchestrator/models/filters/awesome_filter.dart';

// some filters are commented because filter & preview are too different,
// we need to adjust them
List<AwesomeFilter> awesomePresetFiltersList = [
  AwesomeFilter.None,
  AwesomeFilter.AddictiveBlue,
  AwesomeFilter.AddictiveRed,
  AwesomeFilter.Aden,
  AwesomeFilter.Amaro,
  AwesomeFilter.Ashby,
  AwesomeFilter.Brannan,
  AwesomeFilter.Brooklyn,
  // AwesomeFilter.Charmes,
  AwesomeFilter.Clarendon,
  AwesomeFilter.Crema,
  AwesomeFilter.Dogpatch,
  // AwesomeFilter.Earlybird,
  // AwesomeFilter.f1977,
  AwesomeFilter.Gingham,
  AwesomeFilter.Ginza,
  AwesomeFilter.Hefe,
  // AwesomeFilter.Helena,
  AwesomeFilter.Hudson,
  AwesomeFilter.Inkwell,
  AwesomeFilter.Juno,
  // AwesomeFilter.Kelvin,
  AwesomeFilter.Lark,
  AwesomeFilter.LoFi,
  AwesomeFilter.Ludwig,
  // AwesomeFilter.Maven,
  // AwesomeFilter.Mayfair,
  AwesomeFilter.Moon,
  // AwesomeFilter.Nashville,
  AwesomeFilter.Perpetua,
  AwesomeFilter.Reyes,
  // AwesomeFilter.Rise,
  AwesomeFilter.Sierra,
  // AwesomeFilter.Skyline,
  AwesomeFilter.Slumber,
  AwesomeFilter.Stinson,
  AwesomeFilter.Sutro,
  // AwesomeFilter.Toaster,
  // AwesomeFilter.Valencia,
  // AwesomeFilter.Vesper,
  AwesomeFilter.Walden,
  AwesomeFilter.Willow,
  AwesomeFilter.XProII,
];



================================================
FILE: lib/src/orchestrator/states/analysis_camera_state.dart
================================================
import 'dart:ui';

import 'package:camerawesome/camerawesome_plugin.dart';
import 'package:camerawesome/pigeon.dart';
import 'package:camerawesome/src/orchestrator/camera_context.dart';

/// Only image analysis, no preview, no photo or video captures
class AnalysisCameraState extends CameraState {
  AnalysisCameraState({
    required CameraContext cameraContext,
  }) : super(cameraContext);

  factory AnalysisCameraState.from(CameraContext orchestrator) =>
      AnalysisCameraState(
        cameraContext: orchestrator,
      );

  @override
  CaptureMode get captureMode => CaptureMode.analysis_only;

  @override
  void setState(CaptureMode captureMode) {
    if (captureMode == CaptureMode.analysis_only) {
      return;
    }
    cameraContext.changeState(captureMode.toCameraState(cameraContext));
  }

  focus() {
    cameraContext.focus();
  }

  Future<void> focusOnPoint({
    required Offset flutterPosition,
    required PreviewSize pixelPreviewSize,
    required PreviewSize flutterPreviewSize,
    AndroidFocusSettings? androidFocusSettings,
  }) {
    return cameraContext.focusOnPoint(
      flutterPosition: flutterPosition,
      pixelPreviewSize: pixelPreviewSize,
      flutterPreviewSize: flutterPreviewSize,
      androidFocusSettings: androidFocusSettings,
    );
  }

  @override
  void dispose() {}
}



================================================
FILE: lib/src/orchestrator/states/camera_state.dart
================================================
import 'package:camerawesome/camerawesome_plugin.dart';
import 'package:camerawesome/pigeon.dart';
import 'package:camerawesome/src/orchestrator/camera_context.dart';
import 'package:flutter/foundation.dart';

typedef OnVideoMode = Function(VideoCameraState);

typedef OnPhotoMode = Function(PhotoCameraState);

typedef OnPreparingCamera = Function(PreparingCameraState);

typedef OnVideoRecordingMode = Function(VideoRecordingCameraState);

typedef OnPreviewMode = Function(PreviewCameraState);

typedef OnAnalysisOnlyMode = Function(AnalysisCameraState);

abstract class CameraState {
  // TODO Make private
  @protected
  CameraContext cameraContext;

  CameraState(this.cameraContext);

  abstract final CaptureMode? captureMode;

  when({
    OnVideoMode? onVideoMode,
    OnPhotoMode? onPhotoMode,
    OnPreparingCamera? onPreparingCamera,
    OnVideoRecordingMode? onVideoRecordingMode,
    OnPreviewMode? onPreviewMode,
    OnAnalysisOnlyMode? onAnalysisOnlyMode,
  }) {
    return switch (this) {
      (VideoCameraState state) => onVideoMode?.call(state),
      (PhotoCameraState state) => onPhotoMode?.call(state),
      (PreparingCameraState state) => onPreparingCamera?.call(state),
      (VideoRecordingCameraState state) => onVideoRecordingMode?.call(state),
      (PreviewCameraState state) => onPreviewMode?.call(state),
      (AnalysisCameraState state) => onAnalysisOnlyMode?.call(state),
      CameraState() => null,
    };
  }

  /// Closes streams depending on the current state
  void dispose();

  /// Use this stream to listen for capture state
  /// - while recording a video
  /// - while saving an image
  /// Accessible from all states
  Stream<MediaCapture?> get captureState$ => cameraContext.captureState$;

  MediaCapture? get captureState => cameraContext.captureState;

  /// Switch camera from [Sensors.BACK] [Sensors.front]
  /// All states can switch this
  Future<void> switchCameraSensor({
    CameraAspectRatios? aspectRatio,
    double? zoom,
    FlashMode? flash,
    SensorType? type,
  }) async {
    final previous = cameraContext.sensorConfig;

    SensorConfig next;
    if (previous.sensors.length <= 1) {
      next = SensorConfig.single(
        sensor: previous.sensors.first.position == SensorPosition.back
            ? Sensor.position(SensorPosition.front)
            : Sensor.position(SensorPosition.back),
        // TODO Initial values are not set in native when set like this
        aspectRatio: aspectRatio ?? CameraAspectRatios.ratio_4_3,
        zoom: zoom ?? 0.0,
        flashMode: flash ?? FlashMode.none,
      );
    } else {
      // switch all camera position in array by one like this:
      // old: [front, telephoto, wide]
      // new : [wide, front, telephoto]
      final newSensorsCopy = [...previous.sensors.nonNulls];
      next = SensorConfig.multiple(
        sensors: newSensorsCopy
          ..insert(0, newSensorsCopy.removeAt(newSensorsCopy.length - 1)),
        // TODO Initial values are not set in native when set like this
        aspectRatio: aspectRatio ?? CameraAspectRatios.ratio_4_3,
        zoom: zoom ?? 0.0,
        flashMode: flash ?? FlashMode.none,
      );
    }
    await cameraContext.setSensorConfig(next);

    // TODO Once initial sensorConfig is correctly handled, we can remove below lines
    if (aspectRatio != null) {
      await next.setAspectRatio(aspectRatio);
    }
    if (zoom != null) {
      await next.setZoom(zoom);
    }
    if (flash != null) {
      await next.setFlashMode(flash);
    }
  }

  void setSensorType(int cameraPosition, SensorType type, String deviceId) {
    final previous = cameraContext.sensorConfig;
    int sensorIndex = 0;
    final next = SensorConfig.multiple(
      sensors: previous.sensors
          .map((sensor) {
            if (sensorIndex == cameraPosition) {
              if (sensor.type == SensorType.trueDepth) {
                sensor.position = SensorPosition.front;
              } else {
                sensor.position = SensorPosition.back;
              }

              sensor.deviceId = deviceId;
              sensor.type = type;
            }

            sensorIndex++;
            return sensor;
          })
          .nonNulls
          .toList(),
      aspectRatio: previous.aspectRatio,
      flashMode: previous.flashMode,
      zoom: previous.zoom,
    );
    cameraContext.setSensorConfig(next);
  }

  // PigeonSensorType? _sensorTypeFromPigeon(SensorType type) {
  //   switch (type) {
  //     case SensorType.wideAngle:
  //       return PigeonSensorType.wideAngle;
  //     case SensorType.telephoto:
  //       return PigeonSensorType.telephoto;
  //     case SensorType.trueDepth:
  //       return PigeonSensorType.trueDepth;
  //     case SensorType.ultraWideAngle:
  //       return PigeonSensorType.ultraWideAngle;
  //     default:
  //       return null;
  //   }
  // }

  void toggleFilterSelector() {
    cameraContext.toggleFilterSelector();
  }

  Future<void> setFilter(AwesomeFilter newFilter) {
    return cameraContext.setFilter(newFilter);
  }

  /// The sensor config allows you to
  /// - set the [FlashMode]
  /// - set the zoom level
  /// - handle luminosity or get it
  /// - adjust brightness
  SensorConfig get sensorConfig => cameraContext.sensorConfig;

  Stream<SensorConfig> get sensorConfig$ => cameraContext.sensorConfig$;

  Stream<bool> get filterSelectorOpened$ => cameraContext.filterSelectorOpened$;

  Stream<AwesomeFilter> get filter$ => cameraContext.filter$;

  AwesomeFilter get filter => cameraContext.filterController.value;

  /// Switch to a state between
  /// - [CaptureMode.photo]
  /// - [CaptureMode.video]
  /// - [CaptureMode.ANALYSIS]
  void setState(CaptureMode captureMode);

  SaveConfig? get saveConfig => cameraContext.saveConfig;

  Future<PreviewSize> previewSize(int index) {
    return cameraContext.previewSize(index);
  }

  Future<SensorDeviceData> getSensors() {
    return cameraContext.getSensors();
  }

  Future<int?> previewTextureId(int cameraPosition) {
    return cameraContext.previewTextureId(cameraPosition);
  }

  AnalysisController? get analysisController =>
      cameraContext.analysisController;
}



================================================
FILE: lib/src/orchestrator/states/photo_camera_state.dart
================================================
import 'dart:ui';

import 'package:camerawesome/camerawesome_plugin.dart';
import 'package:camerawesome/pigeon.dart';
import 'package:camerawesome/src/orchestrator/camera_context.dart';
import 'package:camerawesome/src/orchestrator/states/handlers/filter_handler.dart';
import 'package:camerawesome/src/photofilters/filters/filters.dart';
import 'package:rxdart/rxdart.dart';

class PhotoFilterModel {
  PhotoFilterModel(this.captureRequest, this.filter);

  final CaptureRequest captureRequest;
  final Filter filter;
}

/// Callback to get the CaptureRequest after the photo has been taken
typedef OnPhotoCallback = Function(CaptureRequest request);

typedef OnPhotoFailedCallback = Function(Exception exception);

/// When Camera is in Image mode
class PhotoCameraState extends CameraState {
  PhotoCameraState({
    required CameraContext cameraContext,
    required this.filePathBuilder,
    required this.exifPreferences,
  }) : super(cameraContext) {
    _saveGpsLocationController =
        BehaviorSubject.seeded(exifPreferences.saveGPSLocation);
    saveGpsLocation$ = _saveGpsLocationController.stream;
  }

  factory PhotoCameraState.from(CameraContext orchestrator) => PhotoCameraState(
        cameraContext: orchestrator,
        filePathBuilder: orchestrator.saveConfig!.photoPathBuilder!,
        exifPreferences: orchestrator.exifPreferences,
      );

  final CaptureRequestBuilder filePathBuilder;

  final ExifPreferences exifPreferences;

  late final BehaviorSubject<bool> _saveGpsLocationController;
  late final Stream<bool> saveGpsLocation$;

  bool get saveGpsLocation => _saveGpsLocationController.value;

  Future<void> shouldSaveGpsLocation(bool saveGPS) async {
    final isGranted = await CamerawesomePlugin.setExifPreferences(
      ExifPreferences(saveGPSLocation: saveGPS),
    );

    // check if user location has been granted,
    // always return true if saveGPS is set to false
    if (isGranted) {
      exifPreferences.saveGPSLocation = saveGPS;
      _saveGpsLocationController.sink.add(saveGPS);
    }
  }

  @override
  CaptureMode get captureMode => CaptureMode.photo;

  /// Photos taken are in JPEG format. [filePath] must end with .jpg
  ///
  /// You can listen to [cameraSetup.mediaCaptureStream] to get updates
  /// of the photo capture (capturing, success/failure)
  Future<CaptureRequest> takePhoto({
    OnPhotoCallback? onPhoto,
    OnPhotoFailedCallback? onPhotoFailed,
  }) async {
    CaptureRequest captureRequest =
        await filePathBuilder(sensorConfig.sensors..nonNulls.toList());
    final mediaCapture = MediaCapture.capturing(captureRequest: captureRequest);
    if (!mediaCapture.isPicture) {
      throw ("CaptureRequest must be a picture. ${captureRequest.when(
        single: (single) => single.file!.path,
        multiple: (multiple) => multiple.fileBySensor.values.first!.path,
      )}");
    }
    _mediaCapture = mediaCapture;
    try {
      final succeeded = await CamerawesomePlugin.takePhoto(captureRequest);
      if (succeeded) {
        await FilterHandler().apply(
          captureRequest: captureRequest,
          filter: filter,
        );

        _mediaCapture = MediaCapture.success(captureRequest: captureRequest);
        onPhoto?.call(captureRequest);
      } else {
        _mediaCapture = MediaCapture.failure(captureRequest: captureRequest);
        onPhotoFailed?.call(Exception("Failed to take photo"));
      }
    } on Exception catch (e) {
      _mediaCapture = MediaCapture.failure(
        captureRequest: captureRequest,
        exception: e,
      );
    }
    return captureRequest;
  }

  bool get hasFilters => cameraContext.availableFilters?.isNotEmpty ?? false;

  List<AwesomeFilter>? get availableFilters =>
      cameraContext.availableFilters?.toList();

  /// PRIVATES

  set _mediaCapture(MediaCapture media) {
    if (!cameraContext.mediaCaptureController.isClosed) {
      cameraContext.mediaCaptureController.add(media);
    }
  }

  @override
  void setState(CaptureMode captureMode) {
    if (captureMode == CaptureMode.photo) {
      return;
    }
    cameraContext.changeState(captureMode.toCameraState(cameraContext));
  }

  @override
  void dispose() {
    _saveGpsLocationController.close();
  }

  focus() {
    cameraContext.focus();
  }

  Future<void> focusOnPoint({
    required Offset flutterPosition,
    required PreviewSize pixelPreviewSize,
    required PreviewSize flutterPreviewSize,
    AndroidFocusSettings? androidFocusSettings,
  }) {
    return cameraContext.focusOnPoint(
      flutterPosition: flutterPosition,
      pixelPreviewSize: pixelPreviewSize,
      flutterPreviewSize: flutterPreviewSize,
      androidFocusSettings: androidFocusSettings,
    );
  }
}



================================================
FILE: lib/src/orchestrator/states/preparing_camera_state.dart
================================================
import 'dart:async';
import 'dart:io';

import 'package:camerawesome/camerawesome_plugin.dart';
import 'package:camerawesome/pigeon.dart';
import 'package:camerawesome/src/orchestrator/exceptions/camera_states_exceptions.dart';
import 'package:camerawesome/src/orchestrator/models/camera_physical_button.dart';

/// When is not ready
class PreparingCameraState extends CameraState {
  /// this is the next state we are preparing to
  final CaptureMode nextCaptureMode;

  /// plugin user can execute some code once the permission has been granted
  final OnPermissionsResult? onPermissionsResult;

  PreparingCameraState(
    super.cameraContext,
    this.nextCaptureMode, {
    this.onPermissionsResult,
  });

  @override
  CaptureMode? get captureMode => null;

  Future<void> start() async {
    final filter = cameraContext.filterController.valueOrNull;
    if (filter != null) {
      await setFilter(filter);
    }
    switch (nextCaptureMode) {
      case CaptureMode.photo:
        await _startPhotoMode();
        break;
      case CaptureMode.video:
        await _startVideoMode();
        break;
      case CaptureMode.preview:
        await _startPreviewMode();
        break;
      case CaptureMode.analysis_only:
        await _startAnalysisMode();
        break;
    }
    await cameraContext.analysisController?.setup();
    if (nextCaptureMode == CaptureMode.analysis_only) {
      // Analysis controller needs to be setup before going to AnalysisCameraState
      cameraContext.changeState(AnalysisCameraState.from(cameraContext));
    }

    if (cameraContext.enablePhysicalButton) {
      initPhysicalButton();
    }
  }

  /// subscription for permissions
  StreamSubscription? _permissionStreamSub;

  /// subscription for physical button
  StreamSubscription? _physicalButtonStreamSub;

  // FIXME: Remove enableImageStream & enablePhysicalButton options here
  Future<void> initPermissions(
    SensorConfig sensorConfig, {
    required bool enableImageStream,
    required bool enablePhysicalButton,
  }) async {
    // wait user accept permissions to init widget completely on android
    if (Platform.isAndroid) {
      _permissionStreamSub =
          CamerawesomePlugin.listenPermissionResult()!.listen(
        (res) {
          if (res && !_isReady) {
            _init(
              enableImageStream: enableImageStream,
              enablePhysicalButton: enablePhysicalButton,
            );
          }
          if (onPermissionsResult != null) {
            onPermissionsResult!(res);
          }
        },
      );
    }
    final grantedPermissions =
        await CamerawesomePlugin.checkAndRequestPermissions(
      cameraContext.exifPreferences.saveGPSLocation,
      checkCameraPermissions: true,
      checkMicrophonePermissions:
          cameraContext.initialCaptureMode == CaptureMode.video,
    );
    if (cameraContext.exifPreferences.saveGPSLocation &&
        !(grantedPermissions?.contains(CamerAwesomePermission.location) ==
            true)) {
      cameraContext.exifPreferences = ExifPreferences(saveGPSLocation: false);
      cameraContext.state
          .when(onPhotoMode: (pm) => pm.shouldSaveGpsLocation(false));
    }
    if (onPermissionsResult != null) {
      onPermissionsResult!(
          grantedPermissions?.hasRequiredPermissions() == true);
    }
  }

  void initPhysicalButton() {
    _physicalButtonStreamSub?.cancel();
    _physicalButtonStreamSub =
        CamerawesomePlugin.listenPhysicalButton()!.listen(
      (res) async {
        if (res == CameraPhysicalButton.volume_down ||
            res == CameraPhysicalButton.volume_up) {
          cameraContext.state.when(
            onPhotoMode: (pm) => pm.takePhoto(),
            onVideoMode: (vm) => vm.startRecording(),
            onVideoRecordingMode: (vrm) => vrm.stopRecording(),
          );
        }
      },
    );
  }

  @override
  void setState(CaptureMode captureMode) {
    throw CameraNotReadyException(
      message:
          '''You can't change current state while camera is in PreparingCameraState''',
    );
  }

  /////////////////////////////////////
  // PRIVATES
  /////////////////////////////////////

  Future _startVideoMode() async {
    await Future.delayed(const Duration(milliseconds: 500));
    await _init(
      enableImageStream: cameraContext.imageAnalysisEnabled,
      enablePhysicalButton: cameraContext.enablePhysicalButton,
    );
    cameraContext.changeState(VideoCameraState.from(cameraContext));

    return CamerawesomePlugin.start();
  }

  Future _startPhotoMode() async {
    await Future.delayed(const Duration(milliseconds: 500));
    await _init(
      enableImageStream: cameraContext.imageAnalysisEnabled,
      enablePhysicalButton: cameraContext.enablePhysicalButton,
    );
    cameraContext.changeState(PhotoCameraState.from(cameraContext));

    return CamerawesomePlugin.start();
  }

  Future _startPreviewMode() async {
    await Future.delayed(const Duration(milliseconds: 500));
    await _init(
      enableImageStream: cameraContext.imageAnalysisEnabled,
      enablePhysicalButton: cameraContext.enablePhysicalButton,
    );
    cameraContext.changeState(PreviewCameraState.from(cameraContext));

    return CamerawesomePlugin.start();
  }

  Future _startAnalysisMode() async {
    await Future.delayed(const Duration(milliseconds: 500));
    await _init(
      enableImageStream: cameraContext.imageAnalysisEnabled,
      enablePhysicalButton: cameraContext.enablePhysicalButton,
    );

    // On iOS, we need to start the camera to get the first frame because there
    // is no "AnalysisMode" at all.
    if (Platform.isIOS) {
      return CamerawesomePlugin.start();
    }
  }

  bool _isReady = false;

  // TODO Refactor this (make it stream providing state)
  Future<bool> _init({
    required bool enableImageStream,
    required bool enablePhysicalButton,
  }) async {
    initPermissions(
      sensorConfig,
      enableImageStream: enableImageStream,
      enablePhysicalButton: enablePhysicalButton,
    );
    await CamerawesomePlugin.init(
      sensorConfig,
      enableImageStream,
      enablePhysicalButton,
      captureMode: nextCaptureMode,
      exifPreferences: cameraContext.exifPreferences,
      videoOptions: saveConfig?.videoOptions,
      mirrorFrontCamera: saveConfig?.mirrorFrontCamera ?? false,
    );
    _isReady = true;
    return _isReady;
  }

  @override
  void dispose() {
    _permissionStreamSub?.cancel();
    _physicalButtonStreamSub?.cancel();
  }
}



================================================
FILE: lib/src/orchestrator/states/preview_camera_state.dart
================================================
import 'dart:ui';

import 'package:camerawesome/camerawesome_plugin.dart';
import 'package:camerawesome/pigeon.dart';
import 'package:camerawesome/src/orchestrator/camera_context.dart';

/// Show the preview with optional image analysis, no photo or video captures
class PreviewCameraState extends CameraState {
  PreviewCameraState({
    required CameraContext cameraContext,
  }) : super(cameraContext);

  factory PreviewCameraState.from(CameraContext orchestrator) =>
      PreviewCameraState(
        cameraContext: orchestrator,
      );

  @override
  CaptureMode get captureMode => CaptureMode.preview;

  @override
  void setState(CaptureMode captureMode) {
    if (captureMode == CaptureMode.preview) {
      return;
    }
    cameraContext.changeState(captureMode.toCameraState(cameraContext));
  }

  focus() {
    cameraContext.focus();
  }

  Future<void> focusOnPoint({
    required Offset flutterPosition,
    required PreviewSize pixelPreviewSize,
    required PreviewSize flutterPreviewSize,
    AndroidFocusSettings? androidFocusSettings,
  }) {
    return cameraContext.focusOnPoint(
      flutterPosition: flutterPosition,
      pixelPreviewSize: pixelPreviewSize,
      flutterPreviewSize: flutterPreviewSize,
      androidFocusSettings: androidFocusSettings,
    );
  }

  @override
  void dispose() {}
}



================================================
FILE: lib/src/orchestrator/states/states.dart
================================================
export 'analysis_camera_state.dart';
export 'camera_state.dart';
export 'photo_camera_state.dart';
export 'preparing_camera_state.dart';
export 'preview_camera_state.dart';
export 'video_camera_recording_state.dart';
export 'video_camera_state.dart';



================================================
FILE: lib/src/orchestrator/states/video_camera_recording_state.dart
================================================
import 'dart:ui';

import 'package:camerawesome/camerawesome_plugin.dart';
import 'package:camerawesome/pigeon.dart';
import 'package:camerawesome/src/logger.dart';
import 'package:camerawesome/src/orchestrator/camera_context.dart';

/// Callback to get the CaptureRequest after the video has been taken
typedef OnVideoCallback = Function(CaptureRequest request);

/// Callback when video recording failed
typedef OnVideoFailedCallback = Function(Exception exception);

/// When Camera is in Video mode
class VideoRecordingCameraState extends CameraState {
  VideoRecordingCameraState({
    required CameraContext cameraContext,
    required this.filePathBuilder,
  }) : super(cameraContext);

  factory VideoRecordingCameraState.from(CameraContext orchestrator) =>
      VideoRecordingCameraState(
        cameraContext: orchestrator,
        filePathBuilder: orchestrator.saveConfig!.videoPathBuilder!,
      );

  final CaptureRequestBuilder filePathBuilder;

  @override
  void setState(CaptureMode captureMode) {
    printLog('''
      warning: You must stop recording before changing state.  
    ''');
  }

  @override
  CaptureMode get captureMode => CaptureMode.video;

  /// Pauses a video recording.
  /// [startRecording] must have been called before.
  /// Call [resumeRecording] to resume the capture.
  Future<void> pauseRecording(MediaCapture currentCapture) async {
    if (!currentCapture.isVideo) {
      throw "Trying to pause a video while currentCapture is not a video (${currentCapture.captureRequest.when(
        single: (single) => single.file!.path,
        multiple: (multiple) => multiple.fileBySensor.values.first!.path,
      )})";
    }
    if (currentCapture.status != MediaCaptureStatus.capturing) {
      throw "Trying to pause a media capture in status ${currentCapture.status} instead of ${MediaCaptureStatus.capturing}";
    }
    await CamerawesomePlugin.pauseVideoRecording();
    _mediaCapture = MediaCapture.capturing(
        captureRequest: currentCapture.captureRequest,
        videoState: VideoState.paused);
  }

  /// Resumes a video recording.
  /// [pauseRecording] must have been called before.
  Future<void> resumeRecording(MediaCapture currentCapture) async {
    if (!currentCapture.isVideo) {
      throw "Trying to pause a video while currentCapture is not a video (${currentCapture.captureRequest.when(
        single: (single) => single.file!.path,
        multiple: (multiple) => multiple.fileBySensor.values.first!.path,
      )})";
    }
    if (currentCapture.status != MediaCaptureStatus.capturing) {
      throw "Trying to pause a media capture in status ${currentCapture.status} instead of ${MediaCaptureStatus.capturing}";
    }
    await CamerawesomePlugin.resumeVideoRecording();
    _mediaCapture = MediaCapture.capturing(
      captureRequest: currentCapture.captureRequest,
      videoState: VideoState.resumed,
    );
  }

  // TODO Video recording might end due to other reasons (not enough space left...)
  // CameraAwesome is not notified in these cases atm
  Future<void> stopRecording({
    OnVideoCallback? onVideo,
    OnVideoFailedCallback? onVideoFailed,
  }) async {
    var currentCapture = cameraContext.mediaCaptureController.value;
    if (currentCapture == null) {
      return;
    }
    final result = await CamerawesomePlugin.stopRecordingVideo();
    if (result) {
      _mediaCapture = MediaCapture.success(
        captureRequest: currentCapture.captureRequest,
      );
      onVideo?.call(currentCapture.captureRequest);
    } else {
      _mediaCapture = MediaCapture.failure(
        captureRequest: currentCapture.captureRequest,
      );
      onVideoFailed?.call(Exception("Error while stop recording"));
    }
    await CamerawesomePlugin.setCaptureMode(CaptureMode.video);
    cameraContext.changeState(VideoCameraState.from(cameraContext));
  }

  /// If video recording should [enableAudio].
  Future<void> enableAudio(bool enableAudio) async {
    printLog('''
      warning: EnableAudio has no effect when recording 
    ''');
  }

  /// PRIVATES
  set _mediaCapture(MediaCapture media) {
    if (!cameraContext.mediaCaptureController.isClosed) {
      cameraContext.mediaCaptureController.add(media);
    }
  }

  @override
  void dispose() {
    // Nothing to do
  }

  focus() {
    cameraContext.focus();
  }

  Future<void> focusOnPoint({
    required Offset flutterPosition,
    required PreviewSize pixelPreviewSize,
    required PreviewSize flutterPreviewSize,
    AndroidFocusSettings? androidFocusSettings,
  }) {
    return cameraContext.focusOnPoint(
      flutterPosition: flutterPosition,
      pixelPreviewSize: pixelPreviewSize,
      flutterPreviewSize: flutterPreviewSize,
      androidFocusSettings: androidFocusSettings,
    );
  }
}



================================================
FILE: lib/src/orchestrator/states/video_camera_state.dart
================================================
import 'dart:ui';

import 'package:camerawesome/camerawesome_plugin.dart';
import 'package:camerawesome/pigeon.dart';
import 'package:camerawesome/src/orchestrator/camera_context.dart';

/// When Camera is in Video mode
class VideoCameraState extends CameraState {
  VideoCameraState({
    required CameraContext cameraContext,
    required this.filePathBuilder,
  }) : super(cameraContext);

  factory VideoCameraState.from(CameraContext cameraContext) =>
      VideoCameraState(
        cameraContext: cameraContext,
        filePathBuilder: cameraContext.saveConfig!.videoPathBuilder!,
      );

  final CaptureRequestBuilder filePathBuilder;

  @override
  void setState(CaptureMode captureMode) {
    if (captureMode == CaptureMode.video) {
      return;
    }
    cameraContext.changeState(captureMode.toCameraState(cameraContext));
  }

  @override
  CaptureMode get captureMode => CaptureMode.video;

  /// You can listen to [cameraSetup.mediaCaptureStream] to get updates
  /// of the photo capture (capturing, success/failure)
  Future<CaptureRequest> startRecording() async {
    CaptureRequest captureRequest =
        await filePathBuilder(sensorConfig.sensors.nonNulls.toList());
    _mediaCapture = MediaCapture.capturing(
        captureRequest: captureRequest, videoState: VideoState.started);
    try {
      await CamerawesomePlugin.recordVideo(captureRequest);
    } on Exception catch (e) {
      _mediaCapture =
          MediaCapture.failure(captureRequest: captureRequest, exception: e);
    }
    cameraContext.changeState(VideoRecordingCameraState.from(cameraContext));
    return captureRequest;
  }

  /// If the video recording should [enableAudio].
  /// This method applies to the next recording. If a recording is ongoing, it will not be affected.
  // TODO Add ability to mute temporarly a video recording
  Future<void> enableAudio(bool enableAudio) {
    return CamerawesomePlugin.setAudioMode(enableAudio);
  }

  /// PRIVATES

  set _mediaCapture(MediaCapture media) {
    if (!cameraContext.mediaCaptureController.isClosed) {
      cameraContext.mediaCaptureController.add(media);
    }
  }

  @override
  void dispose() {
    // Nothing to do
  }

  focus() {
    cameraContext.focus();
  }

  Future<void> focusOnPoint({
    required Offset flutterPosition,
    required PreviewSize pixelPreviewSize,
    required PreviewSize flutterPreviewSize,
    AndroidFocusSettings? androidFocusSettings,
  }) {
    return cameraContext.focusOnPoint(
      flutterPosition: flutterPosition,
      pixelPreviewSize: pixelPreviewSize,
      flutterPreviewSize: flutterPreviewSize,
      androidFocusSettings: androidFocusSettings,
    );
  }
}



================================================
FILE: lib/src/orchestrator/states/handlers/filter_handler.dart
================================================
import 'dart:io';
import 'dart:isolate';
import 'dart:typed_data';

import 'package:camerawesome/src/orchestrator/file/content/file_content.dart';
import 'package:image/image.dart' as img;
import 'package:camerawesome/camerawesome_plugin.dart';

class FilterHandler {
  Isolate? photoFilterIsolate;

  Future<void> apply({
    required CaptureRequest captureRequest,
    required AwesomeFilter filter,
  }) async {
    if (Platform.isIOS && filter.id != AwesomeFilter.None.id) {
      photoFilterIsolate?.kill(priority: Isolate.immediate);

      ReceivePort port = ReceivePort();
      photoFilterIsolate = await Isolate.spawn<PhotoFilterModel>(
        applyFilter,
        PhotoFilterModel(captureRequest, filter.output),
        onExit: port.sendPort,
      );
      await port.first;

      photoFilterIsolate?.kill(priority: Isolate.immediate);
    }
  }
}

Future<CaptureRequest> applyFilter(PhotoFilterModel model) async {
  final files = model.captureRequest.when(
    single: (single) => [single.file],
    multiple: (multiple) => multiple.fileBySensor.values.toList(),
  );
  FileContent fileContent = FileContent();
  for (final f in files) {
    // f is expected to not be null since the picture should have already been taken
    final img.Image? image = img.decodeJpg((await fileContent.read(f!))!);
    if (image == null) {
      throw MediaCapture.failure(
        exception: Exception("could not decode image ${f.path}"),
        captureRequest: model.captureRequest,
      );
    }

    final pixels = image.getBytes();
    model.filter.apply(pixels, image.width, image.height);
    final img.Image out = img.Image.fromBytes(
      width: image.width,
      height: image.height,
      bytes: pixels.buffer,
    );

    final List<int>? encodedImage = img.encodeNamedImage(f.path, out);
    if (encodedImage == null) {
      throw MediaCapture.failure(
        exception: Exception("could not encode image ${f.path}"),
        captureRequest: model.captureRequest,
      );
    }
    await fileContent.write(f, Uint8List.fromList(encodedImage));
  }
  return model.captureRequest;
}



================================================
FILE: lib/src/photofilters/rgba_model.dart
================================================
class RGBA extends Object {
  final int red;
  final int green;
  final int blue;
  final int alpha;

  const RGBA({this.red = 0, this.green = 0, this.blue = 0, this.alpha = 0});
}



================================================
FILE: lib/src/photofilters/filters/color_filters.dart
================================================
import 'dart:typed_data';

import 'package:camerawesome/src/photofilters/filters/filters.dart';
import 'package:camerawesome/src/photofilters/rgba_model.dart';

///The [ColorSubFilter] class is the abstract class to define any ColorSubFilter.
abstract class ColorSubFilter extends SubFilter {
  ///Apply the [SubFilter] to an Image.
  RGBA applyFilter(RGBA color);
}

///The [ColorFilter] class to define a Filter which will applied to each color, consists of multiple [SubFilter]s
class ColorFilter extends Filter {
  List<ColorSubFilter> subFilters;
  ColorFilter({required super.name}) : subFilters = [];

  @override
  void apply(Uint8List pixels, int width, int height) {
    // Remove alpha channel to support image 4 package
    // we changed iteration from 4 to 3
    for (int i = 0; i < pixels.length; i += 3) {
      RGBA color = RGBA(
        red: pixels[i],
        green: pixels[i + 1],
        blue: pixels[i + 2],
        // alpha: pixels[i + 3],
        alpha: 255,
      );
      for (ColorSubFilter subFilter in subFilters) {
        color = subFilter.applyFilter(color);
      }
      pixels[i] = color.red;
      pixels[i + 1] = color.green;
      pixels[i + 2] = color.blue;
      // pixels[i + 3] = color.alpha;
    }
  }

  void addSubFilter(ColorSubFilter subFilter) {
    subFilters.add(subFilter);
  }

  void addSubFilters(List<ColorSubFilter> subFilters) {
    this.subFilters.addAll(subFilters);
  }
}



================================================
FILE: lib/src/photofilters/filters/convolution_filters.dart
================================================
import 'package:camerawesome/src/photofilters/filters/subfilters.dart';

import 'package:camerawesome/src/photofilters/utils/convolution_kernels.dart';
import 'package:camerawesome/src/photofilters/filters/image_filters.dart';

final presetConvolutionFiltersList = [
  ImageFilter(name: "Identity")
    ..addSubFilter(ConvolutionSubFilter.fromKernel(identityKernel)),
  ImageFilter(name: "Sharpen")
    ..addSubFilter(ConvolutionSubFilter.fromKernel(sharpenKernel)),
  ImageFilter(name: "Emboss")
    ..addSubFilter(ConvolutionSubFilter.fromKernel(embossKernel)),
  ImageFilter(name: "Colored Edge Detection")
    ..subFilters
        .add(ConvolutionSubFilter.fromKernel(coloredEdgeDetectionKernel)),
  ImageFilter(name: "Edge Detection Medium")
    ..subFilters
        .add(ConvolutionSubFilter.fromKernel(edgeDetectionMediumKernel)),
  ImageFilter(name: "Edge Detection Hard")
    ..addSubFilter(ConvolutionSubFilter.fromKernel(edgeDetectionHardKernel)),
  ImageFilter(name: "Blur")
    ..addSubFilter(ConvolutionSubFilter.fromKernel(blurKernel)),
  ImageFilter(name: "Guassian 3x3")
    ..addSubFilter(ConvolutionSubFilter.fromKernel(guassian3x3Kernel)),
  ImageFilter(name: "Guassian 5x5")
    ..addSubFilter(ConvolutionSubFilter.fromKernel(guassian5x5Kernel)),
  ImageFilter(name: "Guassian 7x7")
    ..addSubFilter(ConvolutionSubFilter.fromKernel(guassian7x7Kernel)),
  ImageFilter(name: "Mean 3x3")
    ..addSubFilter(ConvolutionSubFilter.fromKernel(mean3x3Kernel)),
  ImageFilter(name: "Mean 5x5")
    ..addSubFilter(ConvolutionSubFilter.fromKernel(mean5x5Kernel)),
  ImageFilter(name: "Low Pass 3x3")
    ..addSubFilter(ConvolutionSubFilter.fromKernel(lowPass3x3Kernel)),
  ImageFilter(name: "Low Pass 5x5")
    ..addSubFilter(ConvolutionSubFilter.fromKernel(lowPass5x5Kernel)),
  ImageFilter(name: "High Pass 3x3")
    ..addSubFilter(ConvolutionSubFilter.fromKernel(highPass3x3Kernel)),
];



================================================
FILE: lib/src/photofilters/filters/filters.dart
================================================
import 'dart:typed_data';

///The [Filter] class to define a Filter consists of multiple [SubFilter]s
abstract class Filter extends Object {
  final String name;
  const Filter({required this.name});

  ///Apply the [SubFilter] to an Image.
  void apply(Uint8List pixels, int width, int height);
}

///The [SubFilter] class is the abstract class to define any SubFilter.
abstract class SubFilter extends Object {}



================================================
FILE: lib/src/photofilters/filters/image_filters.dart
================================================
import 'dart:typed_data';

import 'package:camerawesome/src/photofilters/filters/filters.dart';

///The [ImageSubFilter] class is the abstract class to define any ImageSubFilter.
mixin ImageSubFilter on SubFilter {
  ///Apply the [SubFilter] to an Image.
  void apply(Uint8List pixels, int width, int height);
}

class ImageFilter extends Filter {
  final List<ImageSubFilter> subFilters;

  ImageFilter({required super.name}) : subFilters = [];

  ///Apply the [SubFilter] to an Image.
  @override
  void apply(Uint8List pixels, int width, int height) {
    for (ImageSubFilter subFilter in subFilters) {
      subFilter.apply(pixels, width, height);
    }
  }

  void addSubFilter(ImageSubFilter subFilter) {
    subFilters.add(subFilter);
  }

  void addSubFilters(List<ImageSubFilter> subFilters) {
    this.subFilters.addAll(subFilters);
  }
}



================================================
FILE: lib/src/photofilters/filters/preset_filters.dart
================================================
import 'dart:typed_data';

import 'package:camerawesome/src/photofilters/filters/color_filters.dart';
import 'package:camerawesome/src/photofilters/filters/subfilters.dart';
import 'package:camerawesome/src/photofilters/filters/filters.dart';

// NoFilter: No filter
class NoFilter extends ColorFilter {
  NoFilter() : super(name: "No Filter");

  @override
  void apply(Uint8List pixels, int width, int height) {
    // Do nothing
  }
}

// Clarendon: adds light to lighter areas and dark to darker areas
class ClarendonFilter extends ColorFilter {
  ClarendonFilter() : super(name: "Clarendon") {
    subFilters.add(BrightnessSubFilter(.1));
    subFilters.add(ContrastSubFilter(.1));
    subFilters.add(SaturationSubFilter(.15));
  }
}

class AddictiveRedFilter extends ColorFilter {
  AddictiveRedFilter() : super(name: "AddictiveRed") {
    subFilters.add(AddictiveColorSubFilter(50, 0, 0));
  }
}

class AddictiveBlueFilter extends ColorFilter {
  AddictiveBlueFilter() : super(name: "AddictiveBlue") {
    subFilters.add(AddictiveColorSubFilter(0, 0, 50));
  }
}

// Gingham: Vintage-inspired, taking some color out
class GinghamFilter extends ColorFilter {
  GinghamFilter() : super(name: "Gingham") {
    subFilters.add(SepiaSubFilter(.04));
    subFilters.add(ContrastSubFilter(-.15));
  }
}

// Moon: B/W, increase brightness and decrease contrast
class MoonFilter extends ColorFilter {
  MoonFilter() : super(name: "Moon") {
    subFilters.add(GrayScaleSubFilter());
    subFilters.add(ContrastSubFilter(-.04));
    subFilters.add(BrightnessSubFilter(0.1));
  }
}

// Lark: Brightens and intensifies colours but not red hues
class LarkFilter extends ColorFilter {
  LarkFilter() : super(name: "Lark") {
    subFilters.add(BrightnessSubFilter(0.08));
    subFilters.add(GrayScaleSubFilter());
    subFilters.add(ContrastSubFilter(-.04));
  }
}

// Reyes: a vintage filter, gives your photos a “dusty” look
class ReyesFilter extends ColorFilter {
  ReyesFilter() : super(name: "Reyes") {
    subFilters.add(SepiaSubFilter(0.4));
    subFilters.add(BrightnessSubFilter(0.13));
    subFilters.add(ContrastSubFilter(-.05));
  }
}

// Juno: Brightens colors, and intensifies red and yellow hues
class JunoFilter extends ColorFilter {
  JunoFilter() : super(name: "Juno") {
    subFilters.add(RGBScaleSubFilter(1.01, 1.04, 1));
    subFilters.add(SaturationSubFilter(0.3));
  }
}

// Slumber: Desaturates the image as well as adds haze for a retro, dreamy look – with an emphasis on blacks and blues
class SlumberFilter extends ColorFilter {
  SlumberFilter() : super(name: "Slumber") {
    subFilters.add(BrightnessSubFilter(.1));
    subFilters.add(SaturationSubFilter(-0.5));
  }
}

// Crema: Adds a creamy look that both warms and cools the image
class CremaFilter extends ColorFilter {
  CremaFilter() : super(name: "Crema") {
    subFilters.add(RGBScaleSubFilter(1.04, 1, 1.02));
    subFilters.add(SaturationSubFilter(-0.05));
  }
}

// Ludwig: A slight hint of desaturation that also enhances light
class LudwigFilter extends ColorFilter {
  LudwigFilter() : super(name: "Ludwig") {
    subFilters.add(BrightnessSubFilter(.05));
    subFilters.add(SaturationSubFilter(-0.03));
  }
}

// Aden: This filter gives a blue/pink natural look
class AdenFilter extends ColorFilter {
  AdenFilter() : super(name: "Aden") {
    subFilters.add(RGBOverlaySubFilter(228, 130, 225, 0.13));
    subFilters.add(SaturationSubFilter(-0.2));
  }
}

// Perpetua: Adding a pastel look, this filter is ideal for portraits
class PerpetuaFilter extends ColorFilter {
  PerpetuaFilter() : super(name: "Perpetua") {
    subFilters.add(RGBScaleSubFilter(1.05, 1.1, 1));
  }
}

// Amaro: Adds light to an image, with the focus on the centre
class AmaroFilter extends ColorFilter {
  AmaroFilter() : super(name: "Amaro") {
    subFilters.add(SaturationSubFilter(0.3));
    subFilters.add(BrightnessSubFilter(0.15));
  }
}

// Mayfair: Applies a warm pink tone, subtle vignetting to brighten the photograph center and a thin black border
class MayfairFilter extends ColorFilter {
  MayfairFilter() : super(name: "Mayfair") {
    subFilters.add(RGBOverlaySubFilter(230, 115, 108, 0.05));
    subFilters.add(SaturationSubFilter(0.15));
  }
}

// Rise: Adds a "glow" to the image, with softer lighting of the subject
class RiseFilter extends ColorFilter {
  RiseFilter() : super(name: "Rise") {
    subFilters.add(RGBOverlaySubFilter(255, 170, 0, 0.1));
    subFilters.add(BrightnessSubFilter(0.09));
    subFilters.add(SaturationSubFilter(0.1));
  }
}

// Hudson: Creates an "icy" illusion with heightened shadows, cool tint and dodged center
class HudsonFilter extends ColorFilter {
  HudsonFilter() : super(name: "Hudson") {
    subFilters.add(RGBScaleSubFilter(1, 1, 1.25));
    subFilters.add(ContrastSubFilter(0.1));
    subFilters.add(BrightnessSubFilter(0.15));
  }
}

// Valencia: Fades the image by increasing exposure and warming the colors, to give it an antique feel
class ValenciaFilter extends ColorFilter {
  ValenciaFilter() : super(name: "Valencia") {
    subFilters.add(RGBOverlaySubFilter(255, 225, 80, 0.08));
    subFilters.add(SaturationSubFilter(0.1));
    subFilters.add(ContrastSubFilter(0.05));
  }
}

// X-Pro II: Increases color vibrance with a golden tint, high contrast and slight vignette added to the edges
class XProIIFilter extends ColorFilter {
  XProIIFilter() : super(name: "X-Pro II") {
    subFilters.add(RGBOverlaySubFilter(255, 255, 0, 0.07));
    subFilters.add(SaturationSubFilter(0.2));
    subFilters.add(ContrastSubFilter(0.15));
  }
}

// Sierra: Gives a faded, softer look
class SierraFilter extends ColorFilter {
  SierraFilter() : super(name: "Sierra") {
    subFilters.add(ContrastSubFilter(-0.15));
    subFilters.add(SaturationSubFilter(0.1));
  }
}

// Willow: A monochromatic filter with subtle purple tones and a translucent white border
class WillowFilter extends ColorFilter {
  WillowFilter() : super(name: "Willow") {
    subFilters.add(GrayScaleSubFilter());
    subFilters.add(RGBOverlaySubFilter(100, 28, 210, 0.03));
    subFilters.add(BrightnessSubFilter(0.1));
  }
}

// Lo-Fi: Enriches color and adds strong shadows through the use of saturation and "warming" the temperature
class LoFiFilter extends ColorFilter {
  LoFiFilter() : super(name: "Lo-Fi") {
    subFilters.add(ContrastSubFilter(0.15));
    subFilters.add(SaturationSubFilter(0.2));
  }
}

// Inkwell: Direct shift to black and white
class InkwellFilter extends ColorFilter {
  InkwellFilter() : super(name: "Inkwell") {
    subFilters.add(GrayScaleSubFilter());
  }
}

// Hefe: Hight contrast and saturation, with a similar effect to Lo-Fi but not quite as dramatic
class HefeFilter extends ColorFilter {
  HefeFilter() : super(name: "Hefe") {
    subFilters.add(ContrastSubFilter(0.1));
    subFilters.add(SaturationSubFilter(0.15));
  }
}

// Nashville: Warms the temperature, lowers contrast and increases exposure to give a light "pink" tint – making it feel "nostalgic"
class NashvilleFilter extends ColorFilter {
  NashvilleFilter() : super(name: "Nashville") {
    subFilters.add(RGBOverlaySubFilter(220, 115, 188, 0.12));
    subFilters.add(ContrastSubFilter(-0.05));
  }
}

// Stinson: washing out the colors ever so slightly
class StinsonFilter extends ColorFilter {
  StinsonFilter() : super(name: "Stinson") {
    subFilters.add(BrightnessSubFilter(0.1));
    subFilters.add(SepiaSubFilter(0.3));
  }
}

// Vesper: adds a yellow tint that
class VesperFilter extends ColorFilter {
  VesperFilter() : super(name: "Vesper") {
    subFilters.add(RGBOverlaySubFilter(255, 225, 0, 0.05));
    subFilters.add(BrightnessSubFilter(0.06));
    subFilters.add(ContrastSubFilter(0.06));
  }
}

// Earlybird: Gives an older look with a sepia tint and warm temperature
class EarlybirdFilter extends ColorFilter {
  EarlybirdFilter() : super(name: "Earlybird") {
    subFilters.add(RGBOverlaySubFilter(255, 165, 40, 0.2));
    subFilters.add(SaturationSubFilter(0.15));
  }
}

// Brannan: Increases contrast and exposure and adds a metallic tint
class BrannanFilter extends ColorFilter {
  BrannanFilter() : super(name: "Brannan") {
    subFilters.add(ContrastSubFilter(0.2));
    subFilters.add(RGBOverlaySubFilter(140, 10, 185, 0.1));
  }
}

// Sutro: Burns photo edges, increases highlights and shadows dramatically with a focus on purple and brown colors
class SutroFilter extends ColorFilter {
  SutroFilter() : super(name: "Sutro") {
    subFilters.add(BrightnessSubFilter(-0.1));
    subFilters.add(SaturationSubFilter(-0.1));
  }
}

// Toaster: Ages the image by "burning" the centre and adds a dramatic vignette
class ToasterFilter extends ColorFilter {
  ToasterFilter() : super(name: "Toaster") {
    subFilters.add(SepiaSubFilter(0.1));
    subFilters.add(RGBOverlaySubFilter(255, 145, 0, 0.2));
  }
}

// Walden: Increases exposure and adds a yellow tint
class WaldenFilter extends ColorFilter {
  WaldenFilter() : super(name: "Walden") {
    subFilters.add(BrightnessSubFilter(0.1));
    subFilters.add(RGBOverlaySubFilter(255, 255, 0, 0.2));
  }
}

// 1977: The increased exposure with a red tint gives the photograph a rosy, brighter, faded look.
class F1977Filter extends ColorFilter {
  F1977Filter() : super(name: "1977") {
    subFilters.add(RGBOverlaySubFilter(255, 25, 0, 0.15));
    subFilters.add(BrightnessSubFilter(0.1));
  }
}

// Kelvin: Increases saturation and temperature to give it a radiant "glow"
class KelvinFilter extends ColorFilter {
  KelvinFilter() : super(name: "Kelvin") {
    subFilters.add(RGBOverlaySubFilter(255, 140, 0, 0.1));
    subFilters.add(RGBScaleSubFilter(1.15, 1.05, 1));
    subFilters.add(SaturationSubFilter(0.35));
  }
}

// Maven: darkens images, increases shadows, and adds a slightly yellow tint overal
class MavenFilter extends ColorFilter {
  MavenFilter() : super(name: "Maven") {
    subFilters.add(RGBOverlaySubFilter(225, 240, 0, 0.1));
    subFilters.add(SaturationSubFilter(0.25));
    subFilters.add(ContrastSubFilter(0.05));
  }
}

// Ginza: brightens and adds a warm glow
class GinzaFilter extends ColorFilter {
  GinzaFilter() : super(name: "Ginza") {
    subFilters.add(SepiaSubFilter(0.06));
    subFilters.add(BrightnessSubFilter(0.1));
  }
}

// Skyline: brightens to the image pop
class SkylineFilter extends ColorFilter {
  SkylineFilter() : super(name: "Skyline") {
    subFilters.add(SaturationSubFilter(0.35));
    subFilters.add(BrightnessSubFilter(0.1));
  }
}

// Dogpatch: increases the contrast, while washing out the lighter colors
class DogpatchFilter extends ColorFilter {
  DogpatchFilter() : super(name: "Dogpatch") {
    subFilters.add(ContrastSubFilter(0.15));
    subFilters.add(BrightnessSubFilter(0.1));
  }
}

// Brooklyn
class BrooklynFilter extends ColorFilter {
  BrooklynFilter() : super(name: "Brooklyn") {
    subFilters.add(RGBOverlaySubFilter(25, 240, 252, 0.05));
    subFilters.add(SepiaSubFilter(0.3));
  }
}

// Helena: adds an orange and teal vibe
class HelenaFilter extends ColorFilter {
  HelenaFilter() : super(name: "Helena") {
    subFilters.add(RGBOverlaySubFilter(208, 208, 86, 0.2));
    subFilters.add(ContrastSubFilter(0.15));
  }
}

// Ashby: gives images a great golden glow and a subtle vintage feel
class AshbyFilter extends ColorFilter {
  AshbyFilter() : super(name: "Ashby") {
    subFilters.add(RGBOverlaySubFilter(255, 160, 25, 0.1));
    subFilters.add(BrightnessSubFilter(0.1));
  }
}

// Charmes: a high contrast filter, warming up colors in your image with a red tint
class CharmesFilter extends ColorFilter {
  CharmesFilter() : super(name: "Charmes") {
    subFilters.add(RGBOverlaySubFilter(255, 50, 80, 0.12));
    subFilters.add(ContrastSubFilter(0.05));
  }
}

final List<Filter> presetFiltersList = [
  NoFilter(),
  AddictiveBlueFilter(),
  AddictiveRedFilter(),
  AdenFilter(),
  AmaroFilter(),
  AshbyFilter(),
  BrannanFilter(),
  BrooklynFilter(),
  CharmesFilter(),
  ClarendonFilter(),
  CremaFilter(),
  DogpatchFilter(),
  EarlybirdFilter(),
  F1977Filter(),
  GinghamFilter(),
  GinzaFilter(),
  HefeFilter(),
  HelenaFilter(),
  HudsonFilter(),
  InkwellFilter(),
  JunoFilter(),
  KelvinFilter(),
  LarkFilter(),
  LoFiFilter(),
  LudwigFilter(),
  MavenFilter(),
  MayfairFilter(),
  MoonFilter(),
  NashvilleFilter(),
  PerpetuaFilter(),
  ReyesFilter(),
  RiseFilter(),
  SierraFilter(),
  SkylineFilter(),
  SlumberFilter(),
  StinsonFilter(),
  SutroFilter(),
  ToasterFilter(),
  ValenciaFilter(),
  VesperFilter(),
  WaldenFilter(),
  WillowFilter(),
  XProIIFilter(),
];



================================================
FILE: lib/src/photofilters/filters/subfilters.dart
================================================
import 'dart:typed_data';
import 'package:camerawesome/src/photofilters/utils/image_filter_utils.dart'
    as image_filter_utils;
import 'package:camerawesome/src/photofilters/utils/color_filter_utils.dart'
    as color_filter_utils;
import 'package:camerawesome/src/photofilters/utils/convolution_kernels.dart';

import 'package:camerawesome/src/photofilters/rgba_model.dart';
import 'package:camerawesome/src/photofilters/filters/image_filters.dart';
import 'package:camerawesome/src/photofilters/filters/color_filters.dart';

///The [ContrastSubFilter] class is a SubFilter class to apply [contrast] to an image.
class ContrastSubFilter extends ColorSubFilter with ImageSubFilter {
  final num contrast;

  ContrastSubFilter(this.contrast);

  ///Apply the [ContrastSubFilter] to an Image.
  @override
  void apply(Uint8List pixels, int width, int height) =>
      image_filter_utils.contrast(pixels, contrast);

  ///Apply the [ContrastSubFilter] to a color.
  @override
  RGBA applyFilter(RGBA color) => color_filter_utils.contrast(color, contrast);
}

///The [BrightnessSubFilter] class is a SubFilter class to apply [brightness] to an image.
class BrightnessSubFilter extends ColorSubFilter with ImageSubFilter {
  final num brightness;
  BrightnessSubFilter(this.brightness);

  ///Apply the [BrightnessSubFilter] to an Image.
  @override
  void apply(Uint8List pixels, int width, int height) =>
      image_filter_utils.brightness(pixels, brightness);

  ///Apply the [BrightnessSubFilter] to a color.
  @override
  RGBA applyFilter(RGBA color) =>
      color_filter_utils.brightness(color, brightness);
}

///The [SaturationSubFilter] class is a SubFilter class to apply [saturation] to an image.
class SaturationSubFilter extends ColorSubFilter with ImageSubFilter {
  final num saturation;
  SaturationSubFilter(this.saturation);

  ///Apply the [SaturationSubFilter] to an Image.
  @override
  void apply(Uint8List pixels, int width, int height) =>
      image_filter_utils.saturation(pixels, saturation);

  ///Apply the [SaturationSubFilter] to a saturation.
  @override
  RGBA applyFilter(RGBA color) =>
      color_filter_utils.saturation(color, saturation);
}

///The [SepiaSubFilter] class is a SubFilter class to apply [sepia] to an image.
class SepiaSubFilter extends ColorSubFilter with ImageSubFilter {
  final num sepia;
  SepiaSubFilter(this.sepia);

  ///Apply the [SepiaSubFilter] to an Image.
  @override
  void apply(Uint8List pixels, int width, int height) =>
      image_filter_utils.sepia(pixels, sepia);

  ///Apply the [SepiaSubFilter] to a color.
  @override
  RGBA applyFilter(RGBA color) => color_filter_utils.sepia(color, sepia);
}

///The [GrayScaleSubFilter] class is a SubFilter class to apply GrayScale to an image.
class GrayScaleSubFilter extends ColorSubFilter with ImageSubFilter {
  ///Apply the [GrayScaleSubFilter] to an Image.
  @override
  void apply(Uint8List pixels, int width, int height) =>
      image_filter_utils.grayscale(pixels);

  ///Apply the [GrayScaleSubFilter] to a color.
  @override
  RGBA applyFilter(RGBA color) => color_filter_utils.grayscale(color);
}

///The [InvertSubFilter] class is a SubFilter class to invert an image.
class InvertSubFilter extends ColorSubFilter with ImageSubFilter {
  ///Apply the [InvertSubFilter] to an Image.
  @override
  void apply(Uint8List pixels, int width, int height) =>
      image_filter_utils.invert(pixels);

  ///Apply the [InvertSubFilter] to a color.
  @override
  RGBA applyFilter(RGBA color) => color_filter_utils.invert(color);
}

///The [HueRotationSubFilter] class is a SubFilter class to rotate hue in [degrees].
class HueRotationSubFilter extends ColorSubFilter with ImageSubFilter {
  final int degrees;
  HueRotationSubFilter(this.degrees);

  ///Apply the [HueRotationSubFilter] to an Image.
  @override
  void apply(Uint8List pixels, int width, int height) =>
      image_filter_utils.hueRotation(pixels, degrees);

  ///Apply the [HueRotationSubFilter] to a color.
  @override
  RGBA applyFilter(RGBA color) =>
      color_filter_utils.hueRotation(color, degrees);
}

///The [AddictiveColorSubFilter] class is a SubFilter class to emphasize a color using [red], [green] and [b] values.
class AddictiveColorSubFilter extends ColorSubFilter with ImageSubFilter {
  final int red;
  final int green;
  final int blue;
  AddictiveColorSubFilter(this.red, this.green, this.blue);

  ///Apply the [AddictiveColorSubFilter] to an Image.
  @override
  void apply(Uint8List pixels, int width, int height) =>
      image_filter_utils.addictiveColor(pixels, red, green, blue);

  ///Apply the [AddictiveColorSubFilter] to a color.
  @override
  RGBA applyFilter(RGBA color) =>
      color_filter_utils.addictiveColor(color, red, green, blue);
}

///The [RGBScaleSubFilter] class is a SubFilter class to scale RGB values of every pixels in an image.
class RGBScaleSubFilter extends ColorSubFilter with ImageSubFilter {
  final num red;
  final num green;
  final num blue;
  RGBScaleSubFilter(this.red, this.green, this.blue);

  ///Apply the [RGBScaleSubFilter] to an Image.
  @override
  void apply(Uint8List pixels, int width, int height) =>
      image_filter_utils.rgbScale(pixels, red, green, blue);

  ///Apply the [RGBScaleSubFilter] to a color.
  @override
  RGBA applyFilter(RGBA color) =>
      color_filter_utils.rgbScale(color, red, green, blue);
}

///The [RGBOverlaySubFilter] class is a SubFilter class to apply an overlay to an image.
class RGBOverlaySubFilter extends ColorSubFilter with ImageSubFilter {
  final num red;
  final num green;
  final num blue;
  final num scale;
  RGBOverlaySubFilter(this.red, this.green, this.blue, this.scale);

  ///Apply the [RGBOverlaySubFilter] to an Image.
  @override
  void apply(Uint8List pixels, int width, int height) =>
      image_filter_utils.colorOverlay(pixels, red, green, blue, scale);

  ///Apply the [RGBOverlaySubFilter] to a color.
  @override
  RGBA applyFilter(RGBA color) =>
      color_filter_utils.colorOverlay(color, red, green, blue, scale);
}

///The [ConvolutionSubFilter] class is a ImageFilter class to apply a convolution to an image.
class ConvolutionSubFilter implements ImageSubFilter {
  final List<num> weights;
  final num bias;

  ConvolutionSubFilter(this.weights, [this.bias = 0]);

  ConvolutionSubFilter.fromKernel(ConvolutionKernel kernel)
      : this(kernel.convolution, kernel.bias);

  ///Apply the [ConvolutionSubFilter] to an Image.
  @override
  void apply(Uint8List pixels, int width, int height) => image_filter_utils
      .convolute(pixels, width, height, _normalizeKernel(weights), bias);

  List<num> _normalizeKernel(List<num> kernel) {
    num sum = 0;
    for (var i = 0; i < kernel.length; i++) {
      sum += kernel[i];
    }
    if (sum != 0 && sum != 1) {
      for (var i = 0; i < kernel.length; i++) {
        kernel[i] /= sum;
      }
    }

    return kernel;
  }
}



================================================
FILE: lib/src/photofilters/utils/color_filter_utils.dart
================================================
import 'dart:math';

import 'package:camerawesome/src/photofilters/utils/utils.dart' as image_utils;

import 'package:camerawesome/src/photofilters/rgba_model.dart';

int clampPixel(int x) => x.clamp(0, 255);
RGBA saturation(RGBA color, num saturation) {
  saturation = (saturation < -1) ? -1 : saturation;
  final num gray = 0.2989 * color.red +
      0.5870 * color.green +
      0.1140 * color.blue; //weights from CCIR 601 spec
  return RGBA(
    red:
        clampPixel((-gray * saturation + color.red * (1 + saturation)).round()),
    green: clampPixel(
        (-gray * saturation + color.green * (1 + saturation)).round()),
    blue: clampPixel(
        (-gray * saturation + color.blue * (1 + saturation)).round()),
    alpha: color.alpha,
  );
}

RGBA hueRotation(RGBA color, int degrees) {
  final double U = cos(degrees * pi / 180);
  final double W = sin(degrees * pi / 180);

  final num r = color.red, g = color.green, b = color.blue;
  return RGBA(
    red: clampPixel(((.299 + .701 * U + .168 * W) * r +
            (.587 - .587 * U + .330 * W) * g +
            (.114 - .114 * U - .497 * W) * b)
        .round()),
    green: clampPixel(((.299 - .299 * U - .328 * W) * r +
            (.587 + .413 * U + .035 * W) * g +
            (.114 - .114 * U + .292 * W) * b)
        .round()),
    blue: clampPixel(((.299 - .3 * U + 1.25 * W) * r +
            (.587 - .588 * U - 1.05 * W) * g +
            (.114 + .886 * U - .203 * W) * b)
        .round()),
    alpha: color.alpha,
  );
}

RGBA grayscale(RGBA color) {
  final int avg = clampPixel(
      (0.2126 * color.red + 0.7152 * color.green + 0.0722 * color.blue)
          .round());
  return RGBA(
    red: avg,
    green: avg,
    blue: avg,
    alpha: color.alpha,
  );
}

// Adj is 0 (unchanged) to 1 (sepia)
RGBA sepia(RGBA color, num adj) {
  final int r = color.red, g = color.green, b = color.blue;
  return RGBA(
      red: clampPixel(
          ((r * (1 - (0.607 * adj))) + (g * .769 * adj) + (b * .189 * adj))
              .round()),
      green: clampPixel(
          ((r * .349 * adj) + (g * (1 - (0.314 * adj))) + (b * .168 * adj))
              .round()),
      blue: clampPixel(
          ((r * .272 * adj) + (g * .534 * adj) + (b * (1 - (0.869 * adj))))
              .round()),
      alpha: color.alpha);
}

RGBA invert(RGBA color) {
  return RGBA(
    red: clampPixel(255 - color.red),
    green: clampPixel(255 - color.green),
    blue: clampPixel(255 - color.blue),
    alpha: color.alpha,
  );
}

/* adj should be -1 (darker) to 1 (lighter). 0 is unchanged. */
RGBA brightness(RGBA color, num adj) {
  adj = (adj > 1) ? 1 : adj;
  adj = (adj < -1) ? -1 : adj;
  adj = ~~(255 * adj).round();
  return RGBA(
      red: clampPixel(color.red + (adj as int)),
      green: clampPixel(color.green + adj),
      blue: clampPixel(color.blue + adj),
      alpha: color.alpha);
}

// Better result (slow) - adj should be < 1 (desaturated) to 1 (unchanged) and < 1
RGBA hueSaturation(RGBA color, num adj) {
  final hsv = image_utils.rgbToHsv(color.red, color.green, color.blue);
  hsv[1] = (hsv[1] ?? 0) * adj;
  final rgb = image_utils.hsvToRgb(hsv[0]!, hsv[1]!, hsv[2]!);
  return RGBA(
    red: clampPixel(rgb[0] as int),
    green: clampPixel(rgb[1] as int),
    blue: clampPixel(rgb[2] as int),
    alpha: color.alpha,
  );
}

// Contrast - the adj value should be -1 to 1
RGBA contrast(RGBA color, num adj) {
  adj *= 255;
  final double factor = (259 * (adj + 255)) / (255 * (259 - adj));
  return RGBA(
    red: clampPixel((factor * (color.red - 128) + 128).round()),
    green: clampPixel((factor * (color.green - 128) + 128).round()),
    blue: clampPixel((factor * (color.blue - 128) + 128).round()),
    alpha: color.alpha,
  );
}

// ColorOverlay - add a slight color overlay.
RGBA colorOverlay(RGBA color, num red, num green, num blue, num scale) {
  return RGBA(
    red: clampPixel((color.red - (color.red - red) * scale).round()),
    green: clampPixel((color.green - (color.green - green) * scale).round()),
    blue: clampPixel((color.blue - (color.blue - blue) * scale).round()),
    alpha: color.alpha,
  );
}

// RGB Scale
RGBA rgbScale(RGBA color, num red, num green, num blue) {
  return RGBA(
    red: clampPixel((color.red * red).round()),
    green: clampPixel((color.green * green).round()),
    blue: clampPixel((color.blue * blue).round()),
    alpha: color.alpha,
  );
}

RGBA addictiveColor(RGBA color, int red, int green, int blue) {
  return RGBA(
    red: clampPixel(color.red + red),
    green: clampPixel(color.green + green),
    blue: clampPixel(color.blue + blue),
    alpha: color.alpha,
  );
}



================================================
FILE: lib/src/photofilters/utils/convolution_kernels.dart
================================================
class ConvolutionKernel extends Object {
  final List<num> convolution;
  final double bias;

  const ConvolutionKernel(this.convolution, {this.bias = 0.0});
}

const ConvolutionKernel identityKernel =
    ConvolutionKernel([0, 0, 0, 0, 1, 0, 0, 0, 0]);
const ConvolutionKernel sharpenKernel =
    ConvolutionKernel([-1, -1, -1, -1, 9, -1, -1, -1, -1]);
const ConvolutionKernel embossKernel =
    ConvolutionKernel([-1, -1, 0, -1, 0, 1, 0, 1, 1], bias: 128);
const ConvolutionKernel coloredEdgeDetectionKernel =
    ConvolutionKernel([1, 1, 1, 1, -7, 1, 1, 1, 1]);
const ConvolutionKernel edgeDetectionMediumKernel =
    ConvolutionKernel([0, 1, 0, 1, -4, 1, 0, 1, 0]);
const ConvolutionKernel edgeDetectionHardKernel =
    ConvolutionKernel([-1, -1, -1, -1, 8, -1, -1, -1, -1]);

const ConvolutionKernel blurKernel = ConvolutionKernel([
  0,
  0,
  1,
  0,
  0,
  0,
  1,
  1,
  1,
  0,
  1,
  1,
  1,
  1,
  1,
  0,
  1,
  1,
  1,
  0,
  0,
  0,
  1,
  0,
  0
]);

const ConvolutionKernel guassian3x3Kernel = ConvolutionKernel([
  1,
  2,
  1,
  2,
  4,
  2,
  1,
  2,
  1,
]);

const ConvolutionKernel guassian5x5Kernel = ConvolutionKernel([
  2,
  04,
  05,
  04,
  2,
  4,
  09,
  12,
  09,
  4,
  5,
  12,
  15,
  12,
  5,
  4,
  09,
  12,
  09,
  4,
  2,
  04,
  05,
  04,
  2
]);

const ConvolutionKernel guassian7x7Kernel = ConvolutionKernel([
  1,
  1,
  2,
  2,
  2,
  1,
  1,
  1,
  2,
  2,
  4,
  2,
  2,
  1,
  2,
  2,
  4,
  8,
  4,
  2,
  2,
  2,
  4,
  8,
  16,
  8,
  4,
  2,
  2,
  2,
  4,
  8,
  4,
  2,
  2,
  1,
  2,
  2,
  4,
  2,
  2,
  1,
  1,
  1,
  2,
  2,
  2,
  1,
  1,
]);

const ConvolutionKernel mean3x3Kernel = ConvolutionKernel([
  1,
  1,
  1,
  1,
  1,
  1,
  1,
  1,
  1,
]);

const ConvolutionKernel mean5x5Kernel = ConvolutionKernel([
  1,
  1,
  1,
  1,
  1,
  1,
  1,
  1,
  1,
  1,
  1,
  1,
  1,
  1,
  1,
  1,
  1,
  1,
  1,
  1,
  1,
  1,
  1,
  1,
  1
]);

const ConvolutionKernel lowPass3x3Kernel = ConvolutionKernel([
  1,
  2,
  1,
  2,
  4,
  2,
  1,
  2,
  1,
]);

const ConvolutionKernel lowPass5x5Kernel = ConvolutionKernel([
  1,
  1,
  1,
  1,
  1,
  1,
  4,
  4,
  4,
  1,
  1,
  4,
  12,
  4,
  1,
  1,
  4,
  4,
  4,
  1,
  1,
  1,
  1,
  1,
  1,
]);

const ConvolutionKernel highPass3x3Kernel =
    ConvolutionKernel([0, -0.25, 0, -0.25, 2, -0.25, 0, -0.25, 0]);



================================================
FILE: lib/src/photofilters/utils/image_filter_utils.dart
================================================
import 'dart:math';
import 'dart:typed_data';

import 'package:camerawesome/src/photofilters/utils/utils.dart';

int clampPixel(int x) => x.clamp(0, 255);
void saturation(Uint8List bytes, num saturation) {
  saturation = (saturation < -1) ? -1 : saturation;
  for (int i = 0; i < bytes.length; i += 4) {
    final num r = bytes[i], g = bytes[i + 1], b = bytes[i + 2];
    final num gray =
        0.2989 * r + 0.5870 * g + 0.1140 * b; //weights from CCIR 601 spec
    bytes[i] =
        clampPixel((-gray * saturation + bytes[i] * (1 + saturation)).round());
    bytes[i + 1] = clampPixel(
        (-gray * saturation + bytes[i + 1] * (1 + saturation)).round());
    bytes[i + 2] = clampPixel(
        (-gray * saturation + bytes[i + 2] * (1 + saturation)).round());
  }
}

void hueRotation(Uint8List bytes, int degrees) {
  final double U = cos(degrees * pi / 180);
  final double W = sin(degrees * pi / 180);

  for (int i = 0; i < bytes.length; i += 4) {
    final num r = bytes[i], g = bytes[i + 1], b = bytes[i + 2];
    bytes[i] = clampPixel(((.299 + .701 * U + .168 * W) * r +
            (.587 - .587 * U + .330 * W) * g +
            (.114 - .114 * U - .497 * W) * b)
        .round());
    bytes[i + 1] = clampPixel(((.299 - .299 * U - .328 * W) * r +
            (.587 + .413 * U + .035 * W) * g +
            (.114 - .114 * U + .292 * W) * b)
        .round());
    bytes[i + 2] = clampPixel(((.299 - .3 * U + 1.25 * W) * r +
            (.587 - .588 * U - 1.05 * W) * g +
            (.114 + .886 * U - .203 * W) * b)
        .round());
  }
}

void grayscale(Uint8List bytes) {
  for (int i = 0; i < bytes.length; i += 4) {
    final int r = bytes[i], g = bytes[i + 1], b = bytes[i + 2];
    final int avg = clampPixel((0.2126 * r + 0.7152 * g + 0.0722 * b).round());
    bytes[i] = avg;
    bytes[i + 1] = avg;
    bytes[i + 2] = avg;
  }
}

// Adj is 0 (unchanged) to 1 (sepia)
void sepia(Uint8List bytes, num adj) {
  for (int i = 0; i < bytes.length; i += 4) {
    final int r = bytes[i], g = bytes[i + 1], b = bytes[i + 2];
    bytes[i] = clampPixel(
        ((r * (1 - (0.607 * adj))) + (g * .769 * adj) + (b * .189 * adj))
            .round());
    bytes[i + 1] = clampPixel(
        ((r * .349 * adj) + (g * (1 - (0.314 * adj))) + (b * .168 * adj))
            .round());
    bytes[i + 2] = clampPixel(
        ((r * .272 * adj) + (g * .534 * adj) + (b * (1 - (0.869 * adj))))
            .round());
  }
}

void invert(Uint8List bytes) {
  for (int i = 0; i < bytes.length; i += 4) {
    bytes[i] = clampPixel(255 - bytes[i]);
    bytes[i + 1] = clampPixel(255 - bytes[i + 1]);
    bytes[i + 2] = clampPixel(255 - bytes[i + 2]);
  }
}

/* adj should be -1 (darker) to 1 (lighter). 0 is unchanged. */
void brightness(Uint8List bytes, num adj) {
  adj = (adj > 1) ? 1 : adj;
  adj = (adj < -1) ? -1 : adj;
  adj = ~~(255 * adj).round();
  for (int i = 0; i < bytes.length; i += 4) {
    bytes[i] = clampPixel(bytes[i] + (adj as int));
    bytes[i + 1] = clampPixel(bytes[i + 1] + adj);
    bytes[i + 2] = clampPixel(bytes[i + 2] + adj);
  }
}

// Better result (slow) - adj should be < 1 (desaturated) to 1 (unchanged) and < 1
void hueSaturation(Uint8List bytes, num adj) {
  for (int i = 0; i < bytes.length; i += 4) {
    final hsv = rgbToHsv(bytes[i], bytes[i + 1], bytes[i + 2]);
    hsv[1] = (hsv[1] ?? 0) * adj;
    final rgb = hsvToRgb(hsv[0]!, hsv[1]!, hsv[2]!);
    bytes[i] = clampPixel(rgb[0] as int);
    bytes[i + 1] = clampPixel(rgb[1] as int);
    bytes[i + 2] = clampPixel(rgb[2] as int);
  }
}

// Contrast - the adj value should be -1 to 1
void contrast(Uint8List bytes, num adj) {
  adj *= 255;
  final double factor = (259 * (adj + 255)) / (255 * (259 - adj));
  for (int i = 0; i < bytes.length; i += 4) {
    bytes[i] = clampPixel((factor * (bytes[i] - 128) + 128).round());
    bytes[i + 1] = clampPixel((factor * (bytes[i + 1] - 128) + 128).round());
    bytes[i + 2] = clampPixel((factor * (bytes[i + 2] - 128) + 128).round());
  }
}

// ColorOverlay - add a slight color overlay.
void colorOverlay(Uint8List bytes, num red, num green, num blue, num scale) {
  for (int i = 0; i < bytes.length; i += 4) {
    bytes[i] = clampPixel((bytes[i] - (bytes[i] - red) * scale).round());
    bytes[i + 1] =
        clampPixel((bytes[i + 1] - (bytes[i + 1] - green) * scale).round());
    bytes[i + 2] =
        clampPixel((bytes[i + 2] - (bytes[i + 2] - blue) * scale).round());
  }
}

// RGB Scale
void rgbScale(Uint8List bytes, num red, num green, num blue) {
  for (int i = 0; i < bytes.length; i += 4) {
    bytes[i] = clampPixel((bytes[i] * red).round());
    bytes[i + 1] = clampPixel((bytes[i + 1] * green).round());
    bytes[i + 2] = clampPixel((bytes[i + 2] * blue).round());
  }
}

// Convolute - weights are 3x3 matrix
void convolute(
    Uint8List pixels, int width, int height, List<num> weights, num bias) {
  final bytes = Uint8List.fromList(pixels);
  final int side = sqrt(weights.length).round();
  final int halfSide = ~~(side / 2).round() - side % 2;
  final int sw = width;
  final int sh = height;

  final int w = sw;
  final int h = sh;

  for (int y = 0; y < h; y++) {
    for (int x = 0; x < w; x++) {
      final int sy = y;
      final int sx = x;
      final int dstOff = (y * w + x) * 4;
      num r = bias, g = bias, b = bias;
      for (int cy = 0; cy < side; cy++) {
        for (int cx = 0; cx < side; cx++) {
          final int scy = sy + cy - halfSide;
          final int scx = sx + cx - halfSide;

          if (scy >= 0 && scy < sh && scx >= 0 && scx < sw) {
            final int srcOff = (scy * sw + scx) * 4;
            final num wt = weights[cy * side + cx];

            r += bytes[srcOff] * wt;
            g += bytes[srcOff + 1] * wt;
            b += bytes[srcOff + 2] * wt;
          }
        }
      }
      pixels[dstOff] = clampPixel(r.round());
      pixels[dstOff + 1] = clampPixel(g.round());
      pixels[dstOff + 2] = clampPixel(b.round());
    }
  }
}

void addictiveColor(Uint8List bytes, int red, int green, int blue) {
  for (int i = 0; i < bytes.length; i += 4) {
    bytes[i] = clampPixel(bytes[i] + red);
    bytes[i + 1] = clampPixel(bytes[i + 1] + green);
    bytes[i + 2] = clampPixel(bytes[i + 2] + blue);
  }
}



================================================
FILE: lib/src/photofilters/utils/utils.dart
================================================
import 'dart:math';

List<num?> rgbToHsv(num r, num g, num b) {
  r /= 255;
  g /= 255;
  b /= 255;

  final num mMax = max(
    r,
    max(g, b),
  );
  final num mMin = min(
    r,
    max(g, b),
  );
  final num h, s, v = mMax;

  final num d = mMax - mMin;
  s = mMax == 0 ? 0 : d / mMax;

  if (max == min) {
    h = 0; // achromatic
  } else if (mMax == r) {
    h = (g - b) / d + (g < b ? 6 : 0);
  } else if (mMax == g) {
    h = (b - r) / d + 2;
  } else if (mMax == b) {
    h = (r - g) / d + 4;
  } else {
    h = 0;
  }

  return [h, s, v];
}

List<num> hsvToRgb(num h, num s, num v) {
  final int r, g, b;

  final int i = (h * 6).floor();
  final int f = h * 6 - i as int;
  final int p = v * (1 - s) as int;
  final int q = v * (1 - f * s) as int;
  final int t = v * (1 - (1 - f) * s) as int;

  switch (i % 6) {
    case 0:
      r = v as int;
      g = t;
      b = p;
      break;
    case 1:
      r = q;
      g = v as int;
      b = p;
      break;
    case 2:
      r = p;
      g = v as int;
      b = t;
      break;
    case 3:
      r = p;
      g = q;
      b = v as int;
      break;
    case 4:
      r = t;
      g = p;
      b = v as int;
      break;
    case 5:
      r = v as int;
      g = p;
      b = q;
      break;
    default:
      r = 0;
      g = 0;
      b = 0;
  }

  return [r * 255, g * 255, b * 255];
}



================================================
FILE: lib/src/widgets/awesome_camera_mode_selector.dart
================================================
import 'package:camerawesome/src/orchestrator/models/capture_modes.dart';
import 'package:camerawesome/src/orchestrator/states/states.dart';
import 'package:camerawesome/src/widgets/utils/awesome_bouncing_widget.dart';
import 'package:camerawesome/src/widgets/utils/awesome_theme.dart';
import 'package:flutter/material.dart';

class AwesomeCameraModeSelector extends StatelessWidget {
  final CameraState state;

  const AwesomeCameraModeSelector({
    super.key,
    required this.state,
  });

  @override
  Widget build(BuildContext context) {
    final theme = AwesomeThemeProvider.of(context).theme;
    Widget content;
    if (state is VideoRecordingCameraState || state.saveConfig == null) {
      content = const SizedBox(
        height: 40,
      );
    } else {
      content = CameraModePager(
        initialMode: state.captureMode,
        availableModes: state.saveConfig!.captureModes,
        onChangeCameraRequest: (mode) {
          state.setState(mode);
        },
      );
    }
    return Container(
      color: theme.bottomActionsBackgroundColor,
      padding: const EdgeInsets.only(top: 8),
      child: content,
    );
  }
}

typedef OnChangeCameraRequest = Function(CaptureMode mode);

class CameraModePager extends StatefulWidget {
  final OnChangeCameraRequest onChangeCameraRequest;

  final List<CaptureMode> availableModes;
  final CaptureMode? initialMode;

  const CameraModePager({
    super.key,
    required this.onChangeCameraRequest,
    required this.availableModes,
    required this.initialMode,
  });

  @override
  State<CameraModePager> createState() => _CameraModePagerState();
}

class _CameraModePagerState extends State<CameraModePager> {
  late PageController _pageController;

  int _index = 0;

  @override
  void initState() {
    super.initState();
    _index = widget.initialMode != null
        ? widget.availableModes.indexOf(widget.initialMode!)
        : 0;
    _pageController =
        PageController(viewportFraction: 0.25, initialPage: _index);
  }

  @override
  void dispose() {
    _pageController.dispose();
    super.dispose();
  }

  @override
  Widget build(BuildContext context) {
    if (widget.availableModes.length <= 1) {
      return const SizedBox.shrink();
    }
    return Row(
      children: [
        Expanded(
          child: SizedBox(
            height: 32,
            child: PageView.builder(
              scrollDirection: Axis.horizontal,
              controller: _pageController,
              onPageChanged: (index) {
                final cameraMode = widget.availableModes[index];
                widget.onChangeCameraRequest(cameraMode);
                setState(() {
                  _index = index;
                });
              },
              itemCount: widget.availableModes.length,
              itemBuilder: ((context, index) {
                final cameraMode = widget.availableModes[index];
                return AnimatedOpacity(
                  duration: const Duration(milliseconds: 300),
                  opacity: index == _index ? 1 : 0.2,
                  child: AwesomeBouncingWidget(
                    child: Center(
                      child: Padding(
                        padding: const EdgeInsets.symmetric(vertical: 8),
                        child: Text(
                          cameraMode.name.toUpperCase(),
                          style: const TextStyle(
                            color: Colors.white,
                            fontWeight: FontWeight.bold,
                            shadows: [
                              Shadow(
                                blurRadius: 4,
                                color: Colors.black,
                              )
                            ],
                          ),
                        ),
                      ),
                    ),
                    onTap: () {
                      _pageController.animateToPage(
                        index,
                        curve: Curves.easeIn,
                        duration: const Duration(milliseconds: 200),
                      );
                    },
                  ),
                );
              }),
            ),
          ),
        )
      ],
    );
  }
}



================================================
FILE: lib/src/widgets/awesome_media_preview.dart
================================================
import 'dart:io';

import 'package:camerawesome/src/orchestrator/models/media_capture.dart';
import 'package:camerawesome/src/widgets/camera_awesome_builder.dart';
import 'package:camerawesome/src/widgets/utils/awesome_bouncing_widget.dart';
import 'package:camerawesome/src/widgets/utils/awesome_oriented_widget.dart';
import 'package:flutter/cupertino.dart';
import 'package:flutter/foundation.dart';
import 'package:flutter/material.dart';

class AwesomeMediaPreview extends StatelessWidget {
  final MediaCapture? mediaCapture;
  final OnMediaTap onMediaTap;

  const AwesomeMediaPreview({
    super.key,
    required this.mediaCapture,
    required this.onMediaTap,
  });

  @override
  Widget build(BuildContext context) {
    return AwesomeOrientedWidget(
      child: AspectRatio(
        aspectRatio: 1,
        child: AwesomeBouncingWidget(
          onTap: mediaCapture != null &&
                  onMediaTap != null &&
                  mediaCapture?.status == MediaCaptureStatus.success
              ? () => onMediaTap!(mediaCapture!)
              : null,
          child: AnimatedContainer(
            duration: const Duration(milliseconds: 300),
            decoration: BoxDecoration(
              color: Colors.white30,
              shape: BoxShape.circle,
              border: Border.all(
                color: Colors.white38,
                width: 2,
              ),
            ),
            child: ClipOval(child: _buildMedia(mediaCapture)),
          ),
        ),
      ),
    );
  }

  Widget _buildMedia(MediaCapture? mediaCapture) {
    switch (mediaCapture?.status) {
      case MediaCaptureStatus.capturing:
        return Center(
          child: Padding(
            padding: const EdgeInsets.all(8),
            child: Platform.isIOS
                ? const CupertinoActivityIndicator(
                    color: Colors.white,
                  )
                : const Padding(
                    padding: EdgeInsets.all(8.0),
                    child: CircularProgressIndicator(
                      color: Colors.white,
                      strokeWidth: 2.0,
                    ),
                  ),
          ),
        );
      case MediaCaptureStatus.success:
        if (mediaCapture!.isPicture) {
          if (kIsWeb) {
            // TODO Check if that works
            return FutureBuilder<Uint8List>(
                future: mediaCapture.captureRequest.when(
                  single: (single) => single.file!.readAsBytes(),
                  multiple: (multiple) =>
                      multiple.fileBySensor.values.first!.readAsBytes(),
                ),
                builder: (_, snapshot) {
                  if (snapshot.hasData) {
                    return Image.memory(
                      snapshot.requireData,
                      fit: BoxFit.cover,
                      width: 300,
                    );
                  } else {
                    return Platform.isIOS
                        ? const CupertinoActivityIndicator(
                            color: Colors.white,
                          )
                        : const Padding(
                            padding: EdgeInsets.all(8.0),
                            child: CircularProgressIndicator(
                              color: Colors.white,
                              strokeWidth: 2.0,
                            ),
                          );
                  }
                });
          } else {
            return Image(
              fit: BoxFit.cover,
              image: ResizeImage(
                FileImage(
                  File(
                    mediaCapture.captureRequest.when(
                      single: (single) => single.file!.path,
                      multiple: (multiple) =>
                          multiple.fileBySensor.values.first!.path,
                    ),
                  ),
                ),
                width: 300,
              ),
            );
          }
        } else {
          return const Icon(
            Icons.play_arrow_rounded,
            color: Colors.white,
          );
        }
      case MediaCaptureStatus.failure:
        return const Icon(
          Icons.error,
          color: Colors.white,
        );
      case null:
        return const SizedBox(
          width: 32,
          height: 32,
        );
    }
  }
}



================================================
FILE: lib/src/widgets/awesome_sensor_type_selector.dart
================================================
import 'package:camerawesome/camerawesome_plugin.dart';
import 'package:flutter/material.dart';

class AwesomeSensorTypeSelector extends StatefulWidget {
  final CameraState state;

  const AwesomeSensorTypeSelector({
    super.key,
    required this.state,
  });

  @override
  State<AwesomeSensorTypeSelector> createState() =>
      _AwesomeSensorTypeSelectorState();
}

class _AwesomeSensorTypeSelectorState extends State<AwesomeSensorTypeSelector> {
  SensorDeviceData? _sensorDeviceData;

  @override
  void initState() {
    super.initState();

    widget.state.getSensors().then((sensorDeviceData) {
      setState(() {
        _sensorDeviceData = sensorDeviceData;
      });
    });
  }

  @override
  Widget build(BuildContext context) {
    return StreamBuilder<SensorConfig>(
      stream: widget.state.sensorConfig$,
      builder: (_, sensorConfigSnapshot) {
        return AnimatedSwitcher(
          duration: const Duration(milliseconds: 200),
          child: _buildContent(sensorConfigSnapshot),
        );
      },
    );
  }

  Widget _buildContent(AsyncSnapshot<SensorConfig> sensorConfigSnapshot) {
    if (!sensorConfigSnapshot.hasData) {
      return const SizedBox.shrink();
    }

    if (sensorConfigSnapshot.data != null &&
        sensorConfigSnapshot.data!.sensors.isNotEmpty &&
        sensorConfigSnapshot.data!.sensors.first.position ==
            SensorPosition.front) {
      return const SizedBox.shrink();
    }

    final sensorConfig = sensorConfigSnapshot.requireData;
    return StreamBuilder<SensorType>(
      stream: sensorConfig.sensorType$,
      builder: (context, snapshot) {
        if (!snapshot.hasData) {
          return const SizedBox.shrink();
        }

        if (_sensorDeviceData == null ||
            _sensorDeviceData!.availableBackSensors <= 0) {
          return const SizedBox.shrink();
        }

        return Container(
          height: 50,
          decoration: BoxDecoration(
            color: _sensorDeviceData != null &&
                    _sensorDeviceData!.availableBackSensors > 1
                ? Colors.black.withValues(alpha: 0.2)
                : Colors.transparent,
            borderRadius: BorderRadius.circular(30),
          ),
          child: Padding(
            padding: const EdgeInsets.symmetric(horizontal: 15.0),
            child: Wrap(
              spacing: 10,
              runAlignment: WrapAlignment.center,
              crossAxisAlignment: WrapCrossAlignment.center,
              children: [
                if (_sensorDeviceData?.ultraWideAngle != null)
                  _SensorTypeButton(
                    sensorType: SensorType.ultraWideAngle,
                    isSelected: snapshot.data == SensorType.ultraWideAngle,
                    onTap: () {
                      widget.state.setSensorType(0, SensorType.ultraWideAngle,
                          _sensorDeviceData!.ultraWideAngle!.uid);
                    },
                  ),
                if (_sensorDeviceData?.wideAngle != null)
                  _SensorTypeButton(
                    sensorType: SensorType.wideAngle,
                    isSelected: snapshot.data == SensorType.wideAngle,
                    onTap: () {
                      widget.state.setSensorType(0, SensorType.wideAngle,
                          _sensorDeviceData!.wideAngle!.uid);
                    },
                  ),
                if (_sensorDeviceData?.telephoto != null)
                  _SensorTypeButton(
                    sensorType: SensorType.telephoto,
                    isSelected: snapshot.data == SensorType.telephoto,
                    onTap: () {
                      widget.state.setSensorType(0, SensorType.telephoto,
                          _sensorDeviceData!.telephoto!.uid);
                    },
                  ),
                // Text(snapshot.data.toString()),
              ],
            ),
          ),
        );
      },
    );
  }
}

class _SensorTypeButton extends StatelessWidget {
  final SensorType sensorType;
  final bool isSelected;
  final Function()? onTap;

  const _SensorTypeButton({
    required this.sensorType,
    this.isSelected = false,
    this.onTap,
  });

  @override
  Widget build(BuildContext context) {
    return AwesomeOrientedWidget(
      child: AwesomeBouncingWidget(
        onTap: onTap,
        child: AnimatedContainer(
          duration: const Duration(milliseconds: 200),
          height: isSelected ? 40 : 30,
          width: isSelected ? 40 : 30,
          decoration: BoxDecoration(
            shape: BoxShape.circle,
            color: Colors.black.withValues(alpha: 0.2),
          ),
          child: Center(
            child: Text(
              '$sensorTypeZoomValue${isSelected ? 'x' : ''}',
              maxLines: 1,
              style: TextStyle(
                color: isSelected ? Colors.yellowAccent : Colors.white,
                fontWeight: FontWeight.bold,
                fontSize: isSelected ? 13 : 12,
                letterSpacing: sensorType == SensorType.ultraWideAngle ? -1 : 0,
              ),
            ),
          ),
        ),
      ),
    );
  }

  String get sensorTypeZoomValue {
    switch (sensorType) {
      case SensorType.wideAngle:
        return '1';
      case SensorType.ultraWideAngle:
        return '0.5';
      case SensorType.telephoto:
        return '2';
      default:
        return '1';
    }
  }
}



================================================
FILE: lib/src/widgets/camera_awesome_builder.dart
================================================
import 'dart:async';

import 'package:camerawesome/camerawesome_plugin.dart';
import 'package:camerawesome/pigeon.dart';
import 'package:camerawesome/src/orchestrator/camera_context.dart';
import 'package:flutter/material.dart';
import 'package:flutter/services.dart';

/// This is the builder for your camera interface
/// Using the [state] you can do anything you need without having to think about the camera flow
/// On app start we are in [PreparingCameraState]
/// Then depending on the initialCaptureMode you set you will be [PhotoCameraState] or [VideoCameraState]
/// Starting a video will push a [VideoRecordingCameraState]
/// Stopping the video will push back the [VideoCameraState]
/// ----
/// If you need to call specific function for a state use the 'when' function.
typedef CameraLayoutBuilder = Widget Function(
  CameraState state,

  /// [previewSize] not clipped
  //PreviewSize previewSize,

  /// [previewRect] size might be different than [previewSize] if it has been
  /// clipped. It is often clipped in 1:1 ratio. Use it to show elements
  /// relative to the preview (inside or outside for instance)
  //Rect previewRect,

  AnalysisPreview preview,
);

/// Callback when a video or photo has been saved and user click on thumbnail
typedef OnMediaTap = Function(MediaCapture mediaCapture)?;

/// Used to set a permission result callback
typedef OnPermissionsResult = void Function(bool result);

/// Listener for picture or video capture event
typedef OnMediaCaptureEvent = void Function(MediaCapture mediaCapture);

/// Analysis image stream listener
/// The Preview object will help you to convert a point from the preview to the
/// to your screen
typedef OnImageForAnalysis = Future Function(
  AnalysisImage image,
);

/// This is the entry point of the CameraAwesome plugin
/// You can either
/// - build your custom layout
/// or
/// - use our built in interface
/// with the awesome factory
class CameraAwesomeBuilder extends StatefulWidget {
  /// Which sensors you want to use
  final SensorConfig sensorConfig;

  /// check this for more details
  /// https://api.flutter.dev/flutter/painting/BoxFit.html
  // one of fitWidth, fitHeight, contain, cover
  // currently only work for Android, this do nothing on iOS
  final CameraPreviewFit previewFit;

  /// Enable physical button (volume +/-) to take photo or record video
  final bool enablePhysicalButton;

  /// Path builders when taking photos or recording videos
  final SaveConfig? saveConfig;

  /// Called when the preview of the last captured media is tapped
  final OnMediaTap onMediaTap;

  // Widgets
  final Widget? progressIndicator;

  /// UI Builder
  final CameraLayoutBuilder builder;

  final OnImageForAnalysis? onImageForAnalysis;

  /// only for Android
  final AnalysisConfig? imageAnalysisConfig;

  /// Useful for drawing things based on AI Analysis above the CameraPreview for instance
  final CameraLayoutBuilder? previewDecoratorBuilder;

  final OnPreviewTap Function(CameraState)? onPreviewTapBuilder;
  final OnPreviewScale Function(CameraState)? onPreviewScaleBuilder;

  /// Theme of the camera UI, used in the built-in interface.
  ///
  /// You can also use it in your own UI with [AwesomeThemeProvider].
  /// You might need to wrap your UI in a [Builder] to get a [context].
  final AwesomeTheme theme;

  /// Add padding to the preview to adjust where you want to position it.
  /// See also [previewAlignment].
  final EdgeInsets previewPadding;

  /// Set alignment of the preview to adjust its position.
  /// See also [previewPadding].
  final Alignment previewAlignment;

  /// Set it to true to show a Preview of the camera, false if you only want to
  /// do image analysis
  final bool showPreview;

  final PictureInPictureConfigBuilder? pictureInPictureConfigBuilder;

  /// THe default filter to use when the camera is started.
  final AwesomeFilter? defaultFilter;

  /// List of filters to show in the built-in interface.
  /// (default: [awesomePresetFiltersList])
  /// Push null to hide the filter button
  final List<AwesomeFilter>? availableFilters;

  /// Triggered when a photo or video has been saved
  /// You can use it to do whatever you want once a media has been saved
  final OnMediaCaptureEvent? onMediaCaptureEvent;

  const CameraAwesomeBuilder._({
    required this.sensorConfig,
    required this.enablePhysicalButton,
    required this.progressIndicator,
    required this.saveConfig,
    required this.onMediaTap,
    required this.builder,
    required this.previewFit,
    required this.defaultFilter,
    this.onImageForAnalysis,
    this.imageAnalysisConfig,
    this.onPreviewTapBuilder,
    this.onPreviewScaleBuilder,
    this.previewDecoratorBuilder,
    required this.theme,
    this.previewPadding = EdgeInsets.zero,
    this.previewAlignment = Alignment.center,
    this.showPreview = true,
    required this.pictureInPictureConfigBuilder,
    this.availableFilters,
    this.onMediaCaptureEvent,
  });

  /// Use the camera with the built-in interface.
  ///
  /// You need to provide a [SaveConfig] to define if you want to take
  /// photos, videos or both and where to save them.
  ///
  /// You can initiate the camera with a few parameters through the [SensorConfig]:
  /// - which [sensors] to use ([front] or [back])
  /// - which [flashMode] to use
  /// - how much zoom you want (0.0 = no zoom, 1.0 = max zoom)
  ///
  /// If you want to customize the UI of the camera, you have several options:
  /// - use a [progressIndicator] and define what to do when the preview of the
  /// last media taken is tapped thanks to [onMediaTap]
  /// - use [topActionsBuilder], [bottomActionsBuilder], and
  /// [middleContentBuilder] which let you build entirely the UI similarly to
  /// how the built-in UI is done. Check [AwesomeCameraLayout] for more details.
  /// - build your UI entirely thanks to the [custom] constructor.
  ///
  /// If you want to do image analysis (for AI for instance), you can set the
  /// [imageAnaysisConfig] and listen to the stream of images with
  /// [onImageForAnalysis].
  CameraAwesomeBuilder.awesome(
      {SensorConfig? sensorConfig,
      bool enablePhysicalButton = false,
      Widget? progressIndicator,
      required SaveConfig saveConfig,
      Function(MediaCapture)? onMediaTap,
      OnImageForAnalysis? onImageForAnalysis,
      AnalysisConfig? imageAnalysisConfig,
      OnPreviewTap Function(CameraState)? onPreviewTapBuilder,
      OnPreviewScale Function(CameraState)? onPreviewScaleBuilder,
      CameraPreviewFit? previewFit,
      CameraLayoutBuilder? previewDecoratorBuilder,
      AwesomeTheme? theme,
      Widget Function(CameraState state)? topActionsBuilder,
      Widget Function(CameraState state)? bottomActionsBuilder,
      Widget Function(CameraState state)? middleContentBuilder,
      EdgeInsets previewPadding = EdgeInsets.zero,
      Alignment previewAlignment = Alignment.center,
      PictureInPictureConfigBuilder? pictureInPictureConfigBuilder,
      AwesomeFilter? defaultFilter,
      List<AwesomeFilter>? availableFilters,
      OnMediaCaptureEvent? onMediaCaptureEvent})
      : this._(
          sensorConfig: sensorConfig ??
              SensorConfig.single(
                sensor: Sensor.position(SensorPosition.back),
              ),
          enablePhysicalButton: enablePhysicalButton,
          progressIndicator: progressIndicator,
          builder: (cameraModeState, preview) {
            return AwesomeCameraLayout(
              state: cameraModeState,
              onMediaTap: onMediaTap,
              topActions: topActionsBuilder?.call(cameraModeState),
              bottomActions: bottomActionsBuilder?.call(cameraModeState),
              middleContent: middleContentBuilder?.call(cameraModeState),
            );
          },
          saveConfig: saveConfig,
          onMediaTap: onMediaTap,
          onImageForAnalysis: onImageForAnalysis,
          imageAnalysisConfig: imageAnalysisConfig,
          onPreviewTapBuilder: onPreviewTapBuilder,
          onPreviewScaleBuilder: onPreviewScaleBuilder,
          previewFit: previewFit ?? CameraPreviewFit.cover,
          previewDecoratorBuilder: previewDecoratorBuilder,
          theme: theme ?? AwesomeTheme(),
          previewPadding: previewPadding,
          previewAlignment: previewAlignment,
          pictureInPictureConfigBuilder: pictureInPictureConfigBuilder,
          defaultFilter: defaultFilter,
          availableFilters: availableFilters ?? awesomePresetFiltersList,
          onMediaCaptureEvent: onMediaCaptureEvent,
        );

  /// 🚧 Experimental
  ///
  /// Documentation on its way, API might change
  CameraAwesomeBuilder.custom({
    SensorConfig? sensorConfig,
    bool mirrorFrontCamera = false,
    bool enablePhysicalButton = false,
    Widget? progressIndicator,
    required CameraLayoutBuilder builder,
    required SaveConfig saveConfig,
    AwesomeFilter? filter,
    OnImageForAnalysis? onImageForAnalysis,
    AnalysisConfig? imageAnalysisConfig,
    OnPreviewTap Function(CameraState)? onPreviewTapBuilder,
    OnPreviewScale Function(CameraState)? onPreviewScaleBuilder,
    CameraPreviewFit? previewFit,
    AwesomeTheme? theme,
    EdgeInsets previewPadding = EdgeInsets.zero,
    Alignment previewAlignment = Alignment.center,
    PictureInPictureConfigBuilder? pictureInPictureConfigBuilder,
    List<AwesomeFilter>? filters,
    OnMediaCaptureEvent? onMediaCaptureEvent,
  }) : this._(
          sensorConfig: sensorConfig ??
              SensorConfig.single(
                sensor: Sensor.position(SensorPosition.back),
              ),
          enablePhysicalButton: enablePhysicalButton,
          progressIndicator: progressIndicator,
          builder: builder,
          saveConfig: saveConfig,
          onMediaTap: null,
          defaultFilter: filter,
          onImageForAnalysis: onImageForAnalysis,
          imageAnalysisConfig: imageAnalysisConfig,
          onPreviewTapBuilder: onPreviewTapBuilder,
          onPreviewScaleBuilder: onPreviewScaleBuilder,
          previewFit: previewFit ?? CameraPreviewFit.cover,
          previewDecoratorBuilder: null,
          theme: theme ?? AwesomeTheme(),
          previewPadding: previewPadding,
          previewAlignment: previewAlignment,
          pictureInPictureConfigBuilder: pictureInPictureConfigBuilder,
          availableFilters: filters,
          onMediaCaptureEvent: onMediaCaptureEvent,
        );

  /// Use this constructor when you don't want to take pictures or record videos.
  /// You can still do image analysis.
  CameraAwesomeBuilder.previewOnly({
    SensorConfig? sensorConfig,
    Widget? progressIndicator,
    required CameraLayoutBuilder builder,
    AwesomeFilter? filter,
    OnImageForAnalysis? onImageForAnalysis,
    AnalysisConfig? imageAnalysisConfig,
    OnPreviewTap Function(CameraState)? onPreviewTapBuilder,
    OnPreviewScale Function(CameraState)? onPreviewScaleBuilder,
    CameraPreviewFit? previewFit,
    EdgeInsets previewPadding = EdgeInsets.zero,
    Alignment previewAlignment = Alignment.center,
    PictureInPictureConfigBuilder? pictureInPictureConfigBuilder,
  }) : this._(
          sensorConfig: sensorConfig ??
              SensorConfig.single(sensor: Sensor.position(SensorPosition.back)),
          enablePhysicalButton: false,
          progressIndicator: progressIndicator,
          builder: builder,
          saveConfig: null,
          onMediaTap: null,
          defaultFilter: filter,
          onImageForAnalysis: onImageForAnalysis,
          imageAnalysisConfig: imageAnalysisConfig,
          onPreviewTapBuilder: onPreviewTapBuilder,
          onPreviewScaleBuilder: onPreviewScaleBuilder,
          previewFit: previewFit ?? CameraPreviewFit.cover,
          previewDecoratorBuilder: null,
          theme: AwesomeTheme(),
          previewPadding: previewPadding,
          previewAlignment: previewAlignment,
          pictureInPictureConfigBuilder: pictureInPictureConfigBuilder,
        );

  /// Use this constructor when you only want to do image analysis.
  ///
  /// E.g.: QR code detection, barcode detection, face detection, etc.
  ///
  /// You can't take pictures or record videos and the preview won't be displayed.
  /// You may still show the image from the analysis by converting it to JPEG
  /// and  displaying that JPEG image.
  CameraAwesomeBuilder.analysisOnly({
    SensorConfig? sensorConfig,
    CameraAspectRatios aspectRatio = CameraAspectRatios.ratio_4_3,
    Widget? progressIndicator,
    required CameraLayoutBuilder builder,
    required OnImageForAnalysis onImageForAnalysis,
    AnalysisConfig? imageAnalysisConfig,
  }) : this._(
          sensorConfig: sensorConfig ??
              SensorConfig.single(sensor: Sensor.position(SensorPosition.back)),
          enablePhysicalButton: false,
          progressIndicator: progressIndicator,
          builder: builder,
          saveConfig: null,
          onMediaTap: null,
          defaultFilter: null,
          onImageForAnalysis: onImageForAnalysis,
          imageAnalysisConfig: imageAnalysisConfig,
          onPreviewTapBuilder: null,
          onPreviewScaleBuilder: null,
          previewFit: CameraPreviewFit.cover,
          previewDecoratorBuilder: null,
          theme: AwesomeTheme(),
          previewPadding: EdgeInsets.zero,
          previewAlignment: Alignment.center,
          showPreview: false,
          pictureInPictureConfigBuilder: null,
        );

  @override
  State<StatefulWidget> createState() {
    return _CameraWidgetBuilder();
  }
}

class _CameraWidgetBuilder extends State<CameraAwesomeBuilder>
    with WidgetsBindingObserver {
  late CameraContext _cameraContext;
  final _cameraPreviewKey = GlobalKey<AwesomeCameraPreviewState>();
  StreamSubscription<MediaCapture?>? _captureStateListener;

  @override
  void dispose() {
    WidgetsBinding.instance.removeObserver(this);
    _cameraContext.dispose();
    _captureStateListener?.cancel();
    super.dispose();
  }

  @override
  void didUpdateWidget(covariant CameraAwesomeBuilder oldWidget) {
    super.didUpdateWidget(oldWidget);
  }

  @override
  void didChangeDependencies() {
    SystemChrome.setPreferredOrientations([DeviceOrientation.portraitUp]);
    super.didChangeDependencies();
  }

  @override
  void didChangeAppLifecycleState(AppLifecycleState state) {
    switch (state) {
      case AppLifecycleState.resumed:
        break;
      case AppLifecycleState.inactive:
      case AppLifecycleState.paused:
      case AppLifecycleState.detached:
        _cameraContext //
            .state
            .when(onVideoRecordingMode: (mode) => mode.stopRecording());
        break;
      case AppLifecycleState.hidden:
        break;
    }
    super.didChangeAppLifecycleState(state);
  }

  @override
  void initState() {
    super.initState();
    WidgetsBinding.instance.addObserver(this);

    _cameraContext = CameraContext.create(
      widget.sensorConfig,
      enablePhysicalButton: widget.enablePhysicalButton,
      filter: widget.defaultFilter ?? AwesomeFilter.None,
      initialCaptureMode: widget.saveConfig?.initialCaptureMode ??
          (widget.showPreview
              ? CaptureMode.preview
              : CaptureMode.analysis_only),
      saveConfig: widget.saveConfig,
      onImageForAnalysis: widget.onImageForAnalysis,
      analysisConfig: widget.imageAnalysisConfig,
      exifPreferences: widget.saveConfig?.exifPreferences ??
          ExifPreferences(saveGPSLocation: false),
      availableFilters: widget.availableFilters,
    );

    // Initial CameraState is always PreparingState
    _cameraContext.state.when(onPreparingCamera: (mode) => mode.start());

    _captureStateListener = _cameraContext.captureState$.listen((mediaCapture) {
      if (mediaCapture != null) {
        widget.onMediaCaptureEvent?.call(mediaCapture);
      }
    });
  }

  @override
  Widget build(BuildContext context) {
    return AwesomeThemeProvider(
      theme: widget.theme,
      child: StreamBuilder<CameraState>(
        stream: _cameraContext.state$,
        builder: (context, snapshot) {
          if (!snapshot.hasData ||
              snapshot.data!.captureMode == null ||
              snapshot.requireData is PreparingCameraState) {
            return widget.progressIndicator ??
                const Center(
                  child: CircularProgressIndicator.adaptive(),
                );
          }
          return Stack(
            fit: StackFit.expand,
            children: <Widget>[
              Positioned.fill(
                child: !widget.showPreview
                    ? widget.builder(
                        snapshot.requireData,
                        AnalysisPreview.hidden(),
                      )
                    : AwesomeCameraPreview(
                        key: _cameraPreviewKey,
                        previewFit: widget.previewFit,
                        state: snapshot.requireData,
                        padding: widget.previewPadding,
                        alignment: widget.previewAlignment,
                        onPreviewTap: widget.onPreviewTapBuilder
                                ?.call(snapshot.requireData) ??
                            OnPreviewTap(
                              onTap: (
                                position,
                                flutterPreviewSize,
                                pixelPreviewSize,
                              ) {
                                snapshot.requireData.when(
                                  onPhotoMode: (photoState) =>
                                      photoState.focusOnPoint(
                                    flutterPosition: position,
                                    pixelPreviewSize: pixelPreviewSize,
                                    flutterPreviewSize: flutterPreviewSize,
                                  ),
                                  onVideoMode: (videoState) =>
                                      videoState.focusOnPoint(
                                    flutterPosition: position,
                                    pixelPreviewSize: pixelPreviewSize,
                                    flutterPreviewSize: flutterPreviewSize,
                                  ),
                                  onVideoRecordingMode: (videoRecState) =>
                                      videoRecState.focusOnPoint(
                                    flutterPosition: position,
                                    pixelPreviewSize: pixelPreviewSize,
                                    flutterPreviewSize: flutterPreviewSize,
                                  ),
                                  onPreviewMode: (previewState) =>
                                      previewState.focusOnPoint(
                                    flutterPosition: position,
                                    pixelPreviewSize: pixelPreviewSize,
                                    flutterPreviewSize: flutterPreviewSize,
                                  ),
                                );
                              },
                            ),
                        onPreviewScale: widget.onPreviewScaleBuilder
                                ?.call(snapshot.requireData) ??
                            OnPreviewScale(
                              onScale: (scale) {
                                snapshot.requireData.sensorConfig
                                    .setZoom(scale);
                              },
                            ),
                        interfaceBuilder: widget.builder,
                        previewDecoratorBuilder: widget.previewDecoratorBuilder,
                        pictureInPictureConfigBuilder:
                            widget.pictureInPictureConfigBuilder,
                      ),
              ),
            ],
          );
        },
      ),
    );
  }
}



================================================
FILE: lib/src/widgets/widgets.dart
================================================
export 'awesome_camera_mode_selector.dart';
export 'awesome_media_preview.dart';
export 'awesome_sensor_type_selector.dart';
export 'buttons/buttons.dart';
export 'camera_awesome_builder.dart';
export 'filters/filters.dart';
export 'layout/layout.dart';
export 'preview/preview.dart';
export 'utils/utils.dart';
export 'zoom/zoom.dart';



================================================
FILE: lib/src/widgets/buttons/awesome_aspect_ratio_button.dart
================================================
import 'package:camerawesome/src/orchestrator/models/models.dart';
import 'package:camerawesome/src/orchestrator/states/photo_camera_state.dart';
import 'package:camerawesome/src/widgets/utils/awesome_circle_icon.dart';
import 'package:camerawesome/src/widgets/utils/awesome_oriented_widget.dart';
import 'package:camerawesome/src/widgets/utils/awesome_theme.dart';
import 'package:flutter/material.dart';

class AwesomeAspectRatioButton extends StatelessWidget {
  final PhotoCameraState state;
  final AwesomeTheme? theme;
  final Widget Function(CameraAspectRatios aspectRatio) iconBuilder;
  final void Function(SensorConfig sensorConfig, CameraAspectRatios aspectRatio)
      onAspectRatioTap;

  AwesomeAspectRatioButton({
    super.key,
    required this.state,
    this.theme,
    Widget Function(CameraAspectRatios aspectRatio)? iconBuilder,
    void Function(SensorConfig sensorConfig, CameraAspectRatios aspectRatio)?
        onAspectRatioTap,
  })  : iconBuilder = iconBuilder ??
            ((aspectRatio) {
              final AssetImage icon;
              double width;
              switch (aspectRatio) {
                case CameraAspectRatios.ratio_16_9:
                  width = 32;
                  icon = const AssetImage(
                      "packages/camerawesome/assets/icons/16_9.png");
                  break;
                case CameraAspectRatios.ratio_4_3:
                  width = 24;
                  icon = const AssetImage(
                      "packages/camerawesome/assets/icons/4_3.png");
                  break;
                case CameraAspectRatios.ratio_1_1:
                  width = 24;
                  icon = const AssetImage(
                      "packages/camerawesome/assets/icons/1_1.png");
                  break;
              }

              return Builder(builder: (context) {
                final iconSize = theme?.buttonTheme.iconSize ??
                    AwesomeThemeProvider.of(context).theme.buttonTheme.iconSize;

                final scaleRatio = iconSize / AwesomeButtonTheme.baseIconSize;
                return AwesomeCircleWidget(
                  theme: theme,
                  child: Center(
                    child: SizedBox(
                      width: iconSize,
                      height: iconSize,
                      child: FittedBox(
                        child: Builder(
                          builder: (context) => Image(
                            image: icon,
                            color: AwesomeThemeProvider.of(context)
                                .theme
                                .buttonTheme
                                .foregroundColor,
                            width: width * scaleRatio,
                          ),
                        ),
                      ),
                    ),
                  ),
                );
              });
            }),
        onAspectRatioTap = onAspectRatioTap ??
            ((sensorConfig, aspectRatio) => sensorConfig.switchCameraRatio());

  @override
  Widget build(BuildContext context) {
    final theme = this.theme ?? AwesomeThemeProvider.of(context).theme;
    return StreamBuilder<SensorConfig>(
      key: const ValueKey("ratioButton"),
      stream: state.sensorConfig$,
      builder: (_, sensorConfigSnapshot) {
        if (!sensorConfigSnapshot.hasData) {
          return const SizedBox.shrink();
        }
        final sensorConfig = sensorConfigSnapshot.requireData;
        return StreamBuilder<CameraAspectRatios>(
          stream: sensorConfig.aspectRatio$,
          builder: (context, snapshot) {
            if (!snapshot.hasData) {
              return const SizedBox.shrink();
            }

            return AwesomeOrientedWidget(
              rotateWithDevice: theme.buttonTheme.rotateWithCamera,
              child: theme.buttonTheme.buttonBuilder(
                iconBuilder(snapshot.requireData),
                () => onAspectRatioTap(sensorConfig, snapshot.requireData),
              ),
            );
          },
        );
      },
    );
  }
}



================================================
FILE: lib/src/widgets/buttons/awesome_camera_switch_button.dart
================================================
import 'package:camerawesome/src/orchestrator/states/camera_state.dart';
import 'package:camerawesome/src/widgets/utils/awesome_circle_icon.dart';
import 'package:camerawesome/src/widgets/utils/awesome_oriented_widget.dart';
import 'package:camerawesome/src/widgets/utils/awesome_theme.dart';
import 'package:flutter/material.dart';

class AwesomeCameraSwitchButton extends StatelessWidget {
  final CameraState state;
  final AwesomeTheme? theme;
  final Widget Function() iconBuilder;
  final void Function(CameraState) onSwitchTap;

  AwesomeCameraSwitchButton({
    super.key,
    required this.state,
    this.theme,
    Widget Function()? iconBuilder,
    void Function(CameraState)? onSwitchTap,
    double scale = 1.3,
  })  : iconBuilder = iconBuilder ??
            (() {
              return AwesomeCircleWidget.icon(
                theme: theme,
                icon: Icons.cameraswitch,
                scale: scale,
              );
            }),
        onSwitchTap = onSwitchTap ?? ((state) => state.switchCameraSensor());

  @override
  Widget build(BuildContext context) {
    final theme = this.theme ?? AwesomeThemeProvider.of(context).theme;

    return AwesomeOrientedWidget(
      rotateWithDevice: theme.buttonTheme.rotateWithCamera,
      child: theme.buttonTheme.buttonBuilder(
        iconBuilder(),
        () => onSwitchTap(state),
      ),
    );
  }
}



================================================
FILE: lib/src/widgets/buttons/awesome_capture_button.dart
================================================
// ignore_for_file: library_private_types_in_public_api

import 'package:camerawesome/src/orchestrator/analysis/analysis_controller.dart';
import 'package:camerawesome/src/orchestrator/states/camera_state.dart';
import 'package:flutter/material.dart';
import 'package:flutter/services.dart';

class AwesomeCaptureButton extends StatefulWidget {
  final CameraState state;

  const AwesomeCaptureButton({
    super.key,
    required this.state,
  });

  @override
  _AwesomeCaptureButtonState createState() => _AwesomeCaptureButtonState();
}

class _AwesomeCaptureButtonState extends State<AwesomeCaptureButton>
    with SingleTickerProviderStateMixin {
  late AnimationController _animationController;
  late double _scale;
  final Duration _duration = const Duration(milliseconds: 100);

  @override
  void initState() {
    super.initState();

    _animationController = AnimationController(
      vsync: this,
      duration: _duration,
      lowerBound: 0.0,
      upperBound: 0.1,
    )..addListener(() {
        setState(() {});
      });
  }

  @override
  void dispose() {
    _animationController.dispose();
    super.dispose();
  }

  @override
  Widget build(BuildContext context) {
    if (widget.state is AnalysisController) {
      return Container();
    }
    _scale = 1 - _animationController.value;

    return GestureDetector(
      onTapDown: _onTapDown,
      onTapUp: _onTapUp,
      onTapCancel: _onTapCancel,
      child: SizedBox(
        key: const ValueKey('cameraButton'),
        height: 80,
        width: 80,
        child: Transform.scale(
          scale: _scale,
          child: CustomPaint(
            painter: widget.state.when(
              onPhotoMode: (_) => CameraButtonPainter(),
              onPreparingCamera: (_) => CameraButtonPainter(),
              onVideoMode: (_) => VideoButtonPainter(),
              onVideoRecordingMode: (_) =>
                  VideoButtonPainter(isRecording: true),
            ),
          ),
        ),
      ),
    );
  }

  _onTapDown(TapDownDetails details) {
    HapticFeedback.selectionClick();
    _animationController.forward();
  }

  _onTapUp(TapUpDetails details) {
    Future.delayed(_duration, () {
      _animationController.reverse();
    });

    onTap.call();
  }

  _onTapCancel() {
    _animationController.reverse();
  }

  get onTap => () {
        widget.state.when(
          onPhotoMode: (photoState) => photoState.takePhoto(),
          onVideoMode: (videoState) => videoState.startRecording(),
          onVideoRecordingMode: (videoState) => videoState.stopRecording(),
        );
      };
}

class CameraButtonPainter extends CustomPainter {
  CameraButtonPainter();

  @override
  void paint(Canvas canvas, Size size) {
    var bgPainter = Paint()
      ..style = PaintingStyle.fill
      ..isAntiAlias = true;
    var radius = size.width / 2;
    var center = Offset(size.width / 2, size.height / 2);
    bgPainter.color = Colors.white.withValues(alpha: .5);
    canvas.drawCircle(center, radius, bgPainter);

    bgPainter.color = Colors.white;
    canvas.drawCircle(center, radius - 8, bgPainter);
  }

  @override
  bool shouldRepaint(CustomPainter oldDelegate) => false;
}

class VideoButtonPainter extends CustomPainter {
  final bool isRecording;

  VideoButtonPainter({
    this.isRecording = false,
  });

  @override
  void paint(Canvas canvas, Size size) {
    var bgPainter = Paint()
      ..style = PaintingStyle.fill
      ..isAntiAlias = true;
    var radius = size.width / 2;
    var center = Offset(size.width / 2, size.height / 2);
    bgPainter.color = Colors.white.withValues(alpha: .5);
    canvas.drawCircle(center, radius, bgPainter);

    if (isRecording) {
      bgPainter.color = Colors.red;
      canvas.drawRRect(
          RRect.fromRectAndRadius(
              Rect.fromLTWH(
                17,
                17,
                size.width - (17 * 2),
                size.height - (17 * 2),
              ),
              const Radius.circular(12.0)),
          bgPainter);
    } else {
      bgPainter.color = Colors.red;
      canvas.drawCircle(center, radius - 8, bgPainter);
    }
  }

  @override
  bool shouldRepaint(CustomPainter oldDelegate) => false;
}



================================================
FILE: lib/src/widgets/buttons/awesome_flash_button.dart
================================================
import 'package:camerawesome/src/widgets/utils/awesome_circle_icon.dart';
import 'package:camerawesome/src/widgets/utils/awesome_theme.dart';
import 'package:flutter/material.dart';

import 'package:camerawesome/src/orchestrator/models/camera_flashes.dart';
import 'package:camerawesome/src/orchestrator/models/sensor_config.dart';
import 'package:camerawesome/src/orchestrator/states/camera_state.dart';
import 'package:camerawesome/src/widgets/utils/awesome_oriented_widget.dart';

class AwesomeFlashButton extends StatelessWidget {
  final CameraState state;
  final AwesomeTheme? theme;
  final Widget Function(FlashMode) iconBuilder;
  final void Function(SensorConfig, FlashMode) onFlashTap;

  AwesomeFlashButton({
    super.key,
    required this.state,
    this.theme,
    Widget Function(FlashMode)? iconBuilder,
    void Function(SensorConfig, FlashMode)? onFlashTap,
  })  : iconBuilder = iconBuilder ??
            ((flashMode) {
              final IconData icon;
              switch (flashMode) {
                case FlashMode.none:
                  icon = Icons.flash_off;
                  break;
                case FlashMode.on:
                  icon = Icons.flash_on;
                  break;
                case FlashMode.auto:
                  icon = Icons.flash_auto;
                  break;
                case FlashMode.always:
                  icon = Icons.flashlight_on;
                  break;
              }
              return AwesomeCircleWidget.icon(
                icon: icon,
                theme: theme,
              );
            }),
        onFlashTap = onFlashTap ??
            ((sensorConfig, flashMode) => sensorConfig.switchCameraFlash());

  @override
  Widget build(BuildContext context) {
    final theme = this.theme ?? AwesomeThemeProvider.of(context).theme;
    return StreamBuilder<SensorConfig>(
      stream: state.sensorConfig$,
      builder: (_, sensorConfigSnapshot) {
        if (!sensorConfigSnapshot.hasData) {
          return const SizedBox.shrink();
        }
        final sensorConfig = sensorConfigSnapshot.requireData;
        return StreamBuilder<FlashMode>(
          stream: sensorConfig.flashMode$,
          builder: (context, snapshot) {
            if (!snapshot.hasData) {
              return const SizedBox.shrink();
            }

            return AwesomeOrientedWidget(
              rotateWithDevice: theme.buttonTheme.rotateWithCamera,
              child: theme.buttonTheme.buttonBuilder(
                iconBuilder(snapshot.requireData),
                () => onFlashTap(sensorConfig, snapshot.requireData),
              ),
            );
          },
        );
      },
    );
  }
}



================================================
FILE: lib/src/widgets/buttons/awesome_location_button.dart
================================================
import 'package:camerawesome/src/widgets/utils/awesome_circle_icon.dart';
import 'package:camerawesome/src/widgets/utils/awesome_oriented_widget.dart';
import 'package:camerawesome/src/widgets/utils/awesome_theme.dart';
import 'package:camerawesome/src/orchestrator/states/photo_camera_state.dart';
import 'package:flutter/material.dart';

class AwesomeLocationButton extends StatelessWidget {
  final PhotoCameraState state;
  final AwesomeTheme? theme;
  final Widget Function(bool saveGpsLocation) iconBuilder;
  final void Function(PhotoCameraState state, bool saveGpsLocation)
      onLocationTap;

  AwesomeLocationButton({
    super.key,
    required this.state,
    this.theme,
    Widget Function(bool saveGpsLocation)? iconBuilder,
    void Function(PhotoCameraState state, bool saveGpsLocation)? onLocationTap,
  })  : iconBuilder = iconBuilder ??
            ((saveGpsLocation) {
              return AwesomeCircleWidget.icon(
                theme: theme,
                icon: saveGpsLocation == true
                    ? Icons.location_pin
                    : Icons.location_off_outlined,
              );
            }),
        onLocationTap = onLocationTap ??
            ((state, saveGpsLocation) =>
                state.shouldSaveGpsLocation(saveGpsLocation));

  @override
  Widget build(BuildContext context) {
    final theme = this.theme ?? AwesomeThemeProvider.of(context).theme;
    return StreamBuilder<bool>(
      stream: state.saveGpsLocation$,
      builder: (context, snapshot) {
        if (!snapshot.hasData) {
          return const SizedBox.shrink();
        }

        return AwesomeOrientedWidget(
          rotateWithDevice: theme.buttonTheme.rotateWithCamera,
          child: theme.buttonTheme.buttonBuilder(
            iconBuilder(snapshot.requireData),
            () => onLocationTap(state, !snapshot.requireData),
          ),
        );
      },
    );
  }
}



================================================
FILE: lib/src/widgets/buttons/awesome_pause_resume_button.dart
================================================
import 'package:camerawesome/src/widgets/utils/awesome_circle_icon.dart';
import 'package:flutter/material.dart';

import 'package:camerawesome/src/orchestrator/models/media_capture.dart';
import 'package:camerawesome/src/orchestrator/states/video_camera_recording_state.dart';
import 'package:camerawesome/src/widgets/utils/awesome_oriented_widget.dart';
import 'package:camerawesome/src/widgets/utils/awesome_theme.dart';

class AwesomePauseResumeButton extends StatefulWidget {
  final VideoRecordingCameraState state;
  final AwesomeTheme? theme;

  const AwesomePauseResumeButton({
    super.key,
    required this.state,
    this.theme,
  });

  @override
  State<StatefulWidget> createState() {
    return _AwesomePauseResumeButtonState();
  }
}

class _AwesomePauseResumeButtonState extends State<AwesomePauseResumeButton>
    with SingleTickerProviderStateMixin {
  late AnimationController _controller;
  late Animation<double> _animation;

  @override
  void initState() {
    _controller = AnimationController(
        vsync: this, duration: const Duration(milliseconds: 300));
    _animation = Tween<double>(begin: 0.0, end: 1.0).animate(_controller);
    super.initState();
  }

  @override
  Widget build(BuildContext context) {
    return StreamBuilder<MediaCapture?>(
      stream: widget.state.captureState$,
      builder: (_, snapshot) {
        if (snapshot.data?.isRecordingVideo != true) {
          return const SizedBox(width: 48);
        }

        bool recordingPaused = snapshot.data!.videoState == VideoState.paused;
        final theme = widget.theme ?? AwesomeThemeProvider.of(context).theme;

        return AwesomeOrientedWidget(
          rotateWithDevice: theme.buttonTheme.rotateWithCamera,
          child: theme.buttonTheme.buttonBuilder(
            AwesomeCircleWidget(
              theme: theme,
              child: Padding(
                padding: const EdgeInsets.all(10.0),
                child: AnimatedIcon(
                  icon: AnimatedIcons.pause_play,
                  progress: _animation,
                  color: theme.buttonTheme.foregroundColor,
                ),
              ),
            ),
            () {
              if (recordingPaused) {
                _controller.reverse();
                widget.state.resumeRecording(snapshot.data!);
              } else {
                _controller.forward();
                widget.state.pauseRecording(snapshot.data!);
              }
            },
          ),
        );
      },
    );
  }
}



================================================
FILE: lib/src/widgets/buttons/buttons.dart
================================================
export 'awesome_aspect_ratio_button.dart';
export 'awesome_camera_switch_button.dart';
export 'awesome_capture_button.dart';
export 'awesome_flash_button.dart';
export 'awesome_location_button.dart';
export 'awesome_pause_resume_button.dart';



================================================
FILE: lib/src/widgets/filters/awesome_filter_button.dart
================================================
import 'package:camerawesome/src/orchestrator/models/camera_flashes.dart';
import 'package:camerawesome/src/orchestrator/models/sensor_config.dart';
import 'package:camerawesome/src/orchestrator/states/camera_state.dart';
import 'package:camerawesome/src/widgets/utils/awesome_circle_icon.dart';
import 'package:camerawesome/src/widgets/utils/awesome_oriented_widget.dart';
import 'package:camerawesome/src/widgets/utils/awesome_theme.dart';
import 'package:flutter/material.dart';

class AwesomeFilterButton extends StatelessWidget {
  final CameraState state;
  final AwesomeTheme? theme;
  final Widget Function() iconBuilder;
  final void Function() onFilterTap;

  AwesomeFilterButton({
    super.key,
    required this.state,
    this.theme,
    Widget Function()? iconBuilder,
    final void Function()? onFilterTap,
  })  : iconBuilder = iconBuilder ??
            (() {
              return AwesomeCircleWidget.icon(
                icon: Icons.filter_rounded,
                theme: theme,
              );
            }),
        onFilterTap = onFilterTap ?? (() => state.toggleFilterSelector());

  @override
  Widget build(BuildContext context) {
    final theme = this.theme ?? AwesomeThemeProvider.of(context).theme;
    return StreamBuilder<SensorConfig>(
      stream: state.sensorConfig$,
      builder: (_, sensorConfigSnapshot) {
        if (!sensorConfigSnapshot.hasData) {
          return const SizedBox.shrink();
        }
        final sensorConfig = sensorConfigSnapshot.requireData;
        return StreamBuilder<FlashMode>(
          stream: sensorConfig.flashMode$,
          builder: (context, snapshot) {
            if (!snapshot.hasData) {
              return const SizedBox.shrink();
            }

            return AwesomeOrientedWidget(
              rotateWithDevice: theme.buttonTheme.rotateWithCamera,
              child: theme.buttonTheme.buttonBuilder(
                iconBuilder(),
                () => onFilterTap(),
              ),
            );
          },
        );
      },
    );
  }
}



================================================
FILE: lib/src/widgets/filters/awesome_filter_name_indicator.dart
================================================
import 'package:camerawesome/camerawesome_plugin.dart';
import 'package:flutter/material.dart';

class AwesomeFilterNameIndicator extends StatelessWidget {
  final CameraState state;

  const AwesomeFilterNameIndicator({
    super.key,
    required this.state,
  });

  @override
  Widget build(BuildContext context) {
    return StreamBuilder<AwesomeFilter>(
      stream: state.filter$,
      builder: (context, snapshot) {
        return snapshot.hasData
            ? Container(
                decoration: BoxDecoration(
                  color: Colors.white70,
                  borderRadius: BorderRadius.circular(4),
                ),
                child: Padding(
                  padding:
                      const EdgeInsets.symmetric(vertical: 5, horizontal: 8),
                  child: Text(
                    snapshot.data!.name.toUpperCase().toString(),
                    style: const TextStyle(
                      color: Colors.black,
                      fontSize: 12,
                      fontWeight: FontWeight.w500,
                    ),
                  ),
                ),
              )
            : const SizedBox.shrink();
      },
    );
  }
}



================================================
FILE: lib/src/widgets/filters/awesome_filter_selector.dart
================================================
import 'package:camerawesome/camerawesome_plugin.dart';
import 'package:carousel_slider/carousel_slider.dart';
import 'package:flutter/material.dart';
import 'package:flutter/services.dart';

class AwesomeFilterSelector extends StatefulWidget {
  final PhotoCameraState state;
  final FilterListPosition filterListPosition;
  final Widget indicator;
  final EdgeInsets? filterListPadding;
  final Color? filterListBackgroundColor;

  const AwesomeFilterSelector({
    super.key,
    required this.state,
    required this.filterListPosition,
    required this.filterListPadding,
    required this.filterListBackgroundColor,
    required this.indicator,
  });

  @override
  State<AwesomeFilterSelector> createState() => _AwesomeFilterSelectorState();
}

class _AwesomeFilterSelectorState extends State<AwesomeFilterSelector> {
  final CarouselSliderController _controller = CarouselSliderController();
  int? _textureId;
  int _selected = 0;

  List<String> get presetsIds =>
      widget.state.availableFilters!.map((e) => e.id).toList();

  @override
  void initState() {
    super.initState();

    _selected = presetsIds.indexOf(widget.state.filter.id);

    widget.state.previewTextureId(0).then((textureId) {
      setState(() {
        _textureId = textureId;
      });
    });
  }

  @override
  Widget build(BuildContext context) {
    final children = [
      widget.indicator,
      Container(
        padding: widget.filterListPadding,
        color: widget.filterListBackgroundColor,
        child: Stack(
          children: [
            CarouselSlider(
              options: CarouselOptions(
                height: 60.0,
                initialPage: _selected,
                onPageChanged: (index, reason) {
                  final filter = awesomePresetFiltersList[index];

                  setState(() {
                    _selected = index;
                  });

                  HapticFeedback.selectionClick();
                  widget.state.setFilter(filter);
                },
                enableInfiniteScroll: false,
                viewportFraction: 0.165,
              ),
              carouselController: _controller,
              items: awesomePresetFiltersList.map((filter) {
                return Builder(
                  builder: (BuildContext context) {
                    return AwesomeBouncingWidget(
                      onTap: () {
                        _controller.animateToPage(
                          presetsIds.indexOf(filter.id),
                          curve: Curves.fastLinearToSlowEaseIn,
                          duration: const Duration(milliseconds: 700),
                        );
                      },
                      child: _FilterPreview(
                        filter: filter.preview,
                        textureId: _textureId,
                      ),
                    );
                  },
                );
              }).toList(),
            ),
            IgnorePointer(
              child: Center(
                child: Container(
                  height: 60,
                  width: 60,
                  decoration: BoxDecoration(
                    color: Colors.transparent,
                    borderRadius: const BorderRadius.all(Radius.circular(9)),
                    border: Border.all(
                      color: Colors.white,
                      width: 3,
                    ),
                  ),
                ),
              ),
            ),
          ],
        ),
      ),
    ];
    return Column(
      children: widget.filterListPosition == FilterListPosition.belowButton
          ? children
          : children.reversed.toList(),
    );
  }
}

class _FilterPreview extends StatelessWidget {
  final ColorFilter filter;
  final int? textureId;

  const _FilterPreview({
    required this.filter,
    required this.textureId,
  });

  @override
  Widget build(BuildContext context) {
    return ClipRRect(
      borderRadius: const BorderRadius.all(Radius.circular(9)),
      child: SizedBox(
        width: 60,
        height: 60,
        child: textureId != null
            ? ColorFiltered(
                colorFilter: filter,
                child: OverflowBox(
                  alignment: Alignment.center,
                  child: FittedBox(
                    fit: BoxFit.cover,
                    child: SizedBox(
                      width: 60,
                      // TODO: maybe this is inverted on Android ??
                      height: 60 / (9 / 16),
                      child: Texture(textureId: textureId!),
                    ),
                  ),
                ),
              )
            : const SizedBox.shrink(),
      ),
    );
  }
}



================================================
FILE: lib/src/widgets/filters/awesome_filter_widget.dart
================================================
import 'dart:io';

import 'package:camerawesome/camerawesome_plugin.dart';
import 'package:camerawesome/src/widgets/filters/awesome_filter_name_indicator.dart';
import 'package:camerawesome/src/widgets/filters/awesome_filter_selector.dart';
import 'package:flutter/foundation.dart';
import 'package:flutter/material.dart';

enum FilterListPosition {
  aboveButton,
  belowButton,
}

class AwesomeFilterWidget extends StatefulWidget {
  final CameraState state;
  final FilterListPosition filterListPosition;
  final EdgeInsets? filterListPadding;
  final Widget indicator;
  final Widget? spacer;
  final Curve animationCurve;
  final Duration animationDuration;

  AwesomeFilterWidget({
    required this.state,
    super.key,
    this.filterListPosition = FilterListPosition.belowButton,
    this.filterListPadding,
    Widget? indicator,
    this.spacer = const SizedBox(height: 8),
    this.animationCurve = Curves.easeInOut,
    this.animationDuration = const Duration(milliseconds: 400),
  }) : indicator = Builder(
          builder: (context) => Container(
            color: AwesomeThemeProvider.of(context)
                .theme
                .bottomActionsBackgroundColor,
            padding: const EdgeInsets.symmetric(vertical: 8.0),
            child: const Center(
              child: SizedBox(
                height: 6,
                width: 6,
                child: DecoratedBox(
                  decoration: BoxDecoration(
                    color: Colors.white,
                    shape: BoxShape.circle,
                  ),
                ),
              ),
            ),
          ),
        );

  @override
  State<AwesomeFilterWidget> createState() => _AwesomeFilterWidgetState();
}

class _AwesomeFilterWidgetState extends State<AwesomeFilterWidget> {
  @override
  Widget build(BuildContext context) {
    final theme = AwesomeThemeProvider.of(context).theme;
    final children = [
      SizedBox(
        height: theme.buttonTheme.iconSize +
            theme.buttonTheme.padding.top +
            theme.buttonTheme.padding.bottom,
        width: double.infinity,
        child: Stack(
          children: [
            Positioned.fill(
              child: StreamBuilder<bool>(
                stream: widget.state.filterSelectorOpened$,
                builder: (_, snapshot) {
                  return AnimatedSwitcher(
                    duration: widget.animationDuration,
                    switchInCurve: widget.animationCurve,
                    switchOutCurve: widget.animationCurve.flipped,
                    child: snapshot.data == true
                        ? Align(
                            key: const ValueKey("NameIndicator"),
                            alignment: widget.filterListPosition ==
                                    FilterListPosition.belowButton
                                ? Alignment.bottomCenter
                                : Alignment.topCenter,
                            child:
                                AwesomeFilterNameIndicator(state: widget.state),
                          )
                        : (!kIsWeb &&
                                Platform
                                    .isAndroid) // FIXME this should not be here and makes the code ugly
                            ? Center(
                                key: const ValueKey("ZoomIndicator"),
                                child: AwesomeZoomSelector(state: widget.state),
                              )
                            : Center(
                                key: const ValueKey("SensorTypeSelector"),
                                child: AwesomeSensorTypeSelector(
                                    state: widget.state),
                              ),
                  );
                },
              ),
            ),
            Positioned(
              bottom:
                  widget.filterListPosition == FilterListPosition.belowButton
                      ? 0
                      : null,
              top: widget.filterListPosition == FilterListPosition.belowButton
                  ? null
                  : 0,
              right: 20,
              child: AwesomeFilterButton(state: widget.state),
            ),
          ],
        ),
      ),
      if (widget.spacer != null) widget.spacer!,
      if (widget.state is PhotoCameraState)
        StreamBuilder<bool>(
          stream: widget.state.filterSelectorOpened$,
          builder: (_, snapshot) {
            return AnimatedClipRect(
              open: snapshot.data == true,
              horizontalAnimation: false,
              verticalAnimation: true,
              alignment:
                  widget.filterListPosition == FilterListPosition.belowButton
                      ? Alignment.topCenter
                      : Alignment.bottomCenter,
              duration: widget.animationDuration,
              curve: widget.animationCurve,
              reverseCurve: widget.animationCurve.flipped,
              child: AwesomeFilterSelector(
                state: widget.state as PhotoCameraState,
                filterListPosition: widget.filterListPosition,
                indicator: widget.indicator,
                filterListBackgroundColor: theme.bottomActionsBackgroundColor,
                filterListPadding: widget.filterListPadding,
              ),
            );
          },
        ),
    ];
    return Column(
      children: widget.filterListPosition == FilterListPosition.belowButton
          ? children
          : children.reversed.toList(),
    );
  }
}



================================================
FILE: lib/src/widgets/filters/filters.dart
================================================
export 'awesome_filter_button.dart';
export 'awesome_filter_widget.dart';



================================================
FILE: lib/src/widgets/layout/awesome_bottom_actions.dart
================================================
import 'package:camerawesome/src/orchestrator/models/media_capture.dart';
import 'package:camerawesome/src/orchestrator/states/states.dart';
import 'package:camerawesome/src/widgets/awesome_media_preview.dart';
import 'package:camerawesome/src/widgets/buttons/awesome_camera_switch_button.dart';
import 'package:camerawesome/src/widgets/buttons/awesome_capture_button.dart';
import 'package:camerawesome/src/widgets/buttons/awesome_pause_resume_button.dart';
import 'package:camerawesome/src/widgets/camera_awesome_builder.dart';
import 'package:camerawesome/src/widgets/utils/awesome_theme.dart';
import 'package:flutter/material.dart';

class AwesomeBottomActions extends StatelessWidget {
  final CameraState state;
  final Widget left;
  final Widget right;
  final Widget captureButton;
  final EdgeInsets padding;

  AwesomeBottomActions({
    super.key,
    required this.state,
    Widget? left,
    Widget? right,
    Widget? captureButton,
    OnMediaTap? onMediaTap,
    this.padding = const EdgeInsets.symmetric(vertical: 8),
  })  : captureButton = captureButton ??
            AwesomeCaptureButton(
              state: state,
            ),
        left = left ??
            (state is VideoRecordingCameraState
                ? AwesomePauseResumeButton(
                    state: state,
                  )
                : Builder(builder: (context) {
                    final theme = AwesomeThemeProvider.of(context).theme;
                    return AwesomeCameraSwitchButton(
                      state: state,
                      theme: theme.copyWith(
                        buttonTheme: theme.buttonTheme.copyWith(
                          backgroundColor: Colors.white12,
                        ),
                      ),
                    );
                  })),
        right = right ??
            (state is VideoRecordingCameraState
                ? const SizedBox(width: 48)
                : StreamBuilder<MediaCapture?>(
                    stream: state.captureState$,
                    builder: (context, snapshot) {
                      if (!snapshot.hasData) {
                        return const SizedBox(width: 60, height: 60);
                      }
                      return SizedBox(
                        width: 60,
                        child: AwesomeMediaPreview(
                          mediaCapture: snapshot.requireData,
                          onMediaTap: onMediaTap,
                        ),
                      );
                    },
                  ));

  @override
  Widget build(BuildContext context) {
    return Padding(
      padding: padding,
      child: Row(
        mainAxisAlignment: MainAxisAlignment.spaceEvenly,
        children: [
          Expanded(
            child: Center(
              child: left,
            ),
          ),
          captureButton,
          Expanded(
            child: Center(
              child: right,
            ),
          ),
        ],
      ),
    );
  }
}



================================================
FILE: lib/src/widgets/layout/awesome_camera_layout.dart
================================================
// ignore_for_file: unused_import

import 'dart:io';

import 'package:camerawesome/src/orchestrator/models/capture_modes.dart';
import 'package:camerawesome/src/orchestrator/states/states.dart';
import 'package:camerawesome/src/widgets/awesome_camera_mode_selector.dart';
import 'package:camerawesome/src/widgets/camera_awesome_builder.dart';
import 'package:camerawesome/src/widgets/filters/awesome_filter_widget.dart';
import 'package:camerawesome/src/widgets/layout/layout.dart';
import 'package:camerawesome/src/widgets/utils/awesome_theme.dart';
import 'package:camerawesome/src/widgets/zoom/awesome_zoom_selector.dart';
import 'package:flutter/foundation.dart';
import 'package:flutter/material.dart';

/// This widget doesn't handle [PreparingCameraState]
class AwesomeCameraLayout extends StatelessWidget {
  final CameraState state;
  final Widget middleContent;
  final Widget topActions;
  final Widget bottomActions;

  AwesomeCameraLayout({
    super.key,
    required this.state,
    OnMediaTap? onMediaTap,
    Widget? middleContent,
    Widget? topActions,
    Widget? bottomActions,
  })  : middleContent = middleContent ??
            (Column(
              children: [
                const Spacer(),
                if (state is PhotoCameraState && state.hasFilters)
                  AwesomeFilterWidget(state: state)
                else if (!kIsWeb && Platform.isAndroid)
                  AwesomeZoomSelector(state: state),
                AwesomeCameraModeSelector(state: state),
              ],
            )),
        topActions = topActions ?? AwesomeTopActions(state: state),
        bottomActions = bottomActions ??
            AwesomeBottomActions(state: state, onMediaTap: onMediaTap);

  @override
  Widget build(BuildContext context) {
    final theme = AwesomeThemeProvider.of(context).theme;
    return SafeArea(
      bottom: false,
      child: Column(
        children: [
          topActions,
          Expanded(child: middleContent),
          Container(
            color: theme.bottomActionsBackgroundColor,
            child: SafeArea(
              top: false,
              child: Column(
                children: [
                  bottomActions,
                ],
              ),
            ),
          ),
        ],
      ),
    );
  }
}



================================================
FILE: lib/src/widgets/layout/awesome_top_actions.dart
================================================
import 'package:camerawesome/src/orchestrator/states/states.dart';
import 'package:camerawesome/src/widgets/buttons/awesome_aspect_ratio_button.dart';
import 'package:camerawesome/src/widgets/buttons/awesome_flash_button.dart';
import 'package:camerawesome/src/widgets/buttons/awesome_location_button.dart';
import 'package:flutter/material.dart';

class AwesomeTopActions extends StatelessWidget {
  final CameraState state;

  /// Show only children that are relevant to the current [state]
  final List<Widget> children;
  final EdgeInsets padding;

  AwesomeTopActions({
    super.key,
    required this.state,
    List<Widget>? children,
    this.padding = const EdgeInsets.only(left: 30, right: 30, top: 16),
  }) : children = children ??
            (state is VideoRecordingCameraState
                ? [const SizedBox.shrink()]
                : [
                    AwesomeFlashButton(state: state),
                    if (state is PhotoCameraState)
                      AwesomeAspectRatioButton(state: state),
                    if (state is PhotoCameraState)
                      AwesomeLocationButton(state: state),
                  ]);

  @override
  Widget build(BuildContext context) {
    return Padding(
      padding: padding,
      child: Row(
        mainAxisAlignment: MainAxisAlignment.spaceBetween,
        children: children,
      ),
    );
  }
}



================================================
FILE: lib/src/widgets/layout/layout.dart
================================================
export 'awesome_top_actions.dart';
export 'awesome_camera_layout.dart';
export 'awesome_bottom_actions.dart';



================================================
FILE: lib/src/widgets/preview/awesome_camera_floating_preview.dart
================================================
import 'package:camerawesome/pigeon.dart';
import 'package:camerawesome/src/orchestrator/models/sensors.dart';
import 'package:camerawesome/src/widgets/preview/awesome_preview_fit.dart';
import 'package:camerawesome/src/widgets/widgets.dart';
import 'package:flutter/material.dart';

class AwesomeCameraFloatingPreview extends StatefulWidget {
  final Texture texture;
  final int index;
  final double aspectRatio;
  final Sensor sensor;
  final PictureInPictureConfig pictureInPictureConfig;

  AwesomeCameraFloatingPreview({
    super.key,
    required this.index,
    required this.sensor,
    required this.texture,
    required this.aspectRatio,
    PictureInPictureConfig? pictureInPictureConfig,
  }) : pictureInPictureConfig =
            pictureInPictureConfig ?? PictureInPictureConfig(sensor: sensor);

  @override
  State<AwesomeCameraFloatingPreview> createState() =>
      _AwesomeCameraFloatingPreviewState();
}

class _AwesomeCameraFloatingPreviewState
    extends State<AwesomeCameraFloatingPreview> {
  late Offset _position;

  @override
  void initState() {
    super.initState();
    _position = widget.pictureInPictureConfig.startingPosition;
  }

  @override
  Widget build(BuildContext context) {
    return Positioned(
      left: _position.dx,
      top: _position.dy,
      child: AwesomeBouncingWidget(
        // TODO We can tap behind the preview with the current AwesomeBouncingWidget implementation
        onTap: widget.pictureInPictureConfig.onTap,
        disabledOpacity: 1.0,
        child: GestureDetector(
          onPanUpdate: widget.pictureInPictureConfig.isDraggable
              ? (details) {
                  setState(() {
                    _position = Offset(
                      _position.dx + details.delta.dx,
                      _position.dy + details.delta.dy,
                    );
                  });
                }
              : null,
          child: widget.pictureInPictureConfig.pictureInPictureBuilder(
              AnimatedPreviewFit(
                previewFit: CameraPreviewFit.cover,
                previewSize: PreviewSize(
                  width: 1000,
                  height: 1000 / widget.aspectRatio,
                ), // FIXME  we don't know preview size of other sensors
                constraints: const BoxConstraints(
                  maxWidth: 300,
                  maxHeight: 300,
                ),
                sensor: widget.sensor,
                child: widget.texture,
              ),
              widget.aspectRatio),
        ),
      ),
    );
  }
}



================================================
FILE: lib/src/widgets/preview/awesome_camera_gesture_detector.dart
================================================
import 'dart:async';

import 'package:camerawesome/pigeon.dart';
import 'package:flutter/gestures.dart';
import 'package:flutter/material.dart';

import 'package:camerawesome/src/widgets/preview/awesome_focus_indicator.dart';

Widget _awesomeFocusBuilder(Offset tapPosition) {
  return AwesomeFocusIndicator(position: tapPosition);
}

class OnPreviewTapBuilder {
  // Use getters instead of storing the direct value to retrieve the data onTap
  final PreviewSize Function() pixelPreviewSizeGetter;
  final PreviewSize Function() flutterPreviewSizeGetter;
  final OnPreviewTap onPreviewTap;

  const OnPreviewTapBuilder({
    required this.pixelPreviewSizeGetter,
    required this.flutterPreviewSizeGetter,
    required this.onPreviewTap,
  });
}

class OnPreviewTap {
  final Function(Offset position, PreviewSize flutterPreviewSize,
      PreviewSize pixelPreviewSize) onTap;
  final Widget Function(Offset tapPosition)? onTapPainter;
  final Duration? tapPainterDuration;

  const OnPreviewTap({
    required this.onTap,
    this.onTapPainter = _awesomeFocusBuilder,
    this.tapPainterDuration = const Duration(milliseconds: 2000),
  });
}

class OnPreviewScale {
  final Function(double scale) onScale;

  const OnPreviewScale({
    required this.onScale,
  });
}

class AwesomeCameraGestureDetector extends StatefulWidget {
  final Widget child;
  final OnPreviewTapBuilder? onPreviewTapBuilder;
  final OnPreviewScale? onPreviewScale;
  final double initialZoom;

  const AwesomeCameraGestureDetector({
    super.key,
    required this.child,
    required this.onPreviewScale,
    this.onPreviewTapBuilder,
    this.initialZoom = 0,
  });

  @override
  State<StatefulWidget> createState() {
    return _AwesomeCameraGestureDetector();
  }
}

class _AwesomeCameraGestureDetector
    extends State<AwesomeCameraGestureDetector> {
  double _zoomScale = 0;
  final double _accuracy = 0.01;
  double? _lastScale;

  Offset? _tapPosition;
  Timer? _timer;

  @override
  void initState() {
    _zoomScale = widget.initialZoom;
    super.initState();
  }

  @override
  Widget build(BuildContext context) {
    return RawGestureDetector(
      gestures: <Type, GestureRecognizerFactory>{
        if (widget.onPreviewScale != null)
          ScaleGestureRecognizer:
              GestureRecognizerFactoryWithHandlers<ScaleGestureRecognizer>(
            () => ScaleGestureRecognizer()
              ..onStart = (_) {
                _lastScale = null;
              }
              ..onUpdate = (ScaleUpdateDetails details) {
                _lastScale ??= details.scale;
                if (details.scale < (_lastScale! + 0.01) &&
                    details.scale > (_lastScale! - 0.01)) {
                  return;
                } else if (_lastScale! < details.scale) {
                  _zoomScale += _accuracy;
                } else {
                  _zoomScale -= _accuracy;
                }

                _zoomScale = _zoomScale.clamp(0, 1);
                widget.onPreviewScale!.onScale(_zoomScale);
                _lastScale = details.scale;
              },
            (instance) {},
          ),
        if (widget.onPreviewTapBuilder != null)
          TapGestureRecognizer:
              GestureRecognizerFactoryWithHandlers<TapGestureRecognizer>(
            () => TapGestureRecognizer()
              ..onTapUp = (details) {
                if (widget
                        .onPreviewTapBuilder!.onPreviewTap.tapPainterDuration !=
                    null) {
                  _timer?.cancel();
                  _timer = Timer(
                      widget.onPreviewTapBuilder!.onPreviewTap
                          .tapPainterDuration!, () {
                    setState(() {
                      _tapPosition = null;
                    });
                  });
                }
                setState(() {
                  _tapPosition = details.localPosition;
                });
                widget.onPreviewTapBuilder!.onPreviewTap.onTap(
                  _tapPosition!,
                  widget.onPreviewTapBuilder!.flutterPreviewSizeGetter(),
                  widget.onPreviewTapBuilder!.pixelPreviewSizeGetter(),
                );
              },
            (instance) {},
          ),
      },
      child: Stack(children: [
        Positioned.fill(child: widget.child),
        if (_tapPosition != null &&
            widget.onPreviewTapBuilder?.onPreviewTap.onTapPainter != null)
          widget.onPreviewTapBuilder!.onPreviewTap.onTapPainter!(_tapPosition!),
      ]),
    );
  }

  @override
  dispose() {
    _timer?.cancel();
    super.dispose();
  }
}



================================================
FILE: lib/src/widgets/preview/awesome_camera_preview.dart
================================================
import 'dart:async';
import 'dart:io';

import 'package:camerawesome/camerawesome_plugin.dart';
import 'package:camerawesome/pigeon.dart';
import 'package:camerawesome/src/widgets/preview/awesome_preview_fit.dart';
import 'package:flutter/cupertino.dart';
import 'package:flutter/foundation.dart';
import 'package:flutter/material.dart';

enum CameraPreviewFit {
  fitWidth,
  fitHeight,
  contain,
  cover,
}

/// This is a fullscreen camera preview
/// some part of the preview are cropped so we have a full sized camera preview
class AwesomeCameraPreview extends StatefulWidget {
  final CameraPreviewFit previewFit;
  final Widget? loadingWidget;
  final CameraState state;
  final OnPreviewTap? onPreviewTap;
  final OnPreviewScale? onPreviewScale;
  final CameraLayoutBuilder interfaceBuilder;
  final CameraLayoutBuilder? previewDecoratorBuilder;
  final EdgeInsets padding;
  final Alignment alignment;
  final PictureInPictureConfigBuilder? pictureInPictureConfigBuilder;

  const AwesomeCameraPreview({
    super.key,
    this.loadingWidget,
    required this.state,
    this.onPreviewTap,
    this.onPreviewScale,
    this.previewFit = CameraPreviewFit.cover,
    required this.interfaceBuilder,
    this.previewDecoratorBuilder,
    required this.padding,
    required this.alignment,
    this.pictureInPictureConfigBuilder,
  });

  @override
  State<StatefulWidget> createState() {
    return AwesomeCameraPreviewState();
  }
}

class AwesomeCameraPreviewState extends State<AwesomeCameraPreview> {
  PreviewSize? _previewSize;

  final List<Texture> _textures = [];

  PreviewSize? get pixelPreviewSize => _previewSize;

  StreamSubscription? _sensorConfigSubscription;
  StreamSubscription? _aspectRatioSubscription;
  CameraAspectRatios? _aspectRatio;
  double? _aspectRatioValue;
  AnalysisPreview? _preview;

  // TODO: fetch this value from the native side
  final int kMaximumSupportedFloatingPreview = 3;

  @override
  void initState() {
    super.initState();
    Future.wait([
      widget.state.previewSize(0),
      _loadTextures(),
    ]).then((data) {
      if (mounted) {
        setState(() {
          _previewSize = data[0];
        });
      }
    });

    // refactor this
    _sensorConfigSubscription =
        widget.state.sensorConfig$.listen((sensorConfig) {
      _aspectRatioSubscription?.cancel();
      _aspectRatioSubscription =
          sensorConfig.aspectRatio$.listen((event) async {
        final previewSize = await widget.state.previewSize(0);
        if ((_previewSize != previewSize || _aspectRatio != event) && mounted) {
          setState(() {
            _aspectRatio = event;
            switch (event) {
              case CameraAspectRatios.ratio_16_9:
                _aspectRatioValue = 16 / 9;
                break;
              case CameraAspectRatios.ratio_4_3:
                _aspectRatioValue = 4 / 3;
                break;
              case CameraAspectRatios.ratio_1_1:
                _aspectRatioValue = 1;
                break;
            }
            _previewSize = previewSize;
          });
        }
      });
    });
  }

  Future _loadTextures() async {
    // ignore: invalid_use_of_protected_member
    final sensors = widget.state.cameraContext.sensorConfig.sensors.length;

    // Set it to true to debug the floating preview on a device that doesn't
    // support multicam
    // ignore: dead_code
    if (false) {
      for (int i = 0; i < 2; i++) {
        final textureId = await widget.state.previewTextureId(0);
        if (textureId != null) {
          _textures.add(
            Texture(textureId: textureId),
          );
        }
      }
    } else {
      for (int i = 0; i < sensors; i++) {
        final textureId = await widget.state.previewTextureId(i);
        if (textureId != null) {
          _textures.add(
            Texture(textureId: textureId),
          );
        }
      }
    }
  }

  @override
  void dispose() {
    _sensorConfigSubscription?.cancel();
    _aspectRatioSubscription?.cancel();
    super.dispose();
  }

  @override
  Widget build(BuildContext context) {
    if (_textures.isEmpty || _previewSize == null || _aspectRatio == null) {
      return widget.loadingWidget ??
          Center(
            child: Platform.isIOS
                ? const CupertinoActivityIndicator()
                : const CircularProgressIndicator(),
          );
    }

    return Container(
      color: Colors.black,
      child: LayoutBuilder(
        builder: (context, constraints) {
          return Stack(
            children: [
              Positioned.fill(
                child: AnimatedPreviewFit(
                  alignment: widget.alignment,
                  previewFit: widget.previewFit,
                  previewSize: _previewSize!,
                  constraints: constraints,
                  sensor: widget.state.sensorConfig.sensors.first,
                  onPreviewCalculated: (preview) {
                    WidgetsBinding.instance.addPostFrameCallback((timeStamp) {
                      if (mounted) {
                        setState(() {
                          _preview = preview;
                        });
                      }
                    });
                  },
                  child: AwesomeCameraGestureDetector(
                    onPreviewTapBuilder:
                        widget.onPreviewTap != null && _previewSize != null
                            ? OnPreviewTapBuilder(
                                pixelPreviewSizeGetter: () => _previewSize!,
                                flutterPreviewSizeGetter: () =>
                                    _previewSize!, //croppedPreviewSize,
                                onPreviewTap: widget.onPreviewTap!,
                              )
                            : null,
                    onPreviewScale: widget.onPreviewScale,
                    initialZoom: widget.state.sensorConfig.zoom,
                    child: StreamBuilder<AwesomeFilter>(
                      //FIX performances
                      stream: widget.state.filter$,
                      builder: (context, snapshot) {
                        return snapshot.hasData &&
                                snapshot.data != AwesomeFilter.None
                            ? ColorFiltered(
                                colorFilter: snapshot.data!.preview,
                                child: _textures.first,
                              )
                            : _textures.first;
                      },
                    ),
                  ),
                ),
              ),
              if (widget.previewDecoratorBuilder != null && _preview != null)
                Positioned.fill(
                  child: widget.previewDecoratorBuilder!(
                    widget.state,
                    _preview!,
                  ),
                ),
              if (_preview != null)
                Positioned.fill(
                  child: widget.interfaceBuilder(
                    widget.state,
                    _preview!,
                  ),
                ),
              // TODO: be draggable
              // TODO: add shadow & border
              ..._buildPreviewTextures(),
            ],
          );
        },
      ),
    );
  }

  List<Widget> _buildPreviewTextures() {
    final previewFrames = <Widget>[];
    // if there is only one texture
    if (_textures.length <= 1) {
      return previewFrames;
    }
    // ignore: invalid_use_of_protected_member
    final sensors = widget.state.cameraContext.sensorConfig.sensors;

    for (int i = 1; i < _textures.length; i++) {
      // TODO: add a way to retrive how camera can be added ("budget" on iOS ?)
      if (i >= kMaximumSupportedFloatingPreview) {
        break;
      }

      final texture = _textures[i];
      final sensor = sensors[kDebugMode ? 0 : i];
      final frame = AwesomeCameraFloatingPreview(
        index: i,
        sensor: sensor,
        texture: texture,
        aspectRatio: 1 / _aspectRatioValue!,
        pictureInPictureConfig:
            widget.pictureInPictureConfigBuilder?.call(i, sensor) ??
                PictureInPictureConfig(
                  startingPosition: Offset(
                    i * 20,
                    MediaQuery.of(context).padding.top + 60 + (i * 20),
                  ),
                  sensor: sensor,
                ),
      );
      previewFrames.add(frame);
    }

    return previewFrames;
  }
}



================================================
FILE: lib/src/widgets/preview/awesome_focus_indicator.dart
================================================
import 'dart:io';

import 'package:flutter/material.dart';

class AwesomeFocusIndicator extends StatelessWidget {
  final Offset position;

  const AwesomeFocusIndicator({super.key, required this.position});

  @override
  Widget build(BuildContext context) {
    return IgnorePointer(
      child: TweenAnimationBuilder<double>(
        key: ValueKey(position),
        tween: Tween<double>(
          begin: 80,
          end: 50,
        ),
        duration: const Duration(milliseconds: 2000),
        curve: Curves.fastLinearToSlowEaseIn,
        builder: (_, anim, child) {
          return CustomPaint(
            painter: AwesomeFocusPainter(
              tapPosition: position,
              rectSize: anim,
            ),
          );
        },
      ),
    );
  }
}

class AwesomeFocusPainter extends CustomPainter {
  final double rectSize;
  final Offset tapPosition;

  AwesomeFocusPainter({required this.tapPosition, required this.rectSize});

  @override
  void paint(Canvas canvas, Size size) {
    final isIOS = Platform.isIOS;

    final baseX = tapPosition.dx - rectSize / 2;
    final baseY = tapPosition.dy - rectSize / 2;

    Path pathAndroid = Path()
      ..moveTo(baseX, baseY)
      ..lineTo(baseX + rectSize / 5, baseY)
      ..moveTo(baseX + 4 * rectSize / 5, baseY)
      ..lineTo(baseX + rectSize, baseY)
      ..lineTo(baseX + rectSize, baseY + rectSize / 5)
      ..moveTo(baseX + rectSize, baseY + 4 * rectSize / 5)
      ..lineTo(baseX + rectSize, baseY + rectSize)
      ..lineTo(baseX + 4 * rectSize / 5, baseY + rectSize)
      ..moveTo(baseX + rectSize / 5, baseY + rectSize)
      ..lineTo(baseX, baseY + rectSize)
      ..lineTo(baseX, baseY + 4 * rectSize / 5)
      ..moveTo(baseX, baseY + rectSize / 5)
      ..lineTo(baseX, baseY);

    Path pathIOS = Path()
      ..moveTo(baseX, baseY)
      ..lineTo(baseX + rectSize / 2, baseY)
      ..lineTo(baseX + rectSize / 2, baseY + rectSize / 10)
      ..moveTo(baseX + rectSize / 2, baseY)
      ..lineTo(baseX + rectSize, baseY)
      ..lineTo(baseX + rectSize, baseY + rectSize / 2)
      ..lineTo(baseX + 9 / 10 * rectSize, baseY + rectSize / 2)
      ..moveTo(baseX + rectSize, baseY + rectSize / 2)
      ..lineTo(baseX + rectSize, baseY + rectSize)
      ..lineTo(baseX + rectSize / 2, baseY + rectSize)
      ..lineTo(baseX + rectSize / 2, baseY + 9 / 10 * rectSize)
      ..moveTo(baseX + rectSize / 2, baseY + rectSize)
      ..lineTo(baseX, baseY + rectSize)
      ..lineTo(baseX, baseY + rectSize / 2)
      ..lineTo(baseX + 1 / 10 * rectSize, baseY + rectSize / 2)
      ..moveTo(baseX, baseY + rectSize / 2)
      ..lineTo(baseX, baseY);

    canvas.drawPath(
      isIOS ? pathIOS : pathAndroid,
      Paint()
        ..color = Colors.white
        ..style = PaintingStyle.stroke
        ..strokeWidth = 1.5
        ..strokeJoin = StrokeJoin.round
        ..strokeCap = StrokeCap.round,
    );
  }

  @override
  bool shouldRepaint(covariant AwesomeFocusPainter oldDelegate) {
    return rectSize != oldDelegate.rectSize ||
        tapPosition != oldDelegate.tapPosition;
  }
}



================================================
FILE: lib/src/widgets/preview/awesome_preview_fit.dart
================================================
import 'dart:math';

import 'package:camerawesome/camerawesome_plugin.dart';
import 'package:camerawesome/pigeon.dart';
import 'package:flutter/material.dart';

final previewWidgetKey = GlobalKey();

typedef OnPreviewCalculated = void Function(AnalysisPreview preview);

class AnimatedPreviewFit extends StatefulWidget {
  final Alignment alignment;
  final CameraPreviewFit previewFit;
  final PreviewSize previewSize;
  final BoxConstraints constraints;
  final Widget child;
  final OnPreviewCalculated? onPreviewCalculated;
  final Sensor sensor;

  const AnimatedPreviewFit({
    super.key,
    this.alignment = Alignment.center,
    required this.previewFit,
    required this.previewSize,
    required this.constraints,
    required this.sensor,
    required this.child,
    this.onPreviewCalculated,
  });

  @override
  State<AnimatedPreviewFit> createState() => _AnimatedPreviewFitState();
}

class _AnimatedPreviewFitState extends State<AnimatedPreviewFit> {
  late Tween<Size> animation;
  Size? maxSize;

  PreviewSizeCalculator? sizeCalculator;

  @override
  void initState() {
    super.initState();
    sizeCalculator = PreviewSizeCalculator(
      previewFit: widget.previewFit,
      previewSize: widget.previewSize,
      constraints: widget.constraints,
    );
    sizeCalculator!.compute();
    maxSize = sizeCalculator!.maxSize;

    animation = Tween<Size>(
      begin: maxSize,
      end: maxSize,
    );
    _handPreviewCalculated();
  }

  @override
  void didUpdateWidget(covariant AnimatedPreviewFit oldWidget) {
    super.didUpdateWidget(oldWidget);
    if (widget.previewFit != oldWidget.previewFit ||
        widget.previewSize != oldWidget.previewSize ||
        widget.constraints != oldWidget.constraints) {
      var oldsizeCalculator = PreviewSizeCalculator(
        previewFit: oldWidget.previewFit,
        previewSize: oldWidget.previewSize,
        constraints: oldWidget.constraints,
      );
      sizeCalculator = PreviewSizeCalculator(
        previewFit: widget.previewFit,
        previewSize: widget.previewSize,
        constraints: widget.constraints,
      );
      oldsizeCalculator.compute();
      sizeCalculator!.compute();
      animation = Tween<Size>(
        begin: oldsizeCalculator.maxSize,
        end: sizeCalculator!.maxSize,
      );
      _handPreviewCalculated();
    }
  }

  void _handPreviewCalculated() {
    if (widget.onPreviewCalculated != null) {
      widget.onPreviewCalculated!(
        AnalysisPreview(
          nativePreviewSize: widget.previewSize.toSize(),
          previewSize: sizeCalculator!.maxSize,
          offset: sizeCalculator!.offset,
          scale: sizeCalculator!.zoom,
          sensor: widget.sensor,
        ),
      );
    }
  }

  @override
  Widget build(BuildContext context) {
    // WidgetsBinding.instance.addPostFrameCallback((timeStamp) {
    // final RenderBox renderBox =
    //     previewWidgetKey.currentContext?.findRenderObject() as RenderBox;
    // final position = renderBox.localToGlobal(Offset.zero);
    // this contains the translations from the top left corner of the screen
    // debugPrint(
    //     "==> position ${position.dx}, ${position.dy} | ${renderBox.size}");
    // });

    return TweenAnimationBuilder<Size>(
      builder: (context, currentSize, child) {
        final ratio = sizeCalculator!.zoom;
        return PreviewFitWidget(
          alignment: widget.alignment,
          constraints: widget.constraints,
          previewFit: widget.previewFit,
          previewSize: widget.previewSize,
          scale: ratio,
          maxSize: maxSize!,
          child: child!,
        );
      },
      tween: animation,
      duration: const Duration(milliseconds: 700),
      curve: Curves.fastLinearToSlowEaseIn,
      child: widget.child,
    );
  }
}

class PreviewFitWidget extends StatelessWidget {
  final Alignment alignment;
  final BoxConstraints constraints;
  final CameraPreviewFit previewFit;
  final PreviewSize previewSize;
  final Widget child;
  final double scale;
  final Size maxSize;

  const PreviewFitWidget({
    super.key,
    required this.alignment,
    required this.constraints,
    required this.previewFit,
    required this.previewSize,
    required this.child,
    required this.scale,
    required this.maxSize,
  });

  @override
  Widget build(BuildContext context) {
    final transformController = TransformationController()
      ..value = (Matrix4.identity()..scale(scale));
    return Align(
      alignment: alignment,
      child: SizedBox(
        width: maxSize.width,
        height: maxSize.height,
        child: InteractiveViewer(
          // key: previewWidgetKey,
          transformationController: transformController,
          scaleEnabled: false,
          constrained: false,
          panEnabled: false,
          alignment: FractionalOffset.topLeft,
          clipBehavior: Clip.antiAlias,
          child: Align(
            alignment: Alignment.topLeft,
            child: SizedBox(
              width: previewSize.width,
              height: previewSize.height,
              child: child,
            ),
          ),
        ),
      ),
    );
  }

  double get previewRatio => previewSize.width / previewSize.height;
}

class PreviewSizeCalculator {
  final CameraPreviewFit previewFit;
  final PreviewSize previewSize;
  final BoxConstraints constraints;

  Size? _maxSize;
  double? _zoom;
  Offset? _offset;

  PreviewSizeCalculator({
    required this.previewFit,
    required this.previewSize,
    required this.constraints,
  });

  void compute() {
    _zoom ??= _computeZoom();
    _maxSize ??= _computeMaxSize();
  }

  double get zoom {
    if (_zoom == null) {
      throw Exception("Call compute() before");
    }
    return _zoom!;
  }

  Size get maxSize {
    if (_maxSize == null) {
      throw Exception("Call compute() before");
    }
    return _maxSize!;
  }

  Offset get offset {
    if (_offset == null) {
      throw Exception("Call compute() before");
    }
    return _offset!;
  }

  Size _computeMaxSize() {
    var nativePreviewSize = previewSize.toSize();
    Size maxSize;
    final nativeWidthProjection = constraints.maxWidth * 1 / zoom;
    final wDiff = nativePreviewSize.width - nativeWidthProjection;

    final nativeHeightProjection = constraints.maxHeight * 1 / zoom;
    final hDiff = nativePreviewSize.height - nativeHeightProjection;

    switch (previewFit) {
      case CameraPreviewFit.fitWidth:
        maxSize = Size(constraints.maxWidth, nativePreviewSize.height * zoom);
        _offset = Offset(0, constraints.maxHeight - maxSize.height);
        break;
      case CameraPreviewFit.fitHeight:
        maxSize = Size(nativePreviewSize.width * zoom, constraints.maxHeight);
        _offset = Offset(constraints.maxWidth - maxSize.width, 0);
        break;
      case CameraPreviewFit.cover:
        maxSize = Size(constraints.maxWidth, constraints.maxHeight);

        if (constraints.maxWidth / constraints.maxHeight >
            previewSize.width / previewSize.height) {
          _offset = Offset((hDiff * zoom) * 2, 0);
          // _offset = Offset(0, constraints.maxHeight - maxSize.height);
        } else {
          _offset = Offset(0, (wDiff * zoom));
          // _offset = Offset(constraints.maxWidth - maxSize.width, 0);
        }
        break;
      case CameraPreviewFit.contain:
        maxSize = Size(
            nativePreviewSize.width * zoom, nativePreviewSize.height * zoom);
        _offset = Offset(
          constraints.maxWidth - maxSize.width,
          constraints.maxHeight - maxSize.height,
        );
        break;
    }

    return maxSize;
  }

  PreviewSize getMaxPreviewSize() {
    return PreviewSize(
      width: maxSize.width,
      height: maxSize.height,
    );
  }

  double _computeZoom() {
    late double ratio;
    var nativePreviewSize = previewSize.toSize();

    switch (previewFit) {
      case CameraPreviewFit.fitWidth:
        ratio = constraints.maxWidth / nativePreviewSize.width; // 800 / 960
        break;
      case CameraPreviewFit.fitHeight:
        ratio = constraints.maxHeight / nativePreviewSize.height; // 1220 / 1280
        break;
      case CameraPreviewFit.cover:
        if (constraints.maxWidth / constraints.maxHeight >
            nativePreviewSize.width / nativePreviewSize.height) {
          ratio = constraints.maxWidth / nativePreviewSize.width;
        } else {
          ratio = constraints.maxHeight / nativePreviewSize.height;
        }
        break;
      case CameraPreviewFit.contain:
        final ratioW = constraints.maxWidth / nativePreviewSize.width;
        final ratioH = constraints.maxHeight / nativePreviewSize.height;
        final minRatio = min(ratioW, ratioH);
        ratio = minRatio;
        break;
    }
    return ratio;
  }

  @override
  bool operator ==(Object other) =>
      identical(this, other) ||
      other is PreviewSizeCalculator &&
          runtimeType == other.runtimeType &&
          previewFit == other.previewFit &&
          constraints == other.constraints &&
          previewSize == other.previewSize;

  @override
  int get hashCode => previewSize.hashCode ^ previewSize.hashCode;
}



================================================
FILE: lib/src/widgets/preview/picture_in_picutre_config.dart
================================================
import 'package:camerawesome/src/orchestrator/models/sensors.dart';
import 'package:flutter/material.dart';

/// Builder used to decoarate the [preview] within it. The [preview] is the
/// one tied to the [sensor].
/// The returned widget should include the [preview] in order to display it.
/// You must constrain the size of the [preview]. You can do it by wrapping it
/// within a SizedBox or a Container with a fixed size for example.
typedef PictureInPictureBuilder = Widget Function(
  Widget preview,
  double aspectRatio,
);

typedef PictureInPictureConfigBuilder = PictureInPictureConfig Function(
  int index,
  Sensor sensor,
);

class PictureInPictureConfig {
  final Offset startingPosition;
  final bool isDraggable;
  final Sensor sensor;
  final PictureInPictureBuilder pictureInPictureBuilder;
  final VoidCallback? onTap;

  PictureInPictureConfig({
    this.startingPosition = const Offset(20, 20),
    this.isDraggable = true,
    required this.sensor,
    PictureInPictureBuilder? pictureInPictureBuilder,
    this.onTap,
  }) : pictureInPictureBuilder = pictureInPictureBuilder ??
            ((preview, aspectRatio) {
              return Container(
                  decoration: BoxDecoration(
                    color: Colors.transparent,
                    border: Border.all(color: Colors.white60, width: 3),
                    borderRadius: BorderRadius.circular(23),
                    boxShadow: [
                      BoxShadow(
                        color: Colors.black.withValues(alpha: 0.15),
                        spreadRadius: 10,
                        blurRadius: 20,
                        offset: const Offset(0, 0),
                      ),
                    ],
                  ),
                  child: Stack(
                    children: [
                      ClipRRect(
                        borderRadius: BorderRadius.circular(20),
                        child: SizedBox(
                          // TODO: set size in config
                          height: 200,
                          child: preview,
                          // child: frontPreviewTexture,
                        ),
                      ),
                      Text("${sensor.position}"),
                    ],
                  ));
            });
}



================================================
FILE: lib/src/widgets/preview/preview.dart
================================================
export 'awesome_camera_floating_preview.dart';
export 'awesome_camera_gesture_detector.dart';
export 'awesome_camera_preview.dart';
export 'awesome_focus_indicator.dart';
export 'picture_in_picutre_config.dart';



================================================
FILE: lib/src/widgets/utils/animated_clip_rect.dart
================================================
import 'package:flutter/material.dart';

class AnimatedClipRect extends StatefulWidget {
  @override
  AnimatedClipRectState createState() => AnimatedClipRectState();

  final Widget child;
  final bool open;
  final bool horizontalAnimation;
  final bool verticalAnimation;
  final Alignment alignment;
  final Duration duration;
  final Duration? reverseDuration;
  final Curve curve;
  final Curve? reverseCurve;

  ///The behavior of the controller when [AccessibilityFeatures.disableAnimations] is true.
  final AnimationBehavior animationBehavior;

  const AnimatedClipRect({
    super.key,
    required this.child,
    required this.open,
    this.horizontalAnimation = true,
    this.verticalAnimation = true,
    this.alignment = Alignment.center,
    this.duration = const Duration(milliseconds: 500),
    this.reverseDuration,
    this.curve = Curves.linear,
    this.reverseCurve,
    this.animationBehavior = AnimationBehavior.normal,
  });
}

class AnimatedClipRectState extends State<AnimatedClipRect>
    with TickerProviderStateMixin {
  late AnimationController _animationController;
  late Animation _animation;

  @override
  void dispose() {
    _animationController.dispose();
    super.dispose();
  }

  @override
  void initState() {
    _animationController = AnimationController(
        duration: widget.duration,
        reverseDuration: widget.reverseDuration ?? widget.duration,
        vsync: this,
        value: widget.open ? 1.0 : 0.0,
        animationBehavior: widget.animationBehavior);
    _animation = Tween(begin: 0.0, end: 1.0).animate(CurvedAnimation(
      parent: _animationController,
      curve: widget.curve,
      reverseCurve: widget.reverseCurve ?? widget.curve,
    ));
    super.initState();
  }

  @override
  Widget build(BuildContext context) {
    widget.open
        ? _animationController.forward()
        : _animationController.reverse();

    return ClipRect(
      child: AnimatedBuilder(
        animation: _animationController,
        builder: (_, child) {
          if (_animation.value == 0.0) {
            // The widget becomes invisible since it has an height/width of 0.0
            return const SizedBox.shrink();
          }
          return Align(
            alignment: widget.alignment,
            heightFactor: widget.verticalAnimation ? _animation.value : 1.0,
            widthFactor: widget.horizontalAnimation ? _animation.value : 1.0,
            child: child,
          );
        },
        child: widget.child,
      ),
    );
  }
}



================================================
FILE: lib/src/widgets/utils/awesome_bouncing_widget.dart
================================================
// ignore_for_file: library_private_types_in_public_api

import 'package:flutter/material.dart';
import 'package:flutter/services.dart';

class AwesomeBouncingWidget extends StatefulWidget {
  const AwesomeBouncingWidget({
    super.key,
    required this.child,
    this.onTap,
    this.disabledOpacity = 0.3,
    this.vibrationEnabled = true,
    this.duration = const Duration(milliseconds: 100),
  });

  final Widget child;
  final VoidCallback? onTap;
  final double disabledOpacity;
  final Duration duration;
  final bool? vibrationEnabled;

  @override
  _AwesomeBouncingWidgetState createState() => _AwesomeBouncingWidgetState();
}

class _AwesomeBouncingWidgetState extends State<AwesomeBouncingWidget>
    with SingleTickerProviderStateMixin {
  late AnimationController? _controller;
  late double _scale;

  @override
  void initState() {
    _controller = AnimationController(
      vsync: this,
      duration: widget.duration,
      lowerBound: 0.0,
      upperBound: 0.1,
    )..addListener(() {
        setState(() {});
      });
    super.initState();
  }

  @override
  void dispose() {
    _controller!.stop();
    _controller!.dispose();
    _controller = null;
    super.dispose();
  }

  @override
  Widget build(BuildContext context) {
    _scale = 1 - _controller!.value;

    return IgnorePointer(
      ignoring: widget.onTap == null,
      child: AnimatedOpacity(
        duration: const Duration(milliseconds: 200),
        opacity: (widget.onTap != null) ? 1.0 : widget.disabledOpacity,
        child: GestureDetector(
          onTapDown: _onTapDown,
          onTapUp: _onTapUp,
          onTapCancel: _onTapCancel,
          child: Transform.scale(
            scale: _scale,
            child: widget.child,
          ),
        ),
      ),
    );
  }

  void _onTapDown(TapDownDetails details) {
    if (widget.vibrationEnabled == true) {
      HapticFeedback.selectionClick();
    }
    _controller?.forward.call();
  }

  void _onTapUp(TapUpDetails details) {
    Future.delayed(widget.duration, () {
      _controller?.reverse.call();
    });
    widget.onTap?.call();
  }

  void _onTapCancel() {
    _controller?.reverse.call();
  }
}



================================================
FILE: lib/src/widgets/utils/awesome_circle_icon.dart
================================================
import 'package:camerawesome/src/widgets/utils/awesome_theme.dart';
import 'package:flutter/material.dart';

class AwesomeCircleWidget extends StatelessWidget {
  final Widget? child;
  final IconData? icon;
  final double size;
  final AwesomeTheme? theme;
  final double scale;

  const AwesomeCircleWidget.icon({
    super.key,
    this.size = 50.0,
    required IconData this.icon,
    this.theme,
    this.scale = 1.0,
  }) : child = null;

  const AwesomeCircleWidget({
    super.key,
    this.size = 50.0,
    required Widget this.child,
    this.theme,
    this.scale = 1.0,
  }) : icon = null;

  @override
  Widget build(BuildContext context) {
    final theme = this.theme ?? AwesomeThemeProvider.of(context).theme;
    final buttonTheme = theme.buttonTheme;
    return Material(
      shape: buttonTheme.shape,
      color: buttonTheme.backgroundColor,
      child: Padding(
        padding: buttonTheme.padding * scale,
        child: child ??
            Icon(
              icon,
              color: buttonTheme.foregroundColor,
              size: buttonTheme.iconSize * scale,
            ),
      ),
    );
  }
}



================================================
FILE: lib/src/widgets/utils/awesome_oriented_widget.dart
================================================
import 'package:camerawesome/camerawesome_plugin.dart';
import 'package:flutter/material.dart';

class AwesomeOrientedWidget extends StatefulWidget {
  final Widget child;
  final bool rotateWithDevice;

  const AwesomeOrientedWidget({
    super.key,
    required this.child,
    this.rotateWithDevice = true,
  });

  @override
  State<StatefulWidget> createState() {
    return AwesomeOrientedWidgetState();
  }
}

class AwesomeOrientedWidgetState extends State<AwesomeOrientedWidget> {
  CameraOrientations previousOrientation = CameraOrientations.portrait_up;
  double turns = 0;

  @override
  Widget build(BuildContext context) {
    if (widget.rotateWithDevice) {
      return StreamBuilder<CameraOrientations>(
        stream: CamerawesomePlugin.getNativeOrientation(),
        builder: (_, orientationSnapshot) {
          final orientation = orientationSnapshot.data;
          if (orientation != null && orientation != previousOrientation) {
            turns = shortestTurnsToReachTarget(
              current: turns,
              target: getTurns(orientation),
            );
            previousOrientation = orientation;
          }

          return AnimatedRotation(
            turns: turns,
            duration: const Duration(milliseconds: 200),
            curve: Curves.easeInOut,
            child: widget.child,
          );
        },
      );
    } else {
      return widget.child;
    }
  }

  double getTurns(CameraOrientations orientation) {
    switch (orientation) {
      case CameraOrientations.landscape_left:
        return 0.75;
      case CameraOrientations.landscape_right:
        return 0.25;
      case CameraOrientations.portrait_up:
        return 0;
      case CameraOrientations.portrait_down:
        return 0.5;
    }
  }

  /// Determines which next turn value should be used to have the least rotation
  /// movements between [current] and [target]
  ///
  /// E.g: when being at 0.5 turns, should I go to 0.75 or to -0.25 to minimize
  /// the rotation ?
  double shortestTurnsToReachTarget(
      {required double current, required double target}) {
    final currentDegree = current * 360;
    final targetDegree = target * 360;

    // Determine if we need to go clockwise or counterclockwise to reach
    // the next angle with the least movements
    // See https://math.stackexchange.com/a/2898118
    final clockWise = (targetDegree - currentDegree + 540) % 360 - 180 > 0;
    double resultDegree = currentDegree;
    do {
      resultDegree += (clockWise ? 1 : -1) * 360 / 4;
    } while (resultDegree % 360 != targetDegree % 360);

    // Revert back to turns
    return resultDegree / 360;
  }
}



================================================
FILE: lib/src/widgets/utils/awesome_theme.dart
================================================
import 'package:camerawesome/src/widgets/utils/awesome_bouncing_widget.dart';
import 'package:flutter/material.dart';

typedef ButtonBuilder = Widget Function(
  Widget child,
  VoidCallback onTap,
);

class AwesomeTheme {
  final AwesomeButtonTheme buttonTheme;
  final Color bottomActionsBackgroundColor;

  AwesomeTheme({
    AwesomeButtonTheme? buttonTheme,
    Color? bottomActionsBackgroundColor,
  })  : buttonTheme = buttonTheme ?? AwesomeButtonTheme(),
        bottomActionsBackgroundColor =
            bottomActionsBackgroundColor ?? Colors.black54;

  @override
  bool operator ==(Object other) =>
      identical(this, other) ||
      other is AwesomeTheme &&
          runtimeType == other.runtimeType &&
          buttonTheme == other.buttonTheme &&
          bottomActionsBackgroundColor == other.bottomActionsBackgroundColor;

  @override
  int get hashCode =>
      buttonTheme.hashCode ^ bottomActionsBackgroundColor.hashCode;

  AwesomeTheme copyWith({
    AwesomeButtonTheme? buttonTheme,
    Color? bottomActionsBackgroundColor,
  }) {
    return AwesomeTheme(
      buttonTheme: buttonTheme ?? this.buttonTheme,
      bottomActionsBackgroundColor:
          bottomActionsBackgroundColor ?? this.bottomActionsBackgroundColor,
    );
  }
}

class AwesomeButtonTheme {
  final Color foregroundColor;
  final Color backgroundColor;
  final double iconSize;
  final EdgeInsets padding;
  final ShapeBorder shape;
  final bool rotateWithCamera;
  final ButtonBuilder buttonBuilder;

  static const double baseIconSize = 25;

  AwesomeButtonTheme({
    this.foregroundColor = Colors.white,
    this.backgroundColor = Colors.black12,
    this.iconSize = baseIconSize,
    this.padding = const EdgeInsets.all(12),
    this.shape = const CircleBorder(),
    this.rotateWithCamera = true,
    ButtonBuilder? buttonBuilder,
  }) : buttonBuilder = buttonBuilder ??
            ((Widget child, VoidCallback onTap) =>
                AwesomeBouncingWidget(onTap: onTap, child: child));

  @override
  bool operator ==(Object other) =>
      identical(this, other) ||
      other is AwesomeButtonTheme &&
          runtimeType == other.runtimeType &&
          foregroundColor == other.foregroundColor &&
          backgroundColor == other.backgroundColor &&
          iconSize == other.iconSize &&
          padding == other.padding &&
          shape == other.shape &&
          rotateWithCamera == other.rotateWithCamera &&
          buttonBuilder == other.buttonBuilder;

  @override
  int get hashCode =>
      foregroundColor.hashCode ^
      backgroundColor.hashCode ^
      iconSize.hashCode ^
      padding.hashCode ^
      shape.hashCode ^
      rotateWithCamera.hashCode ^
      buttonBuilder.hashCode;

  AwesomeButtonTheme copyWith({
    Color? foregroundColor,
    Color? backgroundColor,
    double? iconSize,
    EdgeInsets? padding,
    ShapeBorder? shape,
    bool? rotateWithCamera,
    ButtonBuilder? buttonBuilder,
    double? baseIconSize,
  }) {
    return AwesomeButtonTheme(
      foregroundColor: foregroundColor ?? this.foregroundColor,
      backgroundColor: backgroundColor ?? this.backgroundColor,
      iconSize: iconSize ?? this.iconSize,
      padding: padding ?? this.padding,
      shape: shape ?? this.shape,
      rotateWithCamera: rotateWithCamera ?? this.rotateWithCamera,
      buttonBuilder: buttonBuilder ?? this.buttonBuilder,
    );
  }
}

class AwesomeThemeProvider extends InheritedWidget {
  final AwesomeTheme theme;

  AwesomeThemeProvider({
    super.key,
    AwesomeTheme? theme,
    required super.child,
  }) : theme = theme ?? AwesomeTheme();

  static AwesomeThemeProvider of(BuildContext context) {
    return context.dependOnInheritedWidgetOfExactType<AwesomeThemeProvider>()!;
  }

  @override
  bool updateShouldNotify(covariant AwesomeThemeProvider oldWidget) {
    return theme != oldWidget.theme;
  }
}



================================================
FILE: lib/src/widgets/utils/utils.dart
================================================
export 'animated_clip_rect.dart';
export 'awesome_bouncing_widget.dart';
export 'awesome_circle_icon.dart';
export 'awesome_oriented_widget.dart';
export 'awesome_theme.dart';



================================================
FILE: lib/src/widgets/zoom/awesome_zoom_selector.dart
================================================
import 'package:camerawesome/camerawesome_plugin.dart';
import 'package:flutter/material.dart';

class AwesomeZoomSelector extends StatefulWidget {
  final CameraState state;

  const AwesomeZoomSelector({
    super.key,
    required this.state,
  });

  @override
  State<AwesomeZoomSelector> createState() => _AwesomeZoomSelectorState();
}

class _AwesomeZoomSelectorState extends State<AwesomeZoomSelector> {
  double? minZoom;
  double? maxZoom;

  @override
  void initState() {
    super.initState();
    initAsync();
  }

  initAsync() async {
    minZoom = await CamerawesomePlugin.getMinZoom();
    maxZoom = await CamerawesomePlugin.getMaxZoom();
  }

  @override
  Widget build(BuildContext context) {
    return StreamBuilder<SensorConfig>(
      stream: widget.state.sensorConfig$,
      builder: (context, sensorConfigSnapshot) {
        initAsync();
        if (sensorConfigSnapshot.data == null ||
            minZoom == null ||
            maxZoom == null) {
          return const SizedBox.shrink();
        }

        return StreamBuilder<double>(
          stream: sensorConfigSnapshot.requireData.zoom$,
          builder: (context, snapshot) {
            if (snapshot.hasData) {
              return _ZoomIndicatorLayout(
                zoom: snapshot.requireData,
                min: minZoom!,
                max: maxZoom!,
                sensorConfig: widget.state.sensorConfig,
              );
            } else {
              return const SizedBox.shrink();
            }
          },
        );
      },
    );
  }
}

class _ZoomIndicatorLayout extends StatelessWidget {
  final double zoom;
  final double min;
  final double max;
  final SensorConfig sensorConfig;

  const _ZoomIndicatorLayout({
    required this.zoom,
    required this.min,
    required this.max,
    required this.sensorConfig,
  });

  @override
  Widget build(BuildContext context) {
    final displayZoom = (max - min) * zoom + min;
    if (min == 1.0) {
      // Assume there's only one lens for zooming purpose, only display current zoom
      return _ZoomIndicator(
        normalValue: 0.0,
        zoom: zoom,
        selected: true,
        min: min,
        max: max,
        sensorConfig: sensorConfig,
      );
    }

    return Row(
      mainAxisAlignment: MainAxisAlignment.center,
      children: [
        // Show 3 dots for zooming: min, 1.0X and max zoom. The closer one shows
        // text, the other ones a dot.
        _ZoomIndicator(
          normalValue: 0.0,
          zoom: zoom,
          selected: displayZoom < 1.0,
          min: min,
          max: max,
          sensorConfig: sensorConfig,
        ),
        Padding(
          padding: const EdgeInsets.symmetric(horizontal: 8),
          child: _ZoomIndicator(
            normalValue: (1 - min) / (max - min),
            zoom: zoom,
            selected: !(displayZoom < 1.0 || displayZoom == max),
            min: min,
            max: max,
            sensorConfig: sensorConfig,
          ),
        ),
        _ZoomIndicator(
          normalValue: 1.0,
          zoom: zoom,
          selected: displayZoom == max,
          min: min,
          max: max,
          sensorConfig: sensorConfig,
        ),
      ],
    );
  }
}

class _ZoomIndicator extends StatelessWidget {
  final double zoom;
  final double min;
  final double max;
  final double normalValue;
  final SensorConfig sensorConfig;
  final bool selected;

  const _ZoomIndicator({
    required this.zoom,
    required this.min,
    required this.max,
    required this.normalValue,
    required this.sensorConfig,
    required this.selected,
  });

  @override
  Widget build(BuildContext context) {
    final baseTheme = AwesomeThemeProvider.of(context).theme;
    final baseButtonTheme = baseTheme.buttonTheme;
    final displayZoom = (max - min) * zoom + min;
    Widget content = AnimatedSwitcher(
      duration: const Duration(milliseconds: 100),
      transitionBuilder: (child, anim) {
        return ScaleTransition(scale: anim, child: child);
      },
      child: selected
          ? AwesomeBouncingWidget(
              key: ValueKey("zoomIndicator_${normalValue}_selected"),
              onTap: () {
                sensorConfig.setZoom(normalValue);
              },
              child: Container(
                color: Colors.transparent,
                padding: const EdgeInsets.all(0.0),
                child: AwesomeCircleWidget(
                  theme: baseTheme,
                  child: Text(
                    "${displayZoom.toStringAsFixed(1)}X",
                    style: const TextStyle(color: Colors.white, fontSize: 12),
                  ),
                ),
              ),
            )
          : AwesomeBouncingWidget(
              key: ValueKey("zoomIndicator_${normalValue}_unselected"),
              onTap: () {
                sensorConfig.setZoom(normalValue);
              },
              child: Container(
                color: Colors.transparent,
                padding: const EdgeInsets.all(16.0),
                child: AwesomeCircleWidget(
                  theme: baseTheme.copyWith(
                    buttonTheme: baseButtonTheme.copyWith(
                      backgroundColor: baseButtonTheme.foregroundColor,
                      padding: EdgeInsets.zero,
                    ),
                  ),
                  child: const SizedBox(width: 6, height: 6),
                ),
              ),
            ),
    );

    // Same width for each dot to keep them in their position
    return SizedBox(
      width: 56,
      child: Center(
        child: content,
      ),
    );
  }
}



================================================
FILE: lib/src/widgets/zoom/zoom.dart
================================================
export 'awesome_zoom_selector.dart';



================================================
FILE: pigeons/interface.dart
================================================
import 'package:pigeon/pigeon.dart';

class PreviewSize {
  final double width;
  final double height;

  const PreviewSize(this.width, this.height);
}

class PreviewData {
  double? textureId;
  PreviewSize? size;
}

class ExifPreferences {
  bool saveGPSLocation;

  ExifPreferences({required this.saveGPSLocation});
}

class PigeonSensor {
  final PigeonSensorPosition position;
  final PigeonSensorType type;
  final String? deviceId;

  PigeonSensor({
    this.position = PigeonSensorPosition.unknown,
    this.type = PigeonSensorType.unknown,
    this.deviceId,
  });
}

enum PigeonSensorPosition {
  back,
  front,
  unknown,
}

/// Video recording quality, from [sd] to [uhd], with [highest] and [lowest] to
/// let the device choose the best/worst quality available.
/// [highest] is the default quality.
///
/// Qualities are defined like this:
/// [sd] < [hd] < [fhd] < [uhd]
enum VideoRecordingQuality {
  lowest,
  sd,
  hd,
  fhd,
  uhd,
  highest,
}

/// If the specified [VideoRecordingQuality] is not available on the device,
/// the [VideoRecordingQuality] will fallback to [higher] or [lower] quality.
/// [higher] is the default fallback strategy.
enum QualityFallbackStrategy {
  higher,
  lower,
}

/// Video recording options. Some of them are specific to each platform.
class VideoOptions {
  /// Enable audio while video recording
  final bool enableAudio;

  /// The quality of the video recording, defaults to [VideoRecordingQuality.highest].
  final VideoRecordingQuality? quality;

  // TODO if there are properties common to all platform, move them here (iOS, Android and Web)
  final AndroidVideoOptions? android;
  final CupertinoVideoOptions? ios;

  VideoOptions({
    required this.android,
    required this.ios,
    required this.enableAudio,
    required this.quality,
  });
}

class AndroidVideoOptions {
  /// The bitrate of the video recording. Only set it if a custom bitrate is
  /// desired.
  final int? bitrate;

  final QualityFallbackStrategy? fallbackStrategy;

  AndroidVideoOptions({
    required this.bitrate,
    required this.fallbackStrategy,
  });
}

enum CupertinoFileType {
  quickTimeMovie,
  mpeg4,
  appleM4V,
  type3GPP,
  type3GPP2,
}

enum CupertinoCodecType {
  h264,
  hevc,
  hevcWithAlpha,
  jpeg,
  appleProRes4444,
  appleProRes422,
  appleProRes422HQ,
  appleProRes422LT,
  appleProRes422Proxy,
}

class CupertinoVideoOptions {
  /// Specify video file type, defaults to [AVFileTypeQuickTimeMovie].
  final CupertinoFileType? fileType;

  /// Specify video codec, defaults to [AVVideoCodecTypeH264].
  final CupertinoCodecType? codec;

  /// Specify video fps, defaults to [30].
  final int? fps;

  CupertinoVideoOptions({
    this.fileType,
    this.codec,
    this.fps,
  });
}

enum PigeonSensorType {
  /// A built-in wide-angle camera.
  ///
  /// The wide angle sensor is the default sensor for iOS
  wideAngle,

  /// A built-in camera with a shorter focal length than that of the wide-angle camera.
  ultraWideAngle,

  /// A built-in camera device with a longer focal length than the wide-angle camera.
  telephoto,

  /// A device that consists of two cameras, one Infrared and one YUV.
  ///
  /// iOS only
  trueDepth,
  unknown;

  // SensorType get defaultSensorType => SensorType.wideAngle;
  // SensorType defaultSensorType() => SensorType.wideAngle;
}

class PigeonSensorTypeDevice {
  final PigeonSensorType sensorType;

  /// A localized device name for display in the user interface.
  final String name;

  /// The current exposure ISO value.
  final double iso;

  /// A Boolean value that indicates whether the flash is currently available for use.
  final bool flashAvailable;

  /// An identifier that uniquely identifies the device.
  final String uid;

  PigeonSensorTypeDevice({
    required this.sensorType,
    required this.name,
    required this.iso,
    required this.flashAvailable,
    required this.uid,
  });
}

// TODO: instead of storing SensorTypeDevice values,
// this would be useful when CameraX will support multiple sensors.
// store them in a list of SensorTypeDevice.
// ex:
// List<SensorTypeDevice> wideAngle;
// List<SensorTypeDevice> ultraWideAngle;

class PigeonSensorDeviceData {
  /// A built-in wide-angle camera.
  ///
  /// The wide angle sensor is the default sensor for iOS
  PigeonSensorTypeDevice? wideAngle;

  /// A built-in camera with a shorter focal length than that of the wide-angle camera.
  PigeonSensorTypeDevice? ultraWideAngle;

  /// A built-in camera device with a longer focal length than the wide-angle camera.
  PigeonSensorTypeDevice? telephoto;

  /// A device that consists of two cameras, one Infrared and one YUV.
  ///
  /// iOS only
  PigeonSensorTypeDevice? trueDepth;

  PigeonSensorDeviceData({
    this.wideAngle,
    this.ultraWideAngle,
    this.telephoto,
    this.trueDepth,
  });

// int get availableBackSensors => [
//       wideAngle,
//       ultraWideAngle,
//       telephoto,
//     ].where((element) => element != null).length;

// int get availableFrontSensors => [
//       trueDepth,
//     ].where((element) => element != null).length;
}

enum CamerAwesomePermission {
  storage,
  camera,
  location,
  // ignore: constant_identifier_names
  record_audio,
}

class AndroidFocusSettings {
  /// The auto focus will be canceled after the given [autoCancelDurationInMillis].
  /// If [autoCancelDurationInMillis] is equals to 0 (or less), the auto focus
  /// will **not** be canceled. A manual `focusOnPoint` call will be needed to
  /// focus on an other point.
  /// Minimal duration of [autoCancelDurationInMillis] is 1000 ms. If set
  /// between 0 (exclusive) and 1000 (exclusive), it will be raised to 1000.
  int autoCancelDurationInMillis;

  AndroidFocusSettings({required this.autoCancelDurationInMillis});
}

class PlaneWrapper {
  final Uint8List bytes;
  final int bytesPerRow;
  final int? bytesPerPixel;
  final int? width;
  final int? height;

  PlaneWrapper({
    required this.bytes,
    required this.bytesPerRow,
    required this.bytesPerPixel,
    this.width,
    this.height,
  });
}

enum AnalysisImageFormat { yuv_420, bgra8888, jpeg, nv21, unknown }

enum AnalysisRotation {
  rotation0deg,
  rotation90deg,
  rotation180deg,
  rotation270deg
}

class CropRectWrapper {
  final int left;
  final int top;
  final int width;
  final int height;

  CropRectWrapper({
    required this.left,
    required this.top,
    required this.width,
    required this.height,
  });
}

class AnalysisImageWrapper {
  final AnalysisImageFormat format;
  final Uint8List? bytes;
  final int width;
  final int height;
  final List<PlaneWrapper?>? planes;
  final CropRectWrapper? cropRect;
  final AnalysisRotation? rotation;

  AnalysisImageWrapper({
    required this.format,
    required this.bytes,
    required this.width,
    required this.height,
    required this.planes,
    required this.cropRect,
    required this.rotation,
  });
}

@HostApi()
abstract class AnalysisImageUtils {
  @async
  AnalysisImageWrapper nv21toJpeg(
    AnalysisImageWrapper nv21Image,
    int jpegQuality,
  );

  @async
  AnalysisImageWrapper yuv420toJpeg(
    AnalysisImageWrapper yuvImage,
    int jpegQuality,
  );

  @async
  AnalysisImageWrapper yuv420toNv21(AnalysisImageWrapper yuvImage);

  @async
  AnalysisImageWrapper bgra8888toJpeg(
    AnalysisImageWrapper bgra8888image,
    int jpegQuality,
  );
}

@HostApi()
abstract class CameraInterface {
  @async
  bool setupCamera(
    List<PigeonSensor> sensors,
    String aspectRatio,
    double zoom,
    bool mirrorFrontCamera,
    bool enablePhysicalButton,
    String flashMode,
    String captureMode,
    bool enableImageStream,
    ExifPreferences exifPreferences,
    VideoOptions? videoOptions,
  );

  List<String> checkPermissions(List<String> permissions);

  /// Returns given [CamerAwesomePermission] list (as String). Location permission might be
  /// refused but the app should still be able to run.
  @async
  List<String> requestPermissions(bool saveGpsLocation);

  int getPreviewTextureId(int cameraPosition);

  // TODO async with void return type seems to not work (channel-error)
  @async
  bool takePhoto(List<PigeonSensor> sensors, List<String?> paths);

  @async
  void recordVideo(List<PigeonSensor> sensors, List<String?> paths);

  void pauseVideoRecording();

  void resumeVideoRecording();

  void receivedImageFromStream();

  @async
  bool stopRecordingVideo();

  List<PigeonSensorTypeDevice> getFrontSensors();

  List<PigeonSensorTypeDevice> getBackSensors();

  bool start();

  bool stop();

  void setFlashMode(String mode);

  void handleAutoFocus();

  /// Starts auto focus on a point at ([x], [y]).
  ///
  /// On Android, you can control after how much time you want to switch back
  /// to passive focus mode with [androidFocusSettings].
  void focusOnPoint(
    PreviewSize previewSize,
    double x,
    double y,
    AndroidFocusSettings? androidFocusSettings,
  );

  void setZoom(double zoom);

  void setMirrorFrontCamera(bool mirror);

  // TODO: specify the position of the sensor
  void setSensor(List<PigeonSensor> sensors);

  void setCorrection(double brightness);

  double getMinZoom();

  double getMaxZoom();

  void setCaptureMode(String mode);

  @async
  bool setRecordingAudioMode(bool enableAudio);

  List<PreviewSize> availableSizes();

  void refresh();

  PreviewSize? getEffectivPreviewSize(int index);

  void setPhotoSize(PreviewSize size);

  void setPreviewSize(PreviewSize size);

  void setAspectRatio(String aspectRatio);

  void setupImageAnalysisStream(
    String format,
    int width,
    double? maxFramesPerSecond,
    bool autoStart,
  );

  @async
  bool setExifPreferences(ExifPreferences exifPreferences);

  void startAnalysis();

  void stopAnalysis();

  void setFilter(List<double> matrix);

  @async
  bool isVideoRecordingAndImageAnalysisSupported(PigeonSensorPosition sensor);

  bool isMultiCamSupported();
}



================================================
FILE: pigeons/pigeon.sh
================================================
flutter pub run pigeon \
  --input pigeons/interface.dart \
  --dart_out lib/pigeon.dart \
  --experimental_kotlin_out ./android/src/main/kotlin/com/apparence/camerawesome/cameraX/Pigeon.kt \
  --experimental_kotlin_package "com.apparence.camerawesome.cameraX" \
  --objc_source_out ./ios/Sources/camerawesome/Pigeon/Pigeon.m \
  --objc_header_out ./ios/Sources/camerawesome/include/Pigeon.h \


================================================
FILE: test/camera_context_test.dart
================================================




================================================
FILE: test/filter_test.dart
================================================
import 'dart:io';

import 'package:camerawesome/src/photofilters/filters/preset_filters.dart';
import 'package:flutter_test/flutter_test.dart';
import 'package:image/image.dart';

void main() {
  const src = 'test/res/bird.jpg';

  for (var filter in presetFiltersList) {
    test("Apply filter ${filter.name}", () async {
      final dest = 'test/out/${filter.name.replaceAll(" ", "_")}.jpg';
      await File(dest).parent.create(recursive: true);

      final Image image = decodeImage(File(src).readAsBytesSync())!;
      final pixels = image.getBytes();

      // Make treatment
      filter.apply(pixels, image.width, image.height);

      // Save image
      final Image out = Image.fromBytes(
        width: image.width,
        height: image.height,
        bytes: pixels.buffer,
      );
      File(dest).writeAsBytesSync(encodeNamedImage(dest, out)!);
    });
  }
}




================================================
FILE: .github/FUNDING.yml
================================================
# These are supported funding model platforms

github: #[g-apparence]
patreon: # Replace with a single Patreon username
open_collective: camerawesome
ko_fi: # Replace with a single Ko-fi username
tidelift: # Replace with a single Tidelift platform-name/package-name e.g., npm/babel
community_bridge: # Replace with a single Community Bridge project-name e.g., cloud-foundry
liberapay: # Replace with a single Liberapay username
issuehunt: # Replace with a single IssueHunt username
otechie: # Replace with a single Otechie username
lfx_crowdfunding: # Replace with a single LFX Crowdfunding project-name e.g., cloud-foundry
custom: # Replace with up to 4 custom sponsorship URLs e.g., ['link1', 'link2']



================================================
FILE: .github/PULL_REQUEST_TEMPLATE.md
================================================
## Description

*What your Pull Request change ?*

## Checklist

Before creating any Pull Request, confirm that it meets all requirements listed below by checking the relevant checkboxes (`[x]`).

- [ ] 📕 I read the [Contributing page](https://github.com/Apparence-io/camera_awesome/blob/master/CONTRIBUTING.md).
- [ ] 🤝 I match the actual coding style.
- [ ] ✅ I ran ```flutter analyze``` without any issues.

## Breaking Change

- [ ] 🛠 My feature contain breaking change.

*If your feature break something, please detail it*


================================================
FILE: .github/ISSUE_TEMPLATE/BUG.md
================================================
---
name: I found a problem.
about: The camera won't work as described on the Readme (crash, performance, layout etc.).
title: ''
labels: ''
assignees: ''

---

## Steps to Reproduce

*Describe how to reproduce the error*

## Expected results

*What it should be*

## Actual results

*What you see*

## About your device

| Brand   | Model       | OS        |
| ------- | ----------- | --------- |
| *Apple* | *iPhone X*  | *13.6.1*  |

## Your flutter version

Run this in your command line 
```flutter --version```

*Paste the result here*

</details>


================================================
FILE: .github/ISSUE_TEMPLATE/feature_request.md
================================================
---
name: Feature request
about: Suggest a new idea.
title: ''
labels: ''
assignees: ''

---

## Proposal

*Describe your new feature*



================================================
FILE: .run/analysis_image_filter.run.xml
================================================
<component name="ProjectRunConfigurationManager">
    <configuration default="false" name="analysis_image_filter" type="FlutterRunConfigurationType"
        factoryName="Flutter">
        <option name="filePath" value="$PROJECT_DIR$/example/lib/analysis_image_filter.dart" />
        <method v="2" />
    </configuration>
</component>


================================================
FILE: .run/analysis_image_filter_picker.run.xml
================================================
<component name="ProjectRunConfigurationManager">
    <configuration default="false" name="analysis_image_filter_picker"
        type="FlutterRunConfigurationType" factoryName="Flutter">
        <option name="filePath"
            value="$PROJECT_DIR$/example/lib/analysis_image_filter_picker.dart" />
        <method v="2" />
    </configuration>
</component>


================================================
FILE: .run/barcode_overlay_example.run.xml
================================================
<component name="ProjectRunConfigurationManager">
    <configuration default="false" name="barcode_overlay_example" type="FlutterRunConfigurationType"
        factoryName="Flutter">
        <option name="filePath" value="$PROJECT_DIR$/example/lib/preview_overlay_example.dart" />
        <method v="2" />
    </configuration>
</component>


================================================
FILE: .run/camera_analysis_capabilities.run.xml
================================================
<component name="ProjectRunConfigurationManager">
    <configuration default="false" name="camera_analysis_capabilities"
        type="FlutterRunConfigurationType" factoryName="Flutter">
        <option name="filePath"
            value="$PROJECT_DIR$/example/lib/camera_analysis_capabilities.dart" />
        <method v="2" />
    </configuration>
</component>


================================================
FILE: .run/custom_awesome_ui.run.xml
================================================
<component name="ProjectRunConfigurationManager">
    <configuration default="false" name="custom_awesome_ui" type="FlutterRunConfigurationType"
        factoryName="Flutter">
        <option name="filePath" value="$PROJECT_DIR$/example/lib/custom_awesome_ui.dart" />
        <method v="2" />
    </configuration>
</component>


================================================
FILE: .run/custom_theme.run.xml
================================================
<component name="ProjectRunConfigurationManager">
  <configuration default="false" name="custom_theme" type="FlutterRunConfigurationType" factoryName="Flutter">
    <option name="filePath" value="$PROJECT_DIR$/example/lib/custom_theme.dart" />
    <method v="2" />
  </configuration>
</component>


================================================
FILE: .run/detect_barcodes.run.xml
================================================
<component name="ProjectRunConfigurationManager">
    <configuration default="false" name="detect_barcodes" type="FlutterRunConfigurationType"
        factoryName="Flutter">
        <option name="filePath" value="$PROJECT_DIR$/example/lib/ai_analysis_barcode.dart" />
        <method v="2" />
    </configuration>
</component>


================================================
FILE: .run/detect_faces.run.xml
================================================
<component name="ProjectRunConfigurationManager">
    <configuration default="false" name="detect_faces" type="FlutterRunConfigurationType"
        factoryName="Flutter">
        <option name="filePath" value="$PROJECT_DIR$/example/lib/ai_analysis_faces.dart" />
        <method v="2" />
    </configuration>
</component>


================================================
FILE: .run/main.dart.run.xml
================================================
<component name="ProjectRunConfigurationManager">
    <configuration default="false" name="main.dart" type="FlutterRunConfigurationType"
        factoryName="Flutter">
        <option name="filePath" value="$PROJECT_DIR$/example/lib/main.dart" />
        <method v="2" />
    </configuration>
</component>


================================================
FILE: .run/multi_camera.run.xml
================================================
<component name="ProjectRunConfigurationManager">
  <configuration default="false" name="multi_camera" type="FlutterRunConfigurationType" factoryName="Flutter">
    <option name="filePath" value="$PROJECT_DIR$/example/lib/multi_camera.dart" />
    <method v="2" />
  </configuration>
</component>

